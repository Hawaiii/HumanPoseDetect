/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0422 02:54:34.765349  3275 solver.cpp:48] Initializing solver from parameters: 
test_iter: 25
test_interval: 1000
base_lr: 0.001
display: 20
max_iter: 100000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 30000
snapshot: 10000
snapshot_prefix: "models/model"
net: "fcn_alxnet_train_val.prototxt"
I0422 02:54:34.788660  3275 solver.cpp:91] Creating training net from net file: fcn_alxnet_train_val.prototxt
I0422 02:54:34.790222  3275 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0422 02:54:34.790534  3275 net.cpp:49] Initializing net from parameters: 
name: "FCN-AlexNet-joint"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "jointLocation"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 104
    mean_value: 116
    mean_value: 122
  }
  image_data_param {
    source: "/home/mengxin1/HumanPoseDetect/preprocess/train_label_resized.txt"
    batch_size: 10
    shuffle: true
    root_folder: "/home/mengxin1/mpii_human_pose_v1_images_resized/"
    label_num: 48
  }
}
layer {
  name: "jointMap"
  type: "PositionToMap"
  bottom: "data"
  bottom: "jointLocation"
  top: "jointMap"
  gaussian_param {
    std: 25
    half_window_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 100
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "cconv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "cconv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fcc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fcc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 6
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fcc6"
  top: "fcc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fcc6"
  top: "fcc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fcc7"
  type: "Convolution"
  bottom: "fcc6"
  top: "fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fcc7"
  top: "fcc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fcc7"
  top: "fcc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score-fr"
  type: "Convolution"
  bottom: "fcc7"
  top: "score-fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    engine: CAFFE
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "score-fcc7"
  top: "bigscore"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 63
    group: 16
    stride: 32
  }
}
layer {
  name: "crop"
  type: "Crop"
  bottom: "bigscore"
  bottom: "data"
  top: "score"
}
layer {
  name: "balance"
  type: "Balance"
  bottom: "score"
  bottom: "jointMap"
  top: "bscore"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "bscore"
  bottom: "jointMap"
  top: "loss"
}
I0422 02:54:34.790675  3275 layer_factory.hpp:77] Creating layer data
I0422 02:54:34.790704  3275 net.cpp:91] Creating Layer data
I0422 02:54:34.790714  3275 net.cpp:399] data -> data
I0422 02:54:34.790730  3275 net.cpp:399] data -> jointLocation
I0422 02:54:34.790747  3275 image_data_layer.cpp:40] Opening file /home/mengxin1/HumanPoseDetect/preprocess/train_label_resized.txt
I0422 02:54:34.897294  3275 image_data_layer.cpp:57] Shuffling data
I0422 02:54:34.899595  3275 image_data_layer.cpp:62] A total of 9580 images.
I0422 02:54:35.001006  3275 image_data_layer.cpp:89] output data size: 10,3,512,512
I0422 02:54:35.105886  3275 net.cpp:141] Setting up data
I0422 02:54:35.105939  3275 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 02:54:35.105947  3275 net.cpp:148] Top shape: 10 48 1 1 (480)
I0422 02:54:35.105952  3275 net.cpp:156] Memory required for data: 31459200
I0422 02:54:35.105962  3275 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 02:54:35.105983  3275 net.cpp:91] Creating Layer data_data_0_split
I0422 02:54:35.105989  3275 net.cpp:425] data_data_0_split <- data
I0422 02:54:35.105999  3275 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 02:54:35.106014  3275 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 02:54:35.106021  3275 net.cpp:399] data_data_0_split -> data_data_0_split_2
I0422 02:54:35.106223  3275 net.cpp:141] Setting up data_data_0_split
I0422 02:54:35.106235  3275 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 02:54:35.106240  3275 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 02:54:35.106246  3275 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 02:54:35.106250  3275 net.cpp:156] Memory required for data: 125831040
I0422 02:54:35.106254  3275 layer_factory.hpp:77] Creating layer jointMap
I0422 02:54:35.106264  3275 net.cpp:91] Creating Layer jointMap
I0422 02:54:35.106269  3275 net.cpp:425] jointMap <- data_data_0_split_0
I0422 02:54:35.106274  3275 net.cpp:425] jointMap <- jointLocation
I0422 02:54:35.106281  3275 net.cpp:399] jointMap -> jointMap
I0422 02:54:35.106319  3275 net.cpp:141] Setting up jointMap
I0422 02:54:35.106328  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:35.106333  3275 net.cpp:156] Memory required for data: 293603200
I0422 02:54:35.106336  3275 layer_factory.hpp:77] Creating layer jointMap_jointMap_0_split
I0422 02:54:35.106343  3275 net.cpp:91] Creating Layer jointMap_jointMap_0_split
I0422 02:54:35.106348  3275 net.cpp:425] jointMap_jointMap_0_split <- jointMap
I0422 02:54:35.106354  3275 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_0
I0422 02:54:35.106362  3275 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_1
I0422 02:54:35.106411  3275 net.cpp:141] Setting up jointMap_jointMap_0_split
I0422 02:54:35.106420  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:35.106426  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:35.106429  3275 net.cpp:156] Memory required for data: 629147520
I0422 02:54:35.106433  3275 layer_factory.hpp:77] Creating layer conv1
I0422 02:54:35.106451  3275 net.cpp:91] Creating Layer conv1
I0422 02:54:35.106456  3275 net.cpp:425] conv1 <- data_data_0_split_1
I0422 02:54:35.106462  3275 net.cpp:399] conv1 -> conv1
I0422 02:54:35.484583  3275 net.cpp:141] Setting up conv1
I0422 02:54:35.484633  3275 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 02:54:35.484638  3275 net.cpp:156] Memory required for data: 748095360
I0422 02:54:35.484660  3275 layer_factory.hpp:77] Creating layer relu1
I0422 02:54:35.484678  3275 net.cpp:91] Creating Layer relu1
I0422 02:54:35.484683  3275 net.cpp:425] relu1 <- conv1
I0422 02:54:35.484693  3275 net.cpp:386] relu1 -> conv1 (in-place)
I0422 02:54:35.485195  3275 net.cpp:141] Setting up relu1
I0422 02:54:35.485208  3275 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 02:54:35.485213  3275 net.cpp:156] Memory required for data: 867043200
I0422 02:54:35.485219  3275 layer_factory.hpp:77] Creating layer pool1
I0422 02:54:35.485230  3275 net.cpp:91] Creating Layer pool1
I0422 02:54:35.485235  3275 net.cpp:425] pool1 <- conv1
I0422 02:54:35.485244  3275 net.cpp:399] pool1 -> pool1
I0422 02:54:35.485311  3275 net.cpp:141] Setting up pool1
I0422 02:54:35.485326  3275 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 02:54:35.485330  3275 net.cpp:156] Memory required for data: 896780160
I0422 02:54:35.485335  3275 layer_factory.hpp:77] Creating layer norm1
I0422 02:54:35.485347  3275 net.cpp:91] Creating Layer norm1
I0422 02:54:35.485352  3275 net.cpp:425] norm1 <- pool1
I0422 02:54:35.485359  3275 net.cpp:399] norm1 -> norm1
I0422 02:54:35.490134  3275 net.cpp:141] Setting up norm1
I0422 02:54:35.490151  3275 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 02:54:35.490156  3275 net.cpp:156] Memory required for data: 926517120
I0422 02:54:35.490161  3275 layer_factory.hpp:77] Creating layer conv2
I0422 02:54:35.490181  3275 net.cpp:91] Creating Layer conv2
I0422 02:54:35.490186  3275 net.cpp:425] conv2 <- norm1
I0422 02:54:35.490206  3275 net.cpp:399] conv2 -> conv2
I0422 02:54:35.517889  3275 net.cpp:141] Setting up conv2
I0422 02:54:35.517933  3275 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 02:54:35.517940  3275 net.cpp:156] Memory required for data: 1005815680
I0422 02:54:35.517958  3275 layer_factory.hpp:77] Creating layer relu2
I0422 02:54:35.517972  3275 net.cpp:91] Creating Layer relu2
I0422 02:54:35.517978  3275 net.cpp:425] relu2 <- conv2
I0422 02:54:35.517987  3275 net.cpp:386] relu2 -> conv2 (in-place)
I0422 02:54:35.521462  3275 net.cpp:141] Setting up relu2
I0422 02:54:35.521481  3275 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 02:54:35.521486  3275 net.cpp:156] Memory required for data: 1085114240
I0422 02:54:35.521492  3275 layer_factory.hpp:77] Creating layer pool2
I0422 02:54:35.521500  3275 net.cpp:91] Creating Layer pool2
I0422 02:54:35.521505  3275 net.cpp:425] pool2 <- conv2
I0422 02:54:35.521514  3275 net.cpp:399] pool2 -> pool2
I0422 02:54:35.521595  3275 net.cpp:141] Setting up pool2
I0422 02:54:35.521603  3275 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 02:54:35.521607  3275 net.cpp:156] Memory required for data: 1104938880
I0422 02:54:35.521612  3275 layer_factory.hpp:77] Creating layer norm2
I0422 02:54:35.521625  3275 net.cpp:91] Creating Layer norm2
I0422 02:54:35.521628  3275 net.cpp:425] norm2 <- pool2
I0422 02:54:35.521638  3275 net.cpp:399] norm2 -> norm2
I0422 02:54:35.530262  3275 net.cpp:141] Setting up norm2
I0422 02:54:35.530297  3275 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 02:54:35.530302  3275 net.cpp:156] Memory required for data: 1124763520
I0422 02:54:35.530309  3275 layer_factory.hpp:77] Creating layer conv3
I0422 02:54:35.530333  3275 net.cpp:91] Creating Layer conv3
I0422 02:54:35.530339  3275 net.cpp:425] conv3 <- norm2
I0422 02:54:35.530352  3275 net.cpp:399] conv3 -> conv3
I0422 02:54:35.568859  3275 net.cpp:141] Setting up conv3
I0422 02:54:35.568902  3275 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 02:54:35.568907  3275 net.cpp:156] Memory required for data: 1154500480
I0422 02:54:35.568927  3275 layer_factory.hpp:77] Creating layer relu3
I0422 02:54:35.568943  3275 net.cpp:91] Creating Layer relu3
I0422 02:54:35.568950  3275 net.cpp:425] relu3 <- conv3
I0422 02:54:35.568959  3275 net.cpp:386] relu3 -> conv3 (in-place)
I0422 02:54:35.572110  3275 net.cpp:141] Setting up relu3
I0422 02:54:35.572125  3275 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 02:54:35.572129  3275 net.cpp:156] Memory required for data: 1184237440
I0422 02:54:35.572134  3275 layer_factory.hpp:77] Creating layer cconv4
I0422 02:54:35.572154  3275 net.cpp:91] Creating Layer cconv4
I0422 02:54:35.572159  3275 net.cpp:425] cconv4 <- conv3
I0422 02:54:35.572168  3275 net.cpp:399] cconv4 -> conv4
I0422 02:54:35.609491  3275 net.cpp:141] Setting up cconv4
I0422 02:54:35.609539  3275 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 02:54:35.609544  3275 net.cpp:156] Memory required for data: 1213974400
I0422 02:54:35.609557  3275 layer_factory.hpp:77] Creating layer relu4
I0422 02:54:35.609571  3275 net.cpp:91] Creating Layer relu4
I0422 02:54:35.609577  3275 net.cpp:425] relu4 <- conv4
I0422 02:54:35.609586  3275 net.cpp:386] relu4 -> conv4 (in-place)
I0422 02:54:35.613252  3275 net.cpp:141] Setting up relu4
I0422 02:54:35.613270  3275 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 02:54:35.613276  3275 net.cpp:156] Memory required for data: 1243711360
I0422 02:54:35.613281  3275 layer_factory.hpp:77] Creating layer cconv5
I0422 02:54:35.613302  3275 net.cpp:91] Creating Layer cconv5
I0422 02:54:35.613308  3275 net.cpp:425] cconv5 <- conv4
I0422 02:54:35.613322  3275 net.cpp:399] cconv5 -> conv5
I0422 02:54:35.640739  3275 net.cpp:141] Setting up cconv5
I0422 02:54:35.640779  3275 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 02:54:35.640784  3275 net.cpp:156] Memory required for data: 1263536000
I0422 02:54:35.640804  3275 layer_factory.hpp:77] Creating layer relu5
I0422 02:54:35.640817  3275 net.cpp:91] Creating Layer relu5
I0422 02:54:35.640828  3275 net.cpp:425] relu5 <- conv5
I0422 02:54:35.640841  3275 net.cpp:386] relu5 -> conv5 (in-place)
I0422 02:54:35.645745  3275 net.cpp:141] Setting up relu5
I0422 02:54:35.645768  3275 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 02:54:35.645772  3275 net.cpp:156] Memory required for data: 1283360640
I0422 02:54:35.645778  3275 layer_factory.hpp:77] Creating layer pool5
I0422 02:54:35.645789  3275 net.cpp:91] Creating Layer pool5
I0422 02:54:35.645795  3275 net.cpp:425] pool5 <- conv5
I0422 02:54:35.645805  3275 net.cpp:399] pool5 -> pool5
I0422 02:54:35.645891  3275 net.cpp:141] Setting up pool5
I0422 02:54:35.645901  3275 net.cpp:148] Top shape: 10 256 22 22 (1239040)
I0422 02:54:35.645905  3275 net.cpp:156] Memory required for data: 1288316800
I0422 02:54:35.645910  3275 layer_factory.hpp:77] Creating layer fcc6
I0422 02:54:35.645928  3275 net.cpp:91] Creating Layer fcc6
I0422 02:54:35.645934  3275 net.cpp:425] fcc6 <- pool5
I0422 02:54:35.645941  3275 net.cpp:399] fcc6 -> fcc6
I0422 02:54:36.130867  3275 net.cpp:141] Setting up fcc6
I0422 02:54:36.130910  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:36.130915  3275 net.cpp:156] Memory required for data: 1335666560
I0422 02:54:36.130928  3275 layer_factory.hpp:77] Creating layer relu6
I0422 02:54:36.130941  3275 net.cpp:91] Creating Layer relu6
I0422 02:54:36.130947  3275 net.cpp:425] relu6 <- fcc6
I0422 02:54:36.130959  3275 net.cpp:386] relu6 -> fcc6 (in-place)
I0422 02:54:36.132666  3275 net.cpp:141] Setting up relu6
I0422 02:54:36.132678  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:36.132681  3275 net.cpp:156] Memory required for data: 1383016320
I0422 02:54:36.132686  3275 layer_factory.hpp:77] Creating layer drop6
I0422 02:54:36.132700  3275 net.cpp:91] Creating Layer drop6
I0422 02:54:36.132705  3275 net.cpp:425] drop6 <- fcc6
I0422 02:54:36.132714  3275 net.cpp:386] drop6 -> fcc6 (in-place)
I0422 02:54:36.132766  3275 net.cpp:141] Setting up drop6
I0422 02:54:36.132776  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:36.132779  3275 net.cpp:156] Memory required for data: 1430366080
I0422 02:54:36.132784  3275 layer_factory.hpp:77] Creating layer fcc7
I0422 02:54:36.132802  3275 net.cpp:91] Creating Layer fcc7
I0422 02:54:36.132807  3275 net.cpp:425] fcc7 <- fcc6
I0422 02:54:36.132814  3275 net.cpp:399] fcc7 -> fcc7
I0422 02:54:36.363464  3275 net.cpp:141] Setting up fcc7
I0422 02:54:36.363508  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:36.363514  3275 net.cpp:156] Memory required for data: 1477715840
I0422 02:54:36.363528  3275 layer_factory.hpp:77] Creating layer relu7
I0422 02:54:36.363541  3275 net.cpp:91] Creating Layer relu7
I0422 02:54:36.363548  3275 net.cpp:425] relu7 <- fcc7
I0422 02:54:36.363559  3275 net.cpp:386] relu7 -> fcc7 (in-place)
I0422 02:54:36.369340  3275 net.cpp:141] Setting up relu7
I0422 02:54:36.369357  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:36.369361  3275 net.cpp:156] Memory required for data: 1525065600
I0422 02:54:36.369366  3275 layer_factory.hpp:77] Creating layer drop7
I0422 02:54:36.369379  3275 net.cpp:91] Creating Layer drop7
I0422 02:54:36.369385  3275 net.cpp:425] drop7 <- fcc7
I0422 02:54:36.369390  3275 net.cpp:386] drop7 -> fcc7 (in-place)
I0422 02:54:36.369443  3275 net.cpp:141] Setting up drop7
I0422 02:54:36.369454  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:36.369458  3275 net.cpp:156] Memory required for data: 1572415360
I0422 02:54:36.369463  3275 layer_factory.hpp:77] Creating layer score-fr
I0422 02:54:36.369477  3275 net.cpp:91] Creating Layer score-fr
I0422 02:54:36.369482  3275 net.cpp:425] score-fr <- fcc7
I0422 02:54:36.369493  3275 net.cpp:399] score-fr -> score-fcc7
I0422 02:54:36.370615  3275 net.cpp:141] Setting up score-fr
I0422 02:54:36.370626  3275 net.cpp:148] Top shape: 10 16 17 17 (46240)
I0422 02:54:36.370630  3275 net.cpp:156] Memory required for data: 1572600320
I0422 02:54:36.370638  3275 layer_factory.hpp:77] Creating layer upsample
I0422 02:54:36.370656  3275 net.cpp:91] Creating Layer upsample
I0422 02:54:36.370661  3275 net.cpp:425] upsample <- score-fcc7
I0422 02:54:36.370667  3275 net.cpp:399] upsample -> bigscore
I0422 02:54:36.372690  3275 net.cpp:141] Setting up upsample
I0422 02:54:36.372709  3275 net.cpp:148] Top shape: 10 16 575 575 (52900000)
I0422 02:54:36.372714  3275 net.cpp:156] Memory required for data: 1784200320
I0422 02:54:36.372731  3275 layer_factory.hpp:77] Creating layer crop
I0422 02:54:36.372745  3275 net.cpp:91] Creating Layer crop
I0422 02:54:36.372750  3275 net.cpp:425] crop <- bigscore
I0422 02:54:36.372756  3275 net.cpp:425] crop <- data_data_0_split_2
I0422 02:54:36.372762  3275 net.cpp:399] crop -> score
I0422 02:54:36.372817  3275 net.cpp:141] Setting up crop
I0422 02:54:36.372829  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:36.372833  3275 net.cpp:156] Memory required for data: 1951972480
I0422 02:54:36.372838  3275 layer_factory.hpp:77] Creating layer balance
I0422 02:54:36.372846  3275 net.cpp:91] Creating Layer balance
I0422 02:54:36.372851  3275 net.cpp:425] balance <- score
I0422 02:54:36.372858  3275 net.cpp:425] balance <- jointMap_jointMap_0_split_0
I0422 02:54:36.372864  3275 net.cpp:399] balance -> bscore
I0422 02:54:36.372926  3275 net.cpp:141] Setting up balance
I0422 02:54:36.372936  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:36.372939  3275 net.cpp:156] Memory required for data: 2119744640
I0422 02:54:36.372943  3275 layer_factory.hpp:77] Creating layer loss
I0422 02:54:36.372954  3275 net.cpp:91] Creating Layer loss
I0422 02:54:36.372959  3275 net.cpp:425] loss <- bscore
I0422 02:54:36.372964  3275 net.cpp:425] loss <- jointMap_jointMap_0_split_1
I0422 02:54:36.372972  3275 net.cpp:399] loss -> loss
I0422 02:54:36.373034  3275 net.cpp:141] Setting up loss
I0422 02:54:36.373042  3275 net.cpp:148] Top shape: (1)
I0422 02:54:36.373046  3275 net.cpp:151]     with loss weight 1
I0422 02:54:36.373075  3275 net.cpp:156] Memory required for data: 2119744644
I0422 02:54:36.373080  3275 net.cpp:217] loss needs backward computation.
I0422 02:54:36.373085  3275 net.cpp:217] balance needs backward computation.
I0422 02:54:36.373090  3275 net.cpp:217] crop needs backward computation.
I0422 02:54:36.373095  3275 net.cpp:217] upsample needs backward computation.
I0422 02:54:36.373098  3275 net.cpp:217] score-fr needs backward computation.
I0422 02:54:36.373102  3275 net.cpp:217] drop7 needs backward computation.
I0422 02:54:36.373107  3275 net.cpp:217] relu7 needs backward computation.
I0422 02:54:36.373111  3275 net.cpp:217] fcc7 needs backward computation.
I0422 02:54:36.373116  3275 net.cpp:217] drop6 needs backward computation.
I0422 02:54:36.373119  3275 net.cpp:217] relu6 needs backward computation.
I0422 02:54:36.373123  3275 net.cpp:217] fcc6 needs backward computation.
I0422 02:54:36.373127  3275 net.cpp:217] pool5 needs backward computation.
I0422 02:54:36.373131  3275 net.cpp:217] relu5 needs backward computation.
I0422 02:54:36.373136  3275 net.cpp:217] cconv5 needs backward computation.
I0422 02:54:36.373139  3275 net.cpp:217] relu4 needs backward computation.
I0422 02:54:36.373143  3275 net.cpp:217] cconv4 needs backward computation.
I0422 02:54:36.373148  3275 net.cpp:217] relu3 needs backward computation.
I0422 02:54:36.373152  3275 net.cpp:217] conv3 needs backward computation.
I0422 02:54:36.373157  3275 net.cpp:217] norm2 needs backward computation.
I0422 02:54:36.373160  3275 net.cpp:217] pool2 needs backward computation.
I0422 02:54:36.373164  3275 net.cpp:217] relu2 needs backward computation.
I0422 02:54:36.373168  3275 net.cpp:217] conv2 needs backward computation.
I0422 02:54:36.373172  3275 net.cpp:217] norm1 needs backward computation.
I0422 02:54:36.373177  3275 net.cpp:217] pool1 needs backward computation.
I0422 02:54:36.373181  3275 net.cpp:217] relu1 needs backward computation.
I0422 02:54:36.373185  3275 net.cpp:217] conv1 needs backward computation.
I0422 02:54:36.373189  3275 net.cpp:219] jointMap_jointMap_0_split does not need backward computation.
I0422 02:54:36.373198  3275 net.cpp:219] jointMap does not need backward computation.
I0422 02:54:36.373208  3275 net.cpp:219] data_data_0_split does not need backward computation.
I0422 02:54:36.373213  3275 net.cpp:219] data does not need backward computation.
I0422 02:54:36.373216  3275 net.cpp:261] This network produces output loss
I0422 02:54:36.373236  3275 net.cpp:274] Network initialization done.
I0422 02:54:36.374650  3275 solver.cpp:181] Creating test net (#0) specified by net file: fcn_alxnet_train_val.prototxt
I0422 02:54:36.374702  3275 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0422 02:54:36.375015  3275 net.cpp:49] Initializing net from parameters: 
name: "FCN-AlexNet-joint"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "jointLocation"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 104
    mean_value: 116
    mean_value: 122
  }
  image_data_param {
    source: "/home/mengxin1/HumanPoseDetect/preprocess/val_label_resized.txt"
    batch_size: 10
    root_folder: "/home/mengxin1/mpii_human_pose_v1_images_resized/"
    label_num: 48
  }
}
layer {
  name: "jointMap"
  type: "PositionToMap"
  bottom: "data"
  bottom: "jointLocation"
  top: "jointMap"
  gaussian_param {
    std: 25
    half_window_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 100
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "cconv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "cconv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fcc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fcc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 6
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fcc6"
  top: "fcc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fcc6"
  top: "fcc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fcc7"
  type: "Convolution"
  bottom: "fcc6"
  top: "fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fcc7"
  top: "fcc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fcc7"
  top: "fcc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score-fr"
  type: "Convolution"
  bottom: "fcc7"
  top: "score-fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    engine: CAFFE
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "score-fcc7"
  top: "bigscore"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 63
    group: 16
    stride: 32
  }
}
layer {
  name: "crop"
  type: "Crop"
  bottom: "bigscore"
  bottom: "data"
  top: "score"
}
layer {
  name: "balance"
  type: "Balance"
  bottom: "score"
  bottom: "jointMap"
  top: "bscore"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "bscore"
  bottom: "jointMap"
  top: "loss"
}
I0422 02:54:36.375150  3275 layer_factory.hpp:77] Creating layer data
I0422 02:54:36.375169  3275 net.cpp:91] Creating Layer data
I0422 02:54:36.375175  3275 net.cpp:399] data -> data
I0422 02:54:36.375186  3275 net.cpp:399] data -> jointLocation
I0422 02:54:36.375197  3275 image_data_layer.cpp:40] Opening file /home/mengxin1/HumanPoseDetect/preprocess/val_label_resized.txt
I0422 02:54:36.402673  3275 image_data_layer.cpp:62] A total of 2424 images.
I0422 02:54:36.409026  3275 image_data_layer.cpp:89] output data size: 10,3,512,512
I0422 02:54:36.490559  3275 net.cpp:141] Setting up data
I0422 02:54:36.490605  3275 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 02:54:36.490614  3275 net.cpp:148] Top shape: 10 48 1 1 (480)
I0422 02:54:36.490618  3275 net.cpp:156] Memory required for data: 31459200
I0422 02:54:36.490628  3275 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 02:54:36.490645  3275 net.cpp:91] Creating Layer data_data_0_split
I0422 02:54:36.490651  3275 net.cpp:425] data_data_0_split <- data
I0422 02:54:36.490663  3275 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 02:54:36.490677  3275 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 02:54:36.490685  3275 net.cpp:399] data_data_0_split -> data_data_0_split_2
I0422 02:54:36.490820  3275 net.cpp:141] Setting up data_data_0_split
I0422 02:54:36.490830  3275 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 02:54:36.490835  3275 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 02:54:36.490840  3275 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 02:54:36.490844  3275 net.cpp:156] Memory required for data: 125831040
I0422 02:54:36.490849  3275 layer_factory.hpp:77] Creating layer jointMap
I0422 02:54:36.490864  3275 net.cpp:91] Creating Layer jointMap
I0422 02:54:36.490869  3275 net.cpp:425] jointMap <- data_data_0_split_0
I0422 02:54:36.490875  3275 net.cpp:425] jointMap <- jointLocation
I0422 02:54:36.490882  3275 net.cpp:399] jointMap -> jointMap
I0422 02:54:36.490922  3275 net.cpp:141] Setting up jointMap
I0422 02:54:36.490931  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:36.490936  3275 net.cpp:156] Memory required for data: 293603200
I0422 02:54:36.490941  3275 layer_factory.hpp:77] Creating layer jointMap_jointMap_0_split
I0422 02:54:36.490947  3275 net.cpp:91] Creating Layer jointMap_jointMap_0_split
I0422 02:54:36.490952  3275 net.cpp:425] jointMap_jointMap_0_split <- jointMap
I0422 02:54:36.490958  3275 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_0
I0422 02:54:36.490967  3275 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_1
I0422 02:54:36.491025  3275 net.cpp:141] Setting up jointMap_jointMap_0_split
I0422 02:54:36.491034  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:36.491039  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:36.491042  3275 net.cpp:156] Memory required for data: 629147520
I0422 02:54:36.491047  3275 layer_factory.hpp:77] Creating layer conv1
I0422 02:54:36.491065  3275 net.cpp:91] Creating Layer conv1
I0422 02:54:36.491070  3275 net.cpp:425] conv1 <- data_data_0_split_1
I0422 02:54:36.491078  3275 net.cpp:399] conv1 -> conv1
I0422 02:54:36.516839  3275 net.cpp:141] Setting up conv1
I0422 02:54:36.516891  3275 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 02:54:36.516896  3275 net.cpp:156] Memory required for data: 748095360
I0422 02:54:36.516916  3275 layer_factory.hpp:77] Creating layer relu1
I0422 02:54:36.516932  3275 net.cpp:91] Creating Layer relu1
I0422 02:54:36.516937  3275 net.cpp:425] relu1 <- conv1
I0422 02:54:36.516947  3275 net.cpp:386] relu1 -> conv1 (in-place)
I0422 02:54:36.517504  3275 net.cpp:141] Setting up relu1
I0422 02:54:36.517521  3275 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 02:54:36.517529  3275 net.cpp:156] Memory required for data: 867043200
I0422 02:54:36.517534  3275 layer_factory.hpp:77] Creating layer pool1
I0422 02:54:36.517545  3275 net.cpp:91] Creating Layer pool1
I0422 02:54:36.517552  3275 net.cpp:425] pool1 <- conv1
I0422 02:54:36.517560  3275 net.cpp:399] pool1 -> pool1
I0422 02:54:36.517637  3275 net.cpp:141] Setting up pool1
I0422 02:54:36.517647  3275 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 02:54:36.517650  3275 net.cpp:156] Memory required for data: 896780160
I0422 02:54:36.517655  3275 layer_factory.hpp:77] Creating layer norm1
I0422 02:54:36.517665  3275 net.cpp:91] Creating Layer norm1
I0422 02:54:36.517670  3275 net.cpp:425] norm1 <- pool1
I0422 02:54:36.517678  3275 net.cpp:399] norm1 -> norm1
I0422 02:54:36.518656  3275 net.cpp:141] Setting up norm1
I0422 02:54:36.518668  3275 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 02:54:36.518672  3275 net.cpp:156] Memory required for data: 926517120
I0422 02:54:36.518676  3275 layer_factory.hpp:77] Creating layer conv2
I0422 02:54:36.518697  3275 net.cpp:91] Creating Layer conv2
I0422 02:54:36.518702  3275 net.cpp:425] conv2 <- norm1
I0422 02:54:36.518710  3275 net.cpp:399] conv2 -> conv2
I0422 02:54:36.548796  3275 net.cpp:141] Setting up conv2
I0422 02:54:36.548841  3275 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 02:54:36.548846  3275 net.cpp:156] Memory required for data: 1005815680
I0422 02:54:36.548864  3275 layer_factory.hpp:77] Creating layer relu2
I0422 02:54:36.548878  3275 net.cpp:91] Creating Layer relu2
I0422 02:54:36.548884  3275 net.cpp:425] relu2 <- conv2
I0422 02:54:36.548893  3275 net.cpp:386] relu2 -> conv2 (in-place)
I0422 02:54:36.551919  3275 net.cpp:141] Setting up relu2
I0422 02:54:36.551937  3275 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 02:54:36.551941  3275 net.cpp:156] Memory required for data: 1085114240
I0422 02:54:36.551947  3275 layer_factory.hpp:77] Creating layer pool2
I0422 02:54:36.551967  3275 net.cpp:91] Creating Layer pool2
I0422 02:54:36.551972  3275 net.cpp:425] pool2 <- conv2
I0422 02:54:36.551981  3275 net.cpp:399] pool2 -> pool2
I0422 02:54:36.552067  3275 net.cpp:141] Setting up pool2
I0422 02:54:36.552076  3275 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 02:54:36.552080  3275 net.cpp:156] Memory required for data: 1104938880
I0422 02:54:36.552084  3275 layer_factory.hpp:77] Creating layer norm2
I0422 02:54:36.552094  3275 net.cpp:91] Creating Layer norm2
I0422 02:54:36.552099  3275 net.cpp:425] norm2 <- pool2
I0422 02:54:36.552106  3275 net.cpp:399] norm2 -> norm2
I0422 02:54:36.559751  3275 net.cpp:141] Setting up norm2
I0422 02:54:36.559777  3275 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 02:54:36.559782  3275 net.cpp:156] Memory required for data: 1124763520
I0422 02:54:36.559788  3275 layer_factory.hpp:77] Creating layer conv3
I0422 02:54:36.559805  3275 net.cpp:91] Creating Layer conv3
I0422 02:54:36.559811  3275 net.cpp:425] conv3 <- norm2
I0422 02:54:36.559820  3275 net.cpp:399] conv3 -> conv3
I0422 02:54:36.587466  3275 net.cpp:141] Setting up conv3
I0422 02:54:36.587512  3275 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 02:54:36.587517  3275 net.cpp:156] Memory required for data: 1154500480
I0422 02:54:36.587537  3275 layer_factory.hpp:77] Creating layer relu3
I0422 02:54:36.587551  3275 net.cpp:91] Creating Layer relu3
I0422 02:54:36.587558  3275 net.cpp:425] relu3 <- conv3
I0422 02:54:36.587568  3275 net.cpp:386] relu3 -> conv3 (in-place)
I0422 02:54:36.594389  3275 net.cpp:141] Setting up relu3
I0422 02:54:36.594414  3275 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 02:54:36.594419  3275 net.cpp:156] Memory required for data: 1184237440
I0422 02:54:36.594424  3275 layer_factory.hpp:77] Creating layer cconv4
I0422 02:54:36.594442  3275 net.cpp:91] Creating Layer cconv4
I0422 02:54:36.594449  3275 net.cpp:425] cconv4 <- conv3
I0422 02:54:36.594457  3275 net.cpp:399] cconv4 -> conv4
I0422 02:54:36.645169  3275 net.cpp:141] Setting up cconv4
I0422 02:54:36.645220  3275 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 02:54:36.645225  3275 net.cpp:156] Memory required for data: 1213974400
I0422 02:54:36.645237  3275 layer_factory.hpp:77] Creating layer relu4
I0422 02:54:36.645251  3275 net.cpp:91] Creating Layer relu4
I0422 02:54:36.645257  3275 net.cpp:425] relu4 <- conv4
I0422 02:54:36.645265  3275 net.cpp:386] relu4 -> conv4 (in-place)
I0422 02:54:36.668874  3275 net.cpp:141] Setting up relu4
I0422 02:54:36.668920  3275 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 02:54:36.668925  3275 net.cpp:156] Memory required for data: 1243711360
I0422 02:54:36.668932  3275 layer_factory.hpp:77] Creating layer cconv5
I0422 02:54:36.668964  3275 net.cpp:91] Creating Layer cconv5
I0422 02:54:36.668972  3275 net.cpp:425] cconv5 <- conv4
I0422 02:54:36.668985  3275 net.cpp:399] cconv5 -> conv5
I0422 02:54:36.724036  3275 net.cpp:141] Setting up cconv5
I0422 02:54:36.724087  3275 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 02:54:36.724092  3275 net.cpp:156] Memory required for data: 1263536000
I0422 02:54:36.724112  3275 layer_factory.hpp:77] Creating layer relu5
I0422 02:54:36.724126  3275 net.cpp:91] Creating Layer relu5
I0422 02:54:36.724133  3275 net.cpp:425] relu5 <- conv5
I0422 02:54:36.724143  3275 net.cpp:386] relu5 -> conv5 (in-place)
I0422 02:54:36.730715  3275 net.cpp:141] Setting up relu5
I0422 02:54:36.730744  3275 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 02:54:36.730749  3275 net.cpp:156] Memory required for data: 1283360640
I0422 02:54:36.730754  3275 layer_factory.hpp:77] Creating layer pool5
I0422 02:54:36.730767  3275 net.cpp:91] Creating Layer pool5
I0422 02:54:36.730773  3275 net.cpp:425] pool5 <- conv5
I0422 02:54:36.730782  3275 net.cpp:399] pool5 -> pool5
I0422 02:54:36.730886  3275 net.cpp:141] Setting up pool5
I0422 02:54:36.730898  3275 net.cpp:148] Top shape: 10 256 22 22 (1239040)
I0422 02:54:36.730902  3275 net.cpp:156] Memory required for data: 1288316800
I0422 02:54:36.730913  3275 layer_factory.hpp:77] Creating layer fcc6
I0422 02:54:36.730929  3275 net.cpp:91] Creating Layer fcc6
I0422 02:54:36.730934  3275 net.cpp:425] fcc6 <- pool5
I0422 02:54:36.730944  3275 net.cpp:399] fcc6 -> fcc6
I0422 02:54:37.214637  3275 net.cpp:141] Setting up fcc6
I0422 02:54:37.214679  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:37.214684  3275 net.cpp:156] Memory required for data: 1335666560
I0422 02:54:37.214696  3275 layer_factory.hpp:77] Creating layer relu6
I0422 02:54:37.214714  3275 net.cpp:91] Creating Layer relu6
I0422 02:54:37.214721  3275 net.cpp:425] relu6 <- fcc6
I0422 02:54:37.214730  3275 net.cpp:386] relu6 -> fcc6 (in-place)
I0422 02:54:37.216704  3275 net.cpp:141] Setting up relu6
I0422 02:54:37.216717  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:37.216722  3275 net.cpp:156] Memory required for data: 1383016320
I0422 02:54:37.216727  3275 layer_factory.hpp:77] Creating layer drop6
I0422 02:54:37.216737  3275 net.cpp:91] Creating Layer drop6
I0422 02:54:37.216742  3275 net.cpp:425] drop6 <- fcc6
I0422 02:54:37.216749  3275 net.cpp:386] drop6 -> fcc6 (in-place)
I0422 02:54:37.216799  3275 net.cpp:141] Setting up drop6
I0422 02:54:37.216807  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:37.216811  3275 net.cpp:156] Memory required for data: 1430366080
I0422 02:54:37.216816  3275 layer_factory.hpp:77] Creating layer fcc7
I0422 02:54:37.216833  3275 net.cpp:91] Creating Layer fcc7
I0422 02:54:37.216838  3275 net.cpp:425] fcc7 <- fcc6
I0422 02:54:37.216846  3275 net.cpp:399] fcc7 -> fcc7
I0422 02:54:37.464002  3275 net.cpp:141] Setting up fcc7
I0422 02:54:37.464052  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:37.464058  3275 net.cpp:156] Memory required for data: 1477715840
I0422 02:54:37.464072  3275 layer_factory.hpp:77] Creating layer relu7
I0422 02:54:37.464088  3275 net.cpp:91] Creating Layer relu7
I0422 02:54:37.464095  3275 net.cpp:425] relu7 <- fcc7
I0422 02:54:37.464107  3275 net.cpp:386] relu7 -> fcc7 (in-place)
I0422 02:54:37.464324  3275 net.cpp:141] Setting up relu7
I0422 02:54:37.464335  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:37.464340  3275 net.cpp:156] Memory required for data: 1525065600
I0422 02:54:37.464344  3275 layer_factory.hpp:77] Creating layer drop7
I0422 02:54:37.464357  3275 net.cpp:91] Creating Layer drop7
I0422 02:54:37.464362  3275 net.cpp:425] drop7 <- fcc7
I0422 02:54:37.464368  3275 net.cpp:386] drop7 -> fcc7 (in-place)
I0422 02:54:37.464416  3275 net.cpp:141] Setting up drop7
I0422 02:54:37.464426  3275 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 02:54:37.464457  3275 net.cpp:156] Memory required for data: 1572415360
I0422 02:54:37.464462  3275 layer_factory.hpp:77] Creating layer score-fr
I0422 02:54:37.464478  3275 net.cpp:91] Creating Layer score-fr
I0422 02:54:37.464483  3275 net.cpp:425] score-fr <- fcc7
I0422 02:54:37.464493  3275 net.cpp:399] score-fr -> score-fcc7
I0422 02:54:37.466745  3275 net.cpp:141] Setting up score-fr
I0422 02:54:37.466758  3275 net.cpp:148] Top shape: 10 16 17 17 (46240)
I0422 02:54:37.466763  3275 net.cpp:156] Memory required for data: 1572600320
I0422 02:54:37.466771  3275 layer_factory.hpp:77] Creating layer upsample
I0422 02:54:37.466784  3275 net.cpp:91] Creating Layer upsample
I0422 02:54:37.466789  3275 net.cpp:425] upsample <- score-fcc7
I0422 02:54:37.466799  3275 net.cpp:399] upsample -> bigscore
I0422 02:54:37.468713  3275 net.cpp:141] Setting up upsample
I0422 02:54:37.468726  3275 net.cpp:148] Top shape: 10 16 575 575 (52900000)
I0422 02:54:37.468730  3275 net.cpp:156] Memory required for data: 1784200320
I0422 02:54:37.468749  3275 layer_factory.hpp:77] Creating layer crop
I0422 02:54:37.468757  3275 net.cpp:91] Creating Layer crop
I0422 02:54:37.468762  3275 net.cpp:425] crop <- bigscore
I0422 02:54:37.468768  3275 net.cpp:425] crop <- data_data_0_split_2
I0422 02:54:37.468777  3275 net.cpp:399] crop -> score
I0422 02:54:37.468829  3275 net.cpp:141] Setting up crop
I0422 02:54:37.468838  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:37.468847  3275 net.cpp:156] Memory required for data: 1951972480
I0422 02:54:37.468852  3275 layer_factory.hpp:77] Creating layer balance
I0422 02:54:37.468861  3275 net.cpp:91] Creating Layer balance
I0422 02:54:37.468866  3275 net.cpp:425] balance <- score
I0422 02:54:37.468871  3275 net.cpp:425] balance <- jointMap_jointMap_0_split_0
I0422 02:54:37.468878  3275 net.cpp:399] balance -> bscore
I0422 02:54:37.468950  3275 net.cpp:141] Setting up balance
I0422 02:54:37.468960  3275 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 02:54:37.468963  3275 net.cpp:156] Memory required for data: 2119744640
I0422 02:54:37.468967  3275 layer_factory.hpp:77] Creating layer loss
I0422 02:54:37.468976  3275 net.cpp:91] Creating Layer loss
I0422 02:54:37.468979  3275 net.cpp:425] loss <- bscore
I0422 02:54:37.468986  3275 net.cpp:425] loss <- jointMap_jointMap_0_split_1
I0422 02:54:37.468993  3275 net.cpp:399] loss -> loss
I0422 02:54:37.469063  3275 net.cpp:141] Setting up loss
I0422 02:54:37.469074  3275 net.cpp:148] Top shape: (1)
I0422 02:54:37.469079  3275 net.cpp:151]     with loss weight 1
I0422 02:54:37.469094  3275 net.cpp:156] Memory required for data: 2119744644
I0422 02:54:37.469099  3275 net.cpp:217] loss needs backward computation.
I0422 02:54:37.469104  3275 net.cpp:217] balance needs backward computation.
I0422 02:54:37.469107  3275 net.cpp:217] crop needs backward computation.
I0422 02:54:37.469112  3275 net.cpp:217] upsample needs backward computation.
I0422 02:54:37.469116  3275 net.cpp:217] score-fr needs backward computation.
I0422 02:54:37.469120  3275 net.cpp:217] drop7 needs backward computation.
I0422 02:54:37.469125  3275 net.cpp:217] relu7 needs backward computation.
I0422 02:54:37.469128  3275 net.cpp:217] fcc7 needs backward computation.
I0422 02:54:37.469132  3275 net.cpp:217] drop6 needs backward computation.
I0422 02:54:37.469136  3275 net.cpp:217] relu6 needs backward computation.
I0422 02:54:37.469141  3275 net.cpp:217] fcc6 needs backward computation.
I0422 02:54:37.469144  3275 net.cpp:217] pool5 needs backward computation.
I0422 02:54:37.469149  3275 net.cpp:217] relu5 needs backward computation.
I0422 02:54:37.469153  3275 net.cpp:217] cconv5 needs backward computation.
I0422 02:54:37.469157  3275 net.cpp:217] relu4 needs backward computation.
I0422 02:54:37.469161  3275 net.cpp:217] cconv4 needs backward computation.
I0422 02:54:37.469166  3275 net.cpp:217] relu3 needs backward computation.
I0422 02:54:37.469169  3275 net.cpp:217] conv3 needs backward computation.
I0422 02:54:37.469173  3275 net.cpp:217] norm2 needs backward computation.
I0422 02:54:37.469177  3275 net.cpp:217] pool2 needs backward computation.
I0422 02:54:37.469182  3275 net.cpp:217] relu2 needs backward computation.
I0422 02:54:37.469185  3275 net.cpp:217] conv2 needs backward computation.
I0422 02:54:37.469190  3275 net.cpp:217] norm1 needs backward computation.
I0422 02:54:37.469194  3275 net.cpp:217] pool1 needs backward computation.
I0422 02:54:37.469198  3275 net.cpp:217] relu1 needs backward computation.
I0422 02:54:37.469202  3275 net.cpp:217] conv1 needs backward computation.
I0422 02:54:37.469208  3275 net.cpp:219] jointMap_jointMap_0_split does not need backward computation.
I0422 02:54:37.469211  3275 net.cpp:219] jointMap does not need backward computation.
I0422 02:54:37.469218  3275 net.cpp:219] data_data_0_split does not need backward computation.
I0422 02:54:37.469223  3275 net.cpp:219] data does not need backward computation.
I0422 02:54:37.469226  3275 net.cpp:261] This network produces output loss
I0422 02:54:37.469249  3275 net.cpp:274] Network initialization done.
I0422 02:54:37.469362  3275 solver.cpp:60] Solver scaffolding done.
I0422 02:54:38.637033  3275 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: /home/mengxin1/HumanPoseDetect/fcn_joint_predict/bvlc_reference_caffenet.caffemodel
I0422 02:54:38.637076  3275 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.
W0422 02:54:38.637087  3275 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0422 02:54:38.637091  3275 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/mengxin1/HumanPoseDetect/fcn_joint_predict/bvlc_reference_caffenet.caffemodel
I0422 02:54:40.650707  3275 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I0422 02:54:40.662220  3275 net.cpp:752] Ignoring source layer conv4
I0422 02:54:40.662250  3275 net.cpp:752] Ignoring source layer conv5
I0422 02:54:40.662256  3275 net.cpp:752] Ignoring source layer fc6
I0422 02:54:40.662261  3275 net.cpp:752] Ignoring source layer fc7
I0422 02:54:40.662264  3275 net.cpp:752] Ignoring source layer fc8
I0422 02:54:40.683670  3275 solver.cpp:337] Iteration 0, Testing net (#0)
I0422 02:55:30.671983  3275 solver.cpp:404]     Test net output #0: loss = 8.57076 (* 1 = 8.57076 loss)
I0422 02:55:33.717705  3275 solver.cpp:228] Iteration 0, loss = 9.46368
I0422 02:55:33.717754  3275 solver.cpp:244]     Train net output #0: loss = 9.46368 (* 1 = 9.46368 loss)
I0422 02:55:33.717767  3275 sgd_solver.cpp:106] Iteration 0, lr = 0.001
I0422 02:57:12.282896  3275 solver.cpp:228] Iteration 20, loss = nan
I0422 02:57:12.283323  3275 solver.cpp:244]     Train net output #0: loss = nan (* 1 = nan loss)
I0422 02:57:12.283336  3275 sgd_solver.cpp:106] Iteration 20, lr = 0.001
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0422 03:00:22.976164  7583 solver.cpp:48] Initializing solver from parameters: 
test_iter: 25
test_interval: 1000
base_lr: 0.0008
display: 20
max_iter: 100000
lr_policy: "step"
gamma: 0.3
momentum: 0.9
weight_decay: 0.0005
stepsize: 20000
snapshot: 10000
snapshot_prefix: "models/model"
net: "fcn_alxnet_train_val.prototxt"
I0422 03:00:22.988097  7583 solver.cpp:91] Creating training net from net file: fcn_alxnet_train_val.prototxt
I0422 03:00:22.988973  7583 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0422 03:00:22.989274  7583 net.cpp:49] Initializing net from parameters: 
name: "FCN-AlexNet-joint"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "jointLocation"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 104
    mean_value: 116
    mean_value: 122
  }
  image_data_param {
    source: "/home/mengxin1/HumanPoseDetect/preprocess/train_label_resized.txt"
    batch_size: 10
    shuffle: true
    root_folder: "/home/mengxin1/mpii_human_pose_v1_images_resized/"
    label_num: 48
  }
}
layer {
  name: "jointMap"
  type: "PositionToMap"
  bottom: "data"
  bottom: "jointLocation"
  top: "jointMap"
  gaussian_param {
    std: 25
    half_window_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 100
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "cconv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "cconv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fcc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fcc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 6
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fcc6"
  top: "fcc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fcc6"
  top: "fcc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fcc7"
  type: "Convolution"
  bottom: "fcc6"
  top: "fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fcc7"
  top: "fcc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fcc7"
  top: "fcc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score-fr"
  type: "Convolution"
  bottom: "fcc7"
  top: "score-fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    engine: CAFFE
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "score-fcc7"
  top: "bigscore"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 63
    group: 16
    stride: 32
  }
}
layer {
  name: "crop"
  type: "Crop"
  bottom: "bigscore"
  bottom: "data"
  top: "score"
}
layer {
  name: "balance"
  type: "Balance"
  bottom: "score"
  bottom: "jointMap"
  top: "bscore"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "bscore"
  bottom: "jointMap"
  top: "loss"
}
I0422 03:00:22.989415  7583 layer_factory.hpp:77] Creating layer data
I0422 03:00:22.989445  7583 net.cpp:91] Creating Layer data
I0422 03:00:22.989455  7583 net.cpp:399] data -> data
I0422 03:00:22.989472  7583 net.cpp:399] data -> jointLocation
I0422 03:00:22.989491  7583 image_data_layer.cpp:40] Opening file /home/mengxin1/HumanPoseDetect/preprocess/train_label_resized.txt
I0422 03:00:23.089460  7583 image_data_layer.cpp:57] Shuffling data
I0422 03:00:23.091749  7583 image_data_layer.cpp:62] A total of 9580 images.
I0422 03:00:23.218565  7583 image_data_layer.cpp:89] output data size: 10,3,512,512
I0422 03:00:23.327669  7583 net.cpp:141] Setting up data
I0422 03:00:23.327725  7583 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 03:00:23.327733  7583 net.cpp:148] Top shape: 10 48 1 1 (480)
I0422 03:00:23.327738  7583 net.cpp:156] Memory required for data: 31459200
I0422 03:00:23.327749  7583 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 03:00:23.327769  7583 net.cpp:91] Creating Layer data_data_0_split
I0422 03:00:23.327775  7583 net.cpp:425] data_data_0_split <- data
I0422 03:00:23.327785  7583 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 03:00:23.327800  7583 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 03:00:23.327808  7583 net.cpp:399] data_data_0_split -> data_data_0_split_2
I0422 03:00:23.327895  7583 net.cpp:141] Setting up data_data_0_split
I0422 03:00:23.327906  7583 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 03:00:23.327913  7583 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 03:00:23.327918  7583 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 03:00:23.327921  7583 net.cpp:156] Memory required for data: 125831040
I0422 03:00:23.327926  7583 layer_factory.hpp:77] Creating layer jointMap
I0422 03:00:23.327936  7583 net.cpp:91] Creating Layer jointMap
I0422 03:00:23.327941  7583 net.cpp:425] jointMap <- data_data_0_split_0
I0422 03:00:23.327947  7583 net.cpp:425] jointMap <- jointLocation
I0422 03:00:23.327955  7583 net.cpp:399] jointMap -> jointMap
I0422 03:00:23.327988  7583 net.cpp:141] Setting up jointMap
I0422 03:00:23.327997  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:23.328002  7583 net.cpp:156] Memory required for data: 293603200
I0422 03:00:23.328006  7583 layer_factory.hpp:77] Creating layer jointMap_jointMap_0_split
I0422 03:00:23.328013  7583 net.cpp:91] Creating Layer jointMap_jointMap_0_split
I0422 03:00:23.328018  7583 net.cpp:425] jointMap_jointMap_0_split <- jointMap
I0422 03:00:23.328025  7583 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_0
I0422 03:00:23.328033  7583 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_1
I0422 03:00:23.328083  7583 net.cpp:141] Setting up jointMap_jointMap_0_split
I0422 03:00:23.328090  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:23.328096  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:23.328100  7583 net.cpp:156] Memory required for data: 629147520
I0422 03:00:23.328104  7583 layer_factory.hpp:77] Creating layer conv1
I0422 03:00:23.328124  7583 net.cpp:91] Creating Layer conv1
I0422 03:00:23.328128  7583 net.cpp:425] conv1 <- data_data_0_split_1
I0422 03:00:23.328136  7583 net.cpp:399] conv1 -> conv1
I0422 03:00:23.667381  7583 net.cpp:141] Setting up conv1
I0422 03:00:23.667428  7583 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 03:00:23.667433  7583 net.cpp:156] Memory required for data: 748095360
I0422 03:00:23.667464  7583 layer_factory.hpp:77] Creating layer relu1
I0422 03:00:23.667482  7583 net.cpp:91] Creating Layer relu1
I0422 03:00:23.667489  7583 net.cpp:425] relu1 <- conv1
I0422 03:00:23.667497  7583 net.cpp:386] relu1 -> conv1 (in-place)
I0422 03:00:23.668771  7583 net.cpp:141] Setting up relu1
I0422 03:00:23.668784  7583 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 03:00:23.668788  7583 net.cpp:156] Memory required for data: 867043200
I0422 03:00:23.668793  7583 layer_factory.hpp:77] Creating layer pool1
I0422 03:00:23.668807  7583 net.cpp:91] Creating Layer pool1
I0422 03:00:23.668812  7583 net.cpp:425] pool1 <- conv1
I0422 03:00:23.668818  7583 net.cpp:399] pool1 -> pool1
I0422 03:00:23.668884  7583 net.cpp:141] Setting up pool1
I0422 03:00:23.668892  7583 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 03:00:23.668897  7583 net.cpp:156] Memory required for data: 896780160
I0422 03:00:23.668902  7583 layer_factory.hpp:77] Creating layer norm1
I0422 03:00:23.668915  7583 net.cpp:91] Creating Layer norm1
I0422 03:00:23.668920  7583 net.cpp:425] norm1 <- pool1
I0422 03:00:23.668926  7583 net.cpp:399] norm1 -> norm1
I0422 03:00:23.676923  7583 net.cpp:141] Setting up norm1
I0422 03:00:23.676936  7583 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 03:00:23.676941  7583 net.cpp:156] Memory required for data: 926517120
I0422 03:00:23.676946  7583 layer_factory.hpp:77] Creating layer conv2
I0422 03:00:23.676965  7583 net.cpp:91] Creating Layer conv2
I0422 03:00:23.676970  7583 net.cpp:425] conv2 <- norm1
I0422 03:00:23.676978  7583 net.cpp:399] conv2 -> conv2
I0422 03:00:23.714514  7583 net.cpp:141] Setting up conv2
I0422 03:00:23.714530  7583 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 03:00:23.714535  7583 net.cpp:156] Memory required for data: 1005815680
I0422 03:00:23.714546  7583 layer_factory.hpp:77] Creating layer relu2
I0422 03:00:23.714557  7583 net.cpp:91] Creating Layer relu2
I0422 03:00:23.714563  7583 net.cpp:425] relu2 <- conv2
I0422 03:00:23.714570  7583 net.cpp:386] relu2 -> conv2 (in-place)
I0422 03:00:23.715518  7583 net.cpp:141] Setting up relu2
I0422 03:00:23.715531  7583 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 03:00:23.715535  7583 net.cpp:156] Memory required for data: 1085114240
I0422 03:00:23.715540  7583 layer_factory.hpp:77] Creating layer pool2
I0422 03:00:23.715553  7583 net.cpp:91] Creating Layer pool2
I0422 03:00:23.715559  7583 net.cpp:425] pool2 <- conv2
I0422 03:00:23.715565  7583 net.cpp:399] pool2 -> pool2
I0422 03:00:23.715636  7583 net.cpp:141] Setting up pool2
I0422 03:00:23.715644  7583 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 03:00:23.715649  7583 net.cpp:156] Memory required for data: 1104938880
I0422 03:00:23.715653  7583 layer_factory.hpp:77] Creating layer norm2
I0422 03:00:23.715663  7583 net.cpp:91] Creating Layer norm2
I0422 03:00:23.715668  7583 net.cpp:425] norm2 <- pool2
I0422 03:00:23.715675  7583 net.cpp:399] norm2 -> norm2
I0422 03:00:23.733624  7583 net.cpp:141] Setting up norm2
I0422 03:00:23.733638  7583 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 03:00:23.733642  7583 net.cpp:156] Memory required for data: 1124763520
I0422 03:00:23.733647  7583 layer_factory.hpp:77] Creating layer conv3
I0422 03:00:23.733660  7583 net.cpp:91] Creating Layer conv3
I0422 03:00:23.733666  7583 net.cpp:425] conv3 <- norm2
I0422 03:00:23.733676  7583 net.cpp:399] conv3 -> conv3
I0422 03:00:23.750233  7583 net.cpp:141] Setting up conv3
I0422 03:00:23.750248  7583 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 03:00:23.750253  7583 net.cpp:156] Memory required for data: 1154500480
I0422 03:00:23.750265  7583 layer_factory.hpp:77] Creating layer relu3
I0422 03:00:23.750274  7583 net.cpp:91] Creating Layer relu3
I0422 03:00:23.750279  7583 net.cpp:425] relu3 <- conv3
I0422 03:00:23.750289  7583 net.cpp:386] relu3 -> conv3 (in-place)
I0422 03:00:23.751708  7583 net.cpp:141] Setting up relu3
I0422 03:00:23.751719  7583 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 03:00:23.751724  7583 net.cpp:156] Memory required for data: 1184237440
I0422 03:00:23.751732  7583 layer_factory.hpp:77] Creating layer cconv4
I0422 03:00:23.751749  7583 net.cpp:91] Creating Layer cconv4
I0422 03:00:23.751754  7583 net.cpp:425] cconv4 <- conv3
I0422 03:00:23.751765  7583 net.cpp:399] cconv4 -> conv4
I0422 03:00:23.766929  7583 net.cpp:141] Setting up cconv4
I0422 03:00:23.766944  7583 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 03:00:23.766949  7583 net.cpp:156] Memory required for data: 1213974400
I0422 03:00:23.766958  7583 layer_factory.hpp:77] Creating layer relu4
I0422 03:00:23.766969  7583 net.cpp:91] Creating Layer relu4
I0422 03:00:23.766974  7583 net.cpp:425] relu4 <- conv4
I0422 03:00:23.766981  7583 net.cpp:386] relu4 -> conv4 (in-place)
I0422 03:00:23.767328  7583 net.cpp:141] Setting up relu4
I0422 03:00:23.767339  7583 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 03:00:23.767344  7583 net.cpp:156] Memory required for data: 1243711360
I0422 03:00:23.767349  7583 layer_factory.hpp:77] Creating layer cconv5
I0422 03:00:23.767367  7583 net.cpp:91] Creating Layer cconv5
I0422 03:00:23.767374  7583 net.cpp:425] cconv5 <- conv4
I0422 03:00:23.767381  7583 net.cpp:399] cconv5 -> conv5
I0422 03:00:23.783462  7583 net.cpp:141] Setting up cconv5
I0422 03:00:23.783478  7583 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 03:00:23.783483  7583 net.cpp:156] Memory required for data: 1263536000
I0422 03:00:23.783494  7583 layer_factory.hpp:77] Creating layer relu5
I0422 03:00:23.783505  7583 net.cpp:91] Creating Layer relu5
I0422 03:00:23.783510  7583 net.cpp:425] relu5 <- conv5
I0422 03:00:23.783517  7583 net.cpp:386] relu5 -> conv5 (in-place)
I0422 03:00:23.785763  7583 net.cpp:141] Setting up relu5
I0422 03:00:23.785775  7583 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 03:00:23.785780  7583 net.cpp:156] Memory required for data: 1283360640
I0422 03:00:23.785785  7583 layer_factory.hpp:77] Creating layer pool5
I0422 03:00:23.785796  7583 net.cpp:91] Creating Layer pool5
I0422 03:00:23.785801  7583 net.cpp:425] pool5 <- conv5
I0422 03:00:23.785809  7583 net.cpp:399] pool5 -> pool5
I0422 03:00:23.785879  7583 net.cpp:141] Setting up pool5
I0422 03:00:23.785889  7583 net.cpp:148] Top shape: 10 256 22 22 (1239040)
I0422 03:00:23.785893  7583 net.cpp:156] Memory required for data: 1288316800
I0422 03:00:23.785898  7583 layer_factory.hpp:77] Creating layer fcc6
I0422 03:00:23.785914  7583 net.cpp:91] Creating Layer fcc6
I0422 03:00:23.785919  7583 net.cpp:425] fcc6 <- pool5
I0422 03:00:23.785929  7583 net.cpp:399] fcc6 -> fcc6
I0422 03:00:24.270293  7583 net.cpp:141] Setting up fcc6
I0422 03:00:24.270345  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:24.270351  7583 net.cpp:156] Memory required for data: 1335666560
I0422 03:00:24.270364  7583 layer_factory.hpp:77] Creating layer relu6
I0422 03:00:24.270380  7583 net.cpp:91] Creating Layer relu6
I0422 03:00:24.270386  7583 net.cpp:425] relu6 <- fcc6
I0422 03:00:24.270396  7583 net.cpp:386] relu6 -> fcc6 (in-place)
I0422 03:00:24.275899  7583 net.cpp:141] Setting up relu6
I0422 03:00:24.275912  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:24.275915  7583 net.cpp:156] Memory required for data: 1383016320
I0422 03:00:24.275920  7583 layer_factory.hpp:77] Creating layer drop6
I0422 03:00:24.275936  7583 net.cpp:91] Creating Layer drop6
I0422 03:00:24.275941  7583 net.cpp:425] drop6 <- fcc6
I0422 03:00:24.275950  7583 net.cpp:386] drop6 -> fcc6 (in-place)
I0422 03:00:24.275996  7583 net.cpp:141] Setting up drop6
I0422 03:00:24.276006  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:24.276010  7583 net.cpp:156] Memory required for data: 1430366080
I0422 03:00:24.276015  7583 layer_factory.hpp:77] Creating layer fcc7
I0422 03:00:24.276031  7583 net.cpp:91] Creating Layer fcc7
I0422 03:00:24.276036  7583 net.cpp:425] fcc7 <- fcc6
I0422 03:00:24.276046  7583 net.cpp:399] fcc7 -> fcc7
I0422 03:00:24.512492  7583 net.cpp:141] Setting up fcc7
I0422 03:00:24.512545  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:24.512557  7583 net.cpp:156] Memory required for data: 1477715840
I0422 03:00:24.512573  7583 layer_factory.hpp:77] Creating layer relu7
I0422 03:00:24.512591  7583 net.cpp:91] Creating Layer relu7
I0422 03:00:24.512598  7583 net.cpp:425] relu7 <- fcc7
I0422 03:00:24.512609  7583 net.cpp:386] relu7 -> fcc7 (in-place)
I0422 03:00:24.514444  7583 net.cpp:141] Setting up relu7
I0422 03:00:24.514472  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:24.514477  7583 net.cpp:156] Memory required for data: 1525065600
I0422 03:00:24.514483  7583 layer_factory.hpp:77] Creating layer drop7
I0422 03:00:24.514495  7583 net.cpp:91] Creating Layer drop7
I0422 03:00:24.514500  7583 net.cpp:425] drop7 <- fcc7
I0422 03:00:24.514514  7583 net.cpp:386] drop7 -> fcc7 (in-place)
I0422 03:00:24.514586  7583 net.cpp:141] Setting up drop7
I0422 03:00:24.514596  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:24.514600  7583 net.cpp:156] Memory required for data: 1572415360
I0422 03:00:24.514605  7583 layer_factory.hpp:77] Creating layer score-fr
I0422 03:00:24.514624  7583 net.cpp:91] Creating Layer score-fr
I0422 03:00:24.514631  7583 net.cpp:425] score-fr <- fcc7
I0422 03:00:24.514641  7583 net.cpp:399] score-fr -> score-fcc7
I0422 03:00:24.515799  7583 net.cpp:141] Setting up score-fr
I0422 03:00:24.515812  7583 net.cpp:148] Top shape: 10 16 17 17 (46240)
I0422 03:00:24.515817  7583 net.cpp:156] Memory required for data: 1572600320
I0422 03:00:24.515826  7583 layer_factory.hpp:77] Creating layer upsample
I0422 03:00:24.515839  7583 net.cpp:91] Creating Layer upsample
I0422 03:00:24.515844  7583 net.cpp:425] upsample <- score-fcc7
I0422 03:00:24.515853  7583 net.cpp:399] upsample -> bigscore
I0422 03:00:24.518013  7583 net.cpp:141] Setting up upsample
I0422 03:00:24.518052  7583 net.cpp:148] Top shape: 10 16 575 575 (52900000)
I0422 03:00:24.518057  7583 net.cpp:156] Memory required for data: 1784200320
I0422 03:00:24.518080  7583 layer_factory.hpp:77] Creating layer crop
I0422 03:00:24.518100  7583 net.cpp:91] Creating Layer crop
I0422 03:00:24.518107  7583 net.cpp:425] crop <- bigscore
I0422 03:00:24.518115  7583 net.cpp:425] crop <- data_data_0_split_2
I0422 03:00:24.518126  7583 net.cpp:399] crop -> score
I0422 03:00:24.518208  7583 net.cpp:141] Setting up crop
I0422 03:00:24.518218  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:24.518223  7583 net.cpp:156] Memory required for data: 1951972480
I0422 03:00:24.518226  7583 layer_factory.hpp:77] Creating layer balance
I0422 03:00:24.518236  7583 net.cpp:91] Creating Layer balance
I0422 03:00:24.518244  7583 net.cpp:425] balance <- score
I0422 03:00:24.518249  7583 net.cpp:425] balance <- jointMap_jointMap_0_split_0
I0422 03:00:24.518255  7583 net.cpp:399] balance -> bscore
I0422 03:00:24.518326  7583 net.cpp:141] Setting up balance
I0422 03:00:24.518335  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:24.518339  7583 net.cpp:156] Memory required for data: 2119744640
I0422 03:00:24.518344  7583 layer_factory.hpp:77] Creating layer loss
I0422 03:00:24.518354  7583 net.cpp:91] Creating Layer loss
I0422 03:00:24.518359  7583 net.cpp:425] loss <- bscore
I0422 03:00:24.518364  7583 net.cpp:425] loss <- jointMap_jointMap_0_split_1
I0422 03:00:24.518373  7583 net.cpp:399] loss -> loss
I0422 03:00:24.518436  7583 net.cpp:141] Setting up loss
I0422 03:00:24.518448  7583 net.cpp:148] Top shape: (1)
I0422 03:00:24.518452  7583 net.cpp:151]     with loss weight 1
I0422 03:00:24.518486  7583 net.cpp:156] Memory required for data: 2119744644
I0422 03:00:24.518491  7583 net.cpp:217] loss needs backward computation.
I0422 03:00:24.518496  7583 net.cpp:217] balance needs backward computation.
I0422 03:00:24.518501  7583 net.cpp:217] crop needs backward computation.
I0422 03:00:24.518507  7583 net.cpp:217] upsample needs backward computation.
I0422 03:00:24.518512  7583 net.cpp:217] score-fr needs backward computation.
I0422 03:00:24.518517  7583 net.cpp:217] drop7 needs backward computation.
I0422 03:00:24.518520  7583 net.cpp:217] relu7 needs backward computation.
I0422 03:00:24.518530  7583 net.cpp:217] fcc7 needs backward computation.
I0422 03:00:24.518535  7583 net.cpp:217] drop6 needs backward computation.
I0422 03:00:24.518540  7583 net.cpp:217] relu6 needs backward computation.
I0422 03:00:24.518544  7583 net.cpp:217] fcc6 needs backward computation.
I0422 03:00:24.518548  7583 net.cpp:217] pool5 needs backward computation.
I0422 03:00:24.518553  7583 net.cpp:217] relu5 needs backward computation.
I0422 03:00:24.518558  7583 net.cpp:217] cconv5 needs backward computation.
I0422 03:00:24.518563  7583 net.cpp:217] relu4 needs backward computation.
I0422 03:00:24.518566  7583 net.cpp:217] cconv4 needs backward computation.
I0422 03:00:24.518571  7583 net.cpp:217] relu3 needs backward computation.
I0422 03:00:24.518575  7583 net.cpp:217] conv3 needs backward computation.
I0422 03:00:24.518580  7583 net.cpp:217] norm2 needs backward computation.
I0422 03:00:24.518584  7583 net.cpp:217] pool2 needs backward computation.
I0422 03:00:24.518589  7583 net.cpp:217] relu2 needs backward computation.
I0422 03:00:24.518594  7583 net.cpp:217] conv2 needs backward computation.
I0422 03:00:24.518597  7583 net.cpp:217] norm1 needs backward computation.
I0422 03:00:24.518602  7583 net.cpp:217] pool1 needs backward computation.
I0422 03:00:24.518606  7583 net.cpp:217] relu1 needs backward computation.
I0422 03:00:24.518610  7583 net.cpp:217] conv1 needs backward computation.
I0422 03:00:24.518615  7583 net.cpp:219] jointMap_jointMap_0_split does not need backward computation.
I0422 03:00:24.518620  7583 net.cpp:219] jointMap does not need backward computation.
I0422 03:00:24.518626  7583 net.cpp:219] data_data_0_split does not need backward computation.
I0422 03:00:24.518632  7583 net.cpp:219] data does not need backward computation.
I0422 03:00:24.518636  7583 net.cpp:261] This network produces output loss
I0422 03:00:24.518662  7583 net.cpp:274] Network initialization done.
I0422 03:00:24.519781  7583 solver.cpp:181] Creating test net (#0) specified by net file: fcn_alxnet_train_val.prototxt
I0422 03:00:24.519840  7583 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0422 03:00:24.520151  7583 net.cpp:49] Initializing net from parameters: 
name: "FCN-AlexNet-joint"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "jointLocation"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 104
    mean_value: 116
    mean_value: 122
  }
  image_data_param {
    source: "/home/mengxin1/HumanPoseDetect/preprocess/val_label_resized.txt"
    batch_size: 10
    root_folder: "/home/mengxin1/mpii_human_pose_v1_images_resized/"
    label_num: 48
  }
}
layer {
  name: "jointMap"
  type: "PositionToMap"
  bottom: "data"
  bottom: "jointLocation"
  top: "jointMap"
  gaussian_param {
    std: 25
    half_window_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 100
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "cconv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "cconv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fcc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fcc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 6
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fcc6"
  top: "fcc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fcc6"
  top: "fcc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fcc7"
  type: "Convolution"
  bottom: "fcc6"
  top: "fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fcc7"
  top: "fcc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fcc7"
  top: "fcc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score-fr"
  type: "Convolution"
  bottom: "fcc7"
  top: "score-fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    engine: CAFFE
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "score-fcc7"
  top: "bigscore"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 63
    group: 16
    stride: 32
  }
}
layer {
  name: "crop"
  type: "Crop"
  bottom: "bigscore"
  bottom: "data"
  top: "score"
}
layer {
  name: "balance"
  type: "Balance"
  bottom: "score"
  bottom: "jointMap"
  top: "bscore"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "bscore"
  bottom: "jointMap"
  top: "loss"
}
I0422 03:00:24.520285  7583 layer_factory.hpp:77] Creating layer data
I0422 03:00:24.520306  7583 net.cpp:91] Creating Layer data
I0422 03:00:24.520316  7583 net.cpp:399] data -> data
I0422 03:00:24.520334  7583 net.cpp:399] data -> jointLocation
I0422 03:00:24.520345  7583 image_data_layer.cpp:40] Opening file /home/mengxin1/HumanPoseDetect/preprocess/val_label_resized.txt
I0422 03:00:24.546783  7583 image_data_layer.cpp:62] A total of 2424 images.
I0422 03:00:24.553216  7583 image_data_layer.cpp:89] output data size: 10,3,512,512
I0422 03:00:24.636819  7583 net.cpp:141] Setting up data
I0422 03:00:24.636868  7583 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 03:00:24.636876  7583 net.cpp:148] Top shape: 10 48 1 1 (480)
I0422 03:00:24.636880  7583 net.cpp:156] Memory required for data: 31459200
I0422 03:00:24.636893  7583 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 03:00:24.636911  7583 net.cpp:91] Creating Layer data_data_0_split
I0422 03:00:24.636917  7583 net.cpp:425] data_data_0_split <- data
I0422 03:00:24.636929  7583 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 03:00:24.636945  7583 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 03:00:24.636951  7583 net.cpp:399] data_data_0_split -> data_data_0_split_2
I0422 03:00:24.637154  7583 net.cpp:141] Setting up data_data_0_split
I0422 03:00:24.637166  7583 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 03:00:24.637172  7583 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 03:00:24.637177  7583 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 03:00:24.637182  7583 net.cpp:156] Memory required for data: 125831040
I0422 03:00:24.637187  7583 layer_factory.hpp:77] Creating layer jointMap
I0422 03:00:24.637197  7583 net.cpp:91] Creating Layer jointMap
I0422 03:00:24.637202  7583 net.cpp:425] jointMap <- data_data_0_split_0
I0422 03:00:24.637208  7583 net.cpp:425] jointMap <- jointLocation
I0422 03:00:24.637215  7583 net.cpp:399] jointMap -> jointMap
I0422 03:00:24.637261  7583 net.cpp:141] Setting up jointMap
I0422 03:00:24.637271  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:24.637275  7583 net.cpp:156] Memory required for data: 293603200
I0422 03:00:24.637280  7583 layer_factory.hpp:77] Creating layer jointMap_jointMap_0_split
I0422 03:00:24.637289  7583 net.cpp:91] Creating Layer jointMap_jointMap_0_split
I0422 03:00:24.637292  7583 net.cpp:425] jointMap_jointMap_0_split <- jointMap
I0422 03:00:24.637300  7583 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_0
I0422 03:00:24.637307  7583 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_1
I0422 03:00:24.637372  7583 net.cpp:141] Setting up jointMap_jointMap_0_split
I0422 03:00:24.637380  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:24.637387  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:24.637390  7583 net.cpp:156] Memory required for data: 629147520
I0422 03:00:24.637395  7583 layer_factory.hpp:77] Creating layer conv1
I0422 03:00:24.637415  7583 net.cpp:91] Creating Layer conv1
I0422 03:00:24.637420  7583 net.cpp:425] conv1 <- data_data_0_split_1
I0422 03:00:24.637429  7583 net.cpp:399] conv1 -> conv1
I0422 03:00:24.648196  7583 net.cpp:141] Setting up conv1
I0422 03:00:24.648212  7583 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 03:00:24.648217  7583 net.cpp:156] Memory required for data: 748095360
I0422 03:00:24.648231  7583 layer_factory.hpp:77] Creating layer relu1
I0422 03:00:24.648242  7583 net.cpp:91] Creating Layer relu1
I0422 03:00:24.648247  7583 net.cpp:425] relu1 <- conv1
I0422 03:00:24.648254  7583 net.cpp:386] relu1 -> conv1 (in-place)
I0422 03:00:24.660579  7583 net.cpp:141] Setting up relu1
I0422 03:00:24.660593  7583 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 03:00:24.660598  7583 net.cpp:156] Memory required for data: 867043200
I0422 03:00:24.660603  7583 layer_factory.hpp:77] Creating layer pool1
I0422 03:00:24.660612  7583 net.cpp:91] Creating Layer pool1
I0422 03:00:24.660617  7583 net.cpp:425] pool1 <- conv1
I0422 03:00:24.660625  7583 net.cpp:399] pool1 -> pool1
I0422 03:00:24.660701  7583 net.cpp:141] Setting up pool1
I0422 03:00:24.660712  7583 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 03:00:24.660724  7583 net.cpp:156] Memory required for data: 896780160
I0422 03:00:24.660729  7583 layer_factory.hpp:77] Creating layer norm1
I0422 03:00:24.660740  7583 net.cpp:91] Creating Layer norm1
I0422 03:00:24.660745  7583 net.cpp:425] norm1 <- pool1
I0422 03:00:24.660753  7583 net.cpp:399] norm1 -> norm1
I0422 03:00:24.660980  7583 net.cpp:141] Setting up norm1
I0422 03:00:24.660992  7583 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 03:00:24.660997  7583 net.cpp:156] Memory required for data: 926517120
I0422 03:00:24.661001  7583 layer_factory.hpp:77] Creating layer conv2
I0422 03:00:24.661015  7583 net.cpp:91] Creating Layer conv2
I0422 03:00:24.661021  7583 net.cpp:425] conv2 <- norm1
I0422 03:00:24.661029  7583 net.cpp:399] conv2 -> conv2
I0422 03:00:24.691268  7583 net.cpp:141] Setting up conv2
I0422 03:00:24.691284  7583 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 03:00:24.691289  7583 net.cpp:156] Memory required for data: 1005815680
I0422 03:00:24.691301  7583 layer_factory.hpp:77] Creating layer relu2
I0422 03:00:24.691310  7583 net.cpp:91] Creating Layer relu2
I0422 03:00:24.691315  7583 net.cpp:425] relu2 <- conv2
I0422 03:00:24.691326  7583 net.cpp:386] relu2 -> conv2 (in-place)
I0422 03:00:24.695940  7583 net.cpp:141] Setting up relu2
I0422 03:00:24.695953  7583 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 03:00:24.695957  7583 net.cpp:156] Memory required for data: 1085114240
I0422 03:00:24.695962  7583 layer_factory.hpp:77] Creating layer pool2
I0422 03:00:24.695971  7583 net.cpp:91] Creating Layer pool2
I0422 03:00:24.695976  7583 net.cpp:425] pool2 <- conv2
I0422 03:00:24.695984  7583 net.cpp:399] pool2 -> pool2
I0422 03:00:24.696056  7583 net.cpp:141] Setting up pool2
I0422 03:00:24.696066  7583 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 03:00:24.696070  7583 net.cpp:156] Memory required for data: 1104938880
I0422 03:00:24.696075  7583 layer_factory.hpp:77] Creating layer norm2
I0422 03:00:24.696084  7583 net.cpp:91] Creating Layer norm2
I0422 03:00:24.696089  7583 net.cpp:425] norm2 <- pool2
I0422 03:00:24.696096  7583 net.cpp:399] norm2 -> norm2
I0422 03:00:24.719506  7583 net.cpp:141] Setting up norm2
I0422 03:00:24.719552  7583 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 03:00:24.719557  7583 net.cpp:156] Memory required for data: 1124763520
I0422 03:00:24.719565  7583 layer_factory.hpp:77] Creating layer conv3
I0422 03:00:24.719590  7583 net.cpp:91] Creating Layer conv3
I0422 03:00:24.719597  7583 net.cpp:425] conv3 <- norm2
I0422 03:00:24.719610  7583 net.cpp:399] conv3 -> conv3
I0422 03:00:24.758895  7583 net.cpp:141] Setting up conv3
I0422 03:00:24.758944  7583 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 03:00:24.758949  7583 net.cpp:156] Memory required for data: 1154500480
I0422 03:00:24.758970  7583 layer_factory.hpp:77] Creating layer relu3
I0422 03:00:24.758986  7583 net.cpp:91] Creating Layer relu3
I0422 03:00:24.758994  7583 net.cpp:425] relu3 <- conv3
I0422 03:00:24.759004  7583 net.cpp:386] relu3 -> conv3 (in-place)
I0422 03:00:24.760438  7583 net.cpp:141] Setting up relu3
I0422 03:00:24.760452  7583 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 03:00:24.760457  7583 net.cpp:156] Memory required for data: 1184237440
I0422 03:00:24.760462  7583 layer_factory.hpp:77] Creating layer cconv4
I0422 03:00:24.760479  7583 net.cpp:91] Creating Layer cconv4
I0422 03:00:24.760485  7583 net.cpp:425] cconv4 <- conv3
I0422 03:00:24.760494  7583 net.cpp:399] cconv4 -> conv4
I0422 03:00:24.790438  7583 net.cpp:141] Setting up cconv4
I0422 03:00:24.790487  7583 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 03:00:24.790491  7583 net.cpp:156] Memory required for data: 1213974400
I0422 03:00:24.790506  7583 layer_factory.hpp:77] Creating layer relu4
I0422 03:00:24.790525  7583 net.cpp:91] Creating Layer relu4
I0422 03:00:24.790532  7583 net.cpp:425] relu4 <- conv4
I0422 03:00:24.790541  7583 net.cpp:386] relu4 -> conv4 (in-place)
I0422 03:00:24.796694  7583 net.cpp:141] Setting up relu4
I0422 03:00:24.796746  7583 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 03:00:24.796752  7583 net.cpp:156] Memory required for data: 1243711360
I0422 03:00:24.796762  7583 layer_factory.hpp:77] Creating layer cconv5
I0422 03:00:24.796797  7583 net.cpp:91] Creating Layer cconv5
I0422 03:00:24.796804  7583 net.cpp:425] cconv5 <- conv4
I0422 03:00:24.796816  7583 net.cpp:399] cconv5 -> conv5
I0422 03:00:24.831681  7583 net.cpp:141] Setting up cconv5
I0422 03:00:24.831727  7583 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 03:00:24.831733  7583 net.cpp:156] Memory required for data: 1263536000
I0422 03:00:24.831756  7583 layer_factory.hpp:77] Creating layer relu5
I0422 03:00:24.831773  7583 net.cpp:91] Creating Layer relu5
I0422 03:00:24.831780  7583 net.cpp:425] relu5 <- conv5
I0422 03:00:24.831789  7583 net.cpp:386] relu5 -> conv5 (in-place)
I0422 03:00:24.833747  7583 net.cpp:141] Setting up relu5
I0422 03:00:24.833760  7583 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 03:00:24.833765  7583 net.cpp:156] Memory required for data: 1283360640
I0422 03:00:24.833770  7583 layer_factory.hpp:77] Creating layer pool5
I0422 03:00:24.833782  7583 net.cpp:91] Creating Layer pool5
I0422 03:00:24.833787  7583 net.cpp:425] pool5 <- conv5
I0422 03:00:24.833796  7583 net.cpp:399] pool5 -> pool5
I0422 03:00:24.833874  7583 net.cpp:141] Setting up pool5
I0422 03:00:24.833884  7583 net.cpp:148] Top shape: 10 256 22 22 (1239040)
I0422 03:00:24.833889  7583 net.cpp:156] Memory required for data: 1288316800
I0422 03:00:24.833894  7583 layer_factory.hpp:77] Creating layer fcc6
I0422 03:00:24.833911  7583 net.cpp:91] Creating Layer fcc6
I0422 03:00:24.833917  7583 net.cpp:425] fcc6 <- pool5
I0422 03:00:24.833926  7583 net.cpp:399] fcc6 -> fcc6
I0422 03:00:25.359863  7583 net.cpp:141] Setting up fcc6
I0422 03:00:25.359911  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:25.359916  7583 net.cpp:156] Memory required for data: 1335666560
I0422 03:00:25.359930  7583 layer_factory.hpp:77] Creating layer relu6
I0422 03:00:25.359946  7583 net.cpp:91] Creating Layer relu6
I0422 03:00:25.359953  7583 net.cpp:425] relu6 <- fcc6
I0422 03:00:25.359966  7583 net.cpp:386] relu6 -> fcc6 (in-place)
I0422 03:00:25.396255  7583 net.cpp:141] Setting up relu6
I0422 03:00:25.396299  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:25.396306  7583 net.cpp:156] Memory required for data: 1383016320
I0422 03:00:25.396314  7583 layer_factory.hpp:77] Creating layer drop6
I0422 03:00:25.396333  7583 net.cpp:91] Creating Layer drop6
I0422 03:00:25.396342  7583 net.cpp:425] drop6 <- fcc6
I0422 03:00:25.396354  7583 net.cpp:386] drop6 -> fcc6 (in-place)
I0422 03:00:25.396448  7583 net.cpp:141] Setting up drop6
I0422 03:00:25.396458  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:25.396462  7583 net.cpp:156] Memory required for data: 1430366080
I0422 03:00:25.396467  7583 layer_factory.hpp:77] Creating layer fcc7
I0422 03:00:25.396486  7583 net.cpp:91] Creating Layer fcc7
I0422 03:00:25.396491  7583 net.cpp:425] fcc7 <- fcc6
I0422 03:00:25.396500  7583 net.cpp:399] fcc7 -> fcc7
I0422 03:00:25.634083  7583 net.cpp:141] Setting up fcc7
I0422 03:00:25.634132  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:25.634137  7583 net.cpp:156] Memory required for data: 1477715840
I0422 03:00:25.634151  7583 layer_factory.hpp:77] Creating layer relu7
I0422 03:00:25.634167  7583 net.cpp:91] Creating Layer relu7
I0422 03:00:25.634174  7583 net.cpp:425] relu7 <- fcc7
I0422 03:00:25.634183  7583 net.cpp:386] relu7 -> fcc7 (in-place)
I0422 03:00:25.636600  7583 net.cpp:141] Setting up relu7
I0422 03:00:25.636616  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:25.636621  7583 net.cpp:156] Memory required for data: 1525065600
I0422 03:00:25.636626  7583 layer_factory.hpp:77] Creating layer drop7
I0422 03:00:25.636637  7583 net.cpp:91] Creating Layer drop7
I0422 03:00:25.636643  7583 net.cpp:425] drop7 <- fcc7
I0422 03:00:25.636652  7583 net.cpp:386] drop7 -> fcc7 (in-place)
I0422 03:00:25.636713  7583 net.cpp:141] Setting up drop7
I0422 03:00:25.636723  7583 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 03:00:25.636726  7583 net.cpp:156] Memory required for data: 1572415360
I0422 03:00:25.636731  7583 layer_factory.hpp:77] Creating layer score-fr
I0422 03:00:25.636749  7583 net.cpp:91] Creating Layer score-fr
I0422 03:00:25.636754  7583 net.cpp:425] score-fr <- fcc7
I0422 03:00:25.636764  7583 net.cpp:399] score-fr -> score-fcc7
I0422 03:00:25.639070  7583 net.cpp:141] Setting up score-fr
I0422 03:00:25.639102  7583 net.cpp:148] Top shape: 10 16 17 17 (46240)
I0422 03:00:25.639107  7583 net.cpp:156] Memory required for data: 1572600320
I0422 03:00:25.639117  7583 layer_factory.hpp:77] Creating layer upsample
I0422 03:00:25.639134  7583 net.cpp:91] Creating Layer upsample
I0422 03:00:25.639140  7583 net.cpp:425] upsample <- score-fcc7
I0422 03:00:25.639150  7583 net.cpp:399] upsample -> bigscore
I0422 03:00:25.641165  7583 net.cpp:141] Setting up upsample
I0422 03:00:25.641191  7583 net.cpp:148] Top shape: 10 16 575 575 (52900000)
I0422 03:00:25.641196  7583 net.cpp:156] Memory required for data: 1784200320
I0422 03:00:25.641214  7583 layer_factory.hpp:77] Creating layer crop
I0422 03:00:25.641225  7583 net.cpp:91] Creating Layer crop
I0422 03:00:25.641232  7583 net.cpp:425] crop <- bigscore
I0422 03:00:25.641237  7583 net.cpp:425] crop <- data_data_0_split_2
I0422 03:00:25.641244  7583 net.cpp:399] crop -> score
I0422 03:00:25.641309  7583 net.cpp:141] Setting up crop
I0422 03:00:25.641324  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:25.641327  7583 net.cpp:156] Memory required for data: 1951972480
I0422 03:00:25.641332  7583 layer_factory.hpp:77] Creating layer balance
I0422 03:00:25.641340  7583 net.cpp:91] Creating Layer balance
I0422 03:00:25.641345  7583 net.cpp:425] balance <- score
I0422 03:00:25.641351  7583 net.cpp:425] balance <- jointMap_jointMap_0_split_0
I0422 03:00:25.641360  7583 net.cpp:399] balance -> bscore
I0422 03:00:25.641429  7583 net.cpp:141] Setting up balance
I0422 03:00:25.641444  7583 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 03:00:25.641448  7583 net.cpp:156] Memory required for data: 2119744640
I0422 03:00:25.641453  7583 layer_factory.hpp:77] Creating layer loss
I0422 03:00:25.641461  7583 net.cpp:91] Creating Layer loss
I0422 03:00:25.641465  7583 net.cpp:425] loss <- bscore
I0422 03:00:25.641471  7583 net.cpp:425] loss <- jointMap_jointMap_0_split_1
I0422 03:00:25.641477  7583 net.cpp:399] loss -> loss
I0422 03:00:25.641547  7583 net.cpp:141] Setting up loss
I0422 03:00:25.641556  7583 net.cpp:148] Top shape: (1)
I0422 03:00:25.641561  7583 net.cpp:151]     with loss weight 1
I0422 03:00:25.641573  7583 net.cpp:156] Memory required for data: 2119744644
I0422 03:00:25.641578  7583 net.cpp:217] loss needs backward computation.
I0422 03:00:25.641583  7583 net.cpp:217] balance needs backward computation.
I0422 03:00:25.641588  7583 net.cpp:217] crop needs backward computation.
I0422 03:00:25.641593  7583 net.cpp:217] upsample needs backward computation.
I0422 03:00:25.641597  7583 net.cpp:217] score-fr needs backward computation.
I0422 03:00:25.641602  7583 net.cpp:217] drop7 needs backward computation.
I0422 03:00:25.641607  7583 net.cpp:217] relu7 needs backward computation.
I0422 03:00:25.641612  7583 net.cpp:217] fcc7 needs backward computation.
I0422 03:00:25.641615  7583 net.cpp:217] drop6 needs backward computation.
I0422 03:00:25.641619  7583 net.cpp:217] relu6 needs backward computation.
I0422 03:00:25.641623  7583 net.cpp:217] fcc6 needs backward computation.
I0422 03:00:25.641628  7583 net.cpp:217] pool5 needs backward computation.
I0422 03:00:25.641633  7583 net.cpp:217] relu5 needs backward computation.
I0422 03:00:25.641638  7583 net.cpp:217] cconv5 needs backward computation.
I0422 03:00:25.641644  7583 net.cpp:217] relu4 needs backward computation.
I0422 03:00:25.641649  7583 net.cpp:217] cconv4 needs backward computation.
I0422 03:00:25.641654  7583 net.cpp:217] relu3 needs backward computation.
I0422 03:00:25.641662  7583 net.cpp:217] conv3 needs backward computation.
I0422 03:00:25.641667  7583 net.cpp:217] norm2 needs backward computation.
I0422 03:00:25.641671  7583 net.cpp:217] pool2 needs backward computation.
I0422 03:00:25.641676  7583 net.cpp:217] relu2 needs backward computation.
I0422 03:00:25.641681  7583 net.cpp:217] conv2 needs backward computation.
I0422 03:00:25.641685  7583 net.cpp:217] norm1 needs backward computation.
I0422 03:00:25.641690  7583 net.cpp:217] pool1 needs backward computation.
I0422 03:00:25.641695  7583 net.cpp:217] relu1 needs backward computation.
I0422 03:00:25.641698  7583 net.cpp:217] conv1 needs backward computation.
I0422 03:00:25.641703  7583 net.cpp:219] jointMap_jointMap_0_split does not need backward computation.
I0422 03:00:25.641708  7583 net.cpp:219] jointMap does not need backward computation.
I0422 03:00:25.641715  7583 net.cpp:219] data_data_0_split does not need backward computation.
I0422 03:00:25.641721  7583 net.cpp:219] data does not need backward computation.
I0422 03:00:25.641723  7583 net.cpp:261] This network produces output loss
I0422 03:00:25.641746  7583 net.cpp:274] Network initialization done.
I0422 03:00:25.641854  7583 solver.cpp:60] Solver scaffolding done.
I0422 03:00:28.803930  7583 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: /home/mengxin1/HumanPoseDetect/fcn_joint_predict/bvlc_reference_caffenet.caffemodel
I0422 03:00:28.803977  7583 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.
W0422 03:00:28.803982  7583 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0422 03:00:28.803987  7583 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/mengxin1/HumanPoseDetect/fcn_joint_predict/bvlc_reference_caffenet.caffemodel
I0422 03:00:38.109608  7583 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I0422 03:00:38.121398  7583 net.cpp:752] Ignoring source layer conv4
I0422 03:00:38.121429  7583 net.cpp:752] Ignoring source layer conv5
I0422 03:00:38.121434  7583 net.cpp:752] Ignoring source layer fc6
I0422 03:00:38.121439  7583 net.cpp:752] Ignoring source layer fc7
I0422 03:00:38.121443  7583 net.cpp:752] Ignoring source layer fc8
I0422 03:00:38.143007  7583 solver.cpp:337] Iteration 0, Testing net (#0)
I0422 03:01:27.725776  7583 solver.cpp:404]     Test net output #0: loss = 8.57076 (* 1 = 8.57076 loss)
I0422 03:01:30.902673  7583 solver.cpp:228] Iteration 0, loss = 9.65495
I0422 03:01:30.902734  7583 solver.cpp:244]     Train net output #0: loss = 9.65495 (* 1 = 9.65495 loss)
I0422 03:01:30.902750  7583 sgd_solver.cpp:106] Iteration 0, lr = 0.0008
I0422 03:03:10.398877  7583 solver.cpp:228] Iteration 20, loss = 9.99555
I0422 03:03:10.399332  7583 solver.cpp:244]     Train net output #0: loss = 9.99555 (* 1 = 9.99555 loss)
I0422 03:03:10.399348  7583 sgd_solver.cpp:106] Iteration 20, lr = 0.0008
I0422 03:04:47.608816  7583 solver.cpp:228] Iteration 40, loss = 6.4684
I0422 03:04:47.609297  7583 solver.cpp:244]     Train net output #0: loss = 6.4684 (* 1 = 6.4684 loss)
I0422 03:04:47.609313  7583 sgd_solver.cpp:106] Iteration 40, lr = 0.0008
I0422 03:06:25.466420  7583 solver.cpp:228] Iteration 60, loss = 7.96482
I0422 03:06:25.466884  7583 solver.cpp:244]     Train net output #0: loss = 7.96482 (* 1 = 7.96482 loss)
I0422 03:06:25.466899  7583 sgd_solver.cpp:106] Iteration 60, lr = 0.0008
I0422 03:08:03.295610  7583 solver.cpp:228] Iteration 80, loss = 6.7024
I0422 03:08:03.296103  7583 solver.cpp:244]     Train net output #0: loss = 6.7024 (* 1 = 6.7024 loss)
I0422 03:08:03.296118  7583 sgd_solver.cpp:106] Iteration 80, lr = 0.0008
I0422 03:09:40.666095  7583 solver.cpp:228] Iteration 100, loss = 6.11376
I0422 03:09:40.666491  7583 solver.cpp:244]     Train net output #0: loss = 6.11376 (* 1 = 6.11376 loss)
I0422 03:09:40.666506  7583 sgd_solver.cpp:106] Iteration 100, lr = 0.0008
I0422 03:11:18.548446  7583 solver.cpp:228] Iteration 120, loss = 5.83294
I0422 03:11:18.548961  7583 solver.cpp:244]     Train net output #0: loss = 5.83294 (* 1 = 5.83294 loss)
I0422 03:11:18.548979  7583 sgd_solver.cpp:106] Iteration 120, lr = 0.0008
I0422 03:12:55.641752  7583 solver.cpp:228] Iteration 140, loss = 6.0403
I0422 03:12:55.642313  7583 solver.cpp:244]     Train net output #0: loss = 6.0403 (* 1 = 6.0403 loss)
I0422 03:12:55.642336  7583 sgd_solver.cpp:106] Iteration 140, lr = 0.0008
I0422 03:14:33.942587  7583 solver.cpp:228] Iteration 160, loss = 5.32286
I0422 03:14:33.943084  7583 solver.cpp:244]     Train net output #0: loss = 5.32286 (* 1 = 5.32286 loss)
I0422 03:14:33.943099  7583 sgd_solver.cpp:106] Iteration 160, lr = 0.0008
I0422 03:16:12.025657  7583 solver.cpp:228] Iteration 180, loss = 6.66733
I0422 03:16:12.026154  7583 solver.cpp:244]     Train net output #0: loss = 6.66733 (* 1 = 6.66733 loss)
I0422 03:16:12.026170  7583 sgd_solver.cpp:106] Iteration 180, lr = 0.0008
I0422 03:17:50.196225  7583 solver.cpp:228] Iteration 200, loss = 6.33983
I0422 03:17:50.196724  7583 solver.cpp:244]     Train net output #0: loss = 6.33983 (* 1 = 6.33983 loss)
I0422 03:17:50.196739  7583 sgd_solver.cpp:106] Iteration 200, lr = 0.0008
I0422 03:19:28.310843  7583 solver.cpp:228] Iteration 220, loss = 8.37063
I0422 03:19:28.311399  7583 solver.cpp:244]     Train net output #0: loss = 8.37063 (* 1 = 8.37063 loss)
I0422 03:19:28.311422  7583 sgd_solver.cpp:106] Iteration 220, lr = 0.0008
I0422 03:21:06.493510  7583 solver.cpp:228] Iteration 240, loss = 6.26689
I0422 03:21:06.493989  7583 solver.cpp:244]     Train net output #0: loss = 6.26689 (* 1 = 6.26689 loss)
I0422 03:21:06.494004  7583 sgd_solver.cpp:106] Iteration 240, lr = 0.0008
I0422 03:22:44.317443  7583 solver.cpp:228] Iteration 260, loss = 6.6211
I0422 03:22:44.317848  7583 solver.cpp:244]     Train net output #0: loss = 6.6211 (* 1 = 6.6211 loss)
I0422 03:22:44.317863  7583 sgd_solver.cpp:106] Iteration 260, lr = 0.0008
I0422 03:24:22.552573  7583 solver.cpp:228] Iteration 280, loss = 7.23849
I0422 03:24:22.553051  7583 solver.cpp:244]     Train net output #0: loss = 7.23849 (* 1 = 7.23849 loss)
I0422 03:24:22.553066  7583 sgd_solver.cpp:106] Iteration 280, lr = 0.0008
I0422 03:26:01.217730  7583 solver.cpp:228] Iteration 300, loss = 5.92747
I0422 03:26:01.218236  7583 solver.cpp:244]     Train net output #0: loss = 5.92747 (* 1 = 5.92747 loss)
I0422 03:26:01.218252  7583 sgd_solver.cpp:106] Iteration 300, lr = 0.0008
I0422 03:27:38.828845  7583 solver.cpp:228] Iteration 320, loss = 5.76338
I0422 03:27:38.829310  7583 solver.cpp:244]     Train net output #0: loss = 5.76338 (* 1 = 5.76338 loss)
I0422 03:27:38.829327  7583 sgd_solver.cpp:106] Iteration 320, lr = 0.0008
I0422 03:29:16.988440  7583 solver.cpp:228] Iteration 340, loss = 5.506
I0422 03:29:16.988834  7583 solver.cpp:244]     Train net output #0: loss = 5.506 (* 1 = 5.506 loss)
I0422 03:29:16.988850  7583 sgd_solver.cpp:106] Iteration 340, lr = 0.0008
I0422 03:30:55.499924  7583 solver.cpp:228] Iteration 360, loss = 6.74353
I0422 03:30:55.500365  7583 solver.cpp:244]     Train net output #0: loss = 6.74353 (* 1 = 6.74353 loss)
I0422 03:30:55.500380  7583 sgd_solver.cpp:106] Iteration 360, lr = 0.0008
I0422 03:32:32.913420  7583 solver.cpp:228] Iteration 380, loss = 5.75647
I0422 03:32:32.913879  7583 solver.cpp:244]     Train net output #0: loss = 5.75647 (* 1 = 5.75647 loss)
I0422 03:32:32.913894  7583 sgd_solver.cpp:106] Iteration 380, lr = 0.0008
I0422 03:34:11.226121  7583 solver.cpp:228] Iteration 400, loss = 8.18287
I0422 03:34:11.226557  7583 solver.cpp:244]     Train net output #0: loss = 8.18287 (* 1 = 8.18287 loss)
I0422 03:34:11.226573  7583 sgd_solver.cpp:106] Iteration 400, lr = 0.0008
I0422 03:35:49.871105  7583 solver.cpp:228] Iteration 420, loss = 7.56687
I0422 03:35:49.871562  7583 solver.cpp:244]     Train net output #0: loss = 7.56687 (* 1 = 7.56687 loss)
I0422 03:35:49.871577  7583 sgd_solver.cpp:106] Iteration 420, lr = 0.0008
I0422 03:37:28.713841  7583 solver.cpp:228] Iteration 440, loss = 7.04269
I0422 03:37:28.714325  7583 solver.cpp:244]     Train net output #0: loss = 7.04269 (* 1 = 7.04269 loss)
I0422 03:37:28.714344  7583 sgd_solver.cpp:106] Iteration 440, lr = 0.0008
I0422 03:39:09.833070  7583 solver.cpp:228] Iteration 460, loss = 5.27334
I0422 03:39:09.833634  7583 solver.cpp:244]     Train net output #0: loss = 5.27334 (* 1 = 5.27334 loss)
I0422 03:39:09.833663  7583 sgd_solver.cpp:106] Iteration 460, lr = 0.0008
I0422 03:40:49.747810  7583 solver.cpp:228] Iteration 480, loss = 4.72088
I0422 03:40:49.748373  7583 solver.cpp:244]     Train net output #0: loss = 4.72088 (* 1 = 4.72088 loss)
I0422 03:40:49.748395  7583 sgd_solver.cpp:106] Iteration 480, lr = 0.0008
I0422 03:42:31.977105  7583 solver.cpp:228] Iteration 500, loss = 5.34847
I0422 03:42:31.977516  7583 solver.cpp:244]     Train net output #0: loss = 5.34847 (* 1 = 5.34847 loss)
I0422 03:42:31.977537  7583 sgd_solver.cpp:106] Iteration 500, lr = 0.0008
I0422 03:44:14.547917  7583 solver.cpp:228] Iteration 520, loss = 7.95789
I0422 03:44:14.548451  7583 solver.cpp:244]     Train net output #0: loss = 7.95789 (* 1 = 7.95789 loss)
I0422 03:44:14.548470  7583 sgd_solver.cpp:106] Iteration 520, lr = 0.0008
I0422 03:45:56.731607  7583 solver.cpp:228] Iteration 540, loss = 6.16418
I0422 03:45:56.733116  7583 solver.cpp:244]     Train net output #0: loss = 6.16418 (* 1 = 6.16418 loss)
I0422 03:45:56.733142  7583 sgd_solver.cpp:106] Iteration 540, lr = 0.0008
I0422 03:47:36.710222  7583 solver.cpp:228] Iteration 560, loss = 5.7556
I0422 03:47:36.710752  7583 solver.cpp:244]     Train net output #0: loss = 5.7556 (* 1 = 5.7556 loss)
I0422 03:47:36.710772  7583 sgd_solver.cpp:106] Iteration 560, lr = 0.0008
I0422 03:49:17.660230  7583 solver.cpp:228] Iteration 580, loss = 5.92514
I0422 03:49:17.660753  7583 solver.cpp:244]     Train net output #0: loss = 5.92514 (* 1 = 5.92514 loss)
I0422 03:49:17.660778  7583 sgd_solver.cpp:106] Iteration 580, lr = 0.0008
I0422 03:50:58.366943  7583 solver.cpp:228] Iteration 600, loss = 5.2555
I0422 03:50:58.367383  7583 solver.cpp:244]     Train net output #0: loss = 5.2555 (* 1 = 5.2555 loss)
I0422 03:50:58.367413  7583 sgd_solver.cpp:106] Iteration 600, lr = 0.0008
I0422 03:52:40.461071  7583 solver.cpp:228] Iteration 620, loss = 5.75749
I0422 03:52:40.461607  7583 solver.cpp:244]     Train net output #0: loss = 5.75749 (* 1 = 5.75749 loss)
I0422 03:52:40.461627  7583 sgd_solver.cpp:106] Iteration 620, lr = 0.0008
I0422 03:54:22.641855  7583 solver.cpp:228] Iteration 640, loss = 6.50515
I0422 03:54:22.642436  7583 solver.cpp:244]     Train net output #0: loss = 6.50515 (* 1 = 6.50515 loss)
I0422 03:54:22.642462  7583 sgd_solver.cpp:106] Iteration 640, lr = 0.0008
I0422 03:56:03.482669  7583 solver.cpp:228] Iteration 660, loss = 5.64776
I0422 03:56:03.483197  7583 solver.cpp:244]     Train net output #0: loss = 5.64776 (* 1 = 5.64776 loss)
I0422 03:56:03.483218  7583 sgd_solver.cpp:106] Iteration 660, lr = 0.0008
I0422 03:57:45.116930  7583 solver.cpp:228] Iteration 680, loss = 7.19882
I0422 03:57:45.117496  7583 solver.cpp:244]     Train net output #0: loss = 7.19882 (* 1 = 7.19882 loss)
I0422 03:57:45.117514  7583 sgd_solver.cpp:106] Iteration 680, lr = 0.0008
I0422 03:59:25.702918  7583 solver.cpp:228] Iteration 700, loss = 7.85665
I0422 03:59:25.703441  7583 solver.cpp:244]     Train net output #0: loss = 7.85665 (* 1 = 7.85665 loss)
I0422 03:59:25.703469  7583 sgd_solver.cpp:106] Iteration 700, lr = 0.0008
I0422 04:01:06.293125  7583 solver.cpp:228] Iteration 720, loss = 4.71267
I0422 04:01:06.293611  7583 solver.cpp:244]     Train net output #0: loss = 4.71267 (* 1 = 4.71267 loss)
I0422 04:01:06.293632  7583 sgd_solver.cpp:106] Iteration 720, lr = 0.0008
I0422 04:02:47.166770  7583 solver.cpp:228] Iteration 740, loss = 5.3019
I0422 04:02:47.167239  7583 solver.cpp:244]     Train net output #0: loss = 5.3019 (* 1 = 5.3019 loss)
I0422 04:02:47.167255  7583 sgd_solver.cpp:106] Iteration 740, lr = 0.0008
I0422 04:04:26.885478  7583 solver.cpp:228] Iteration 760, loss = 6.03662
I0422 04:04:26.885939  7583 solver.cpp:244]     Train net output #0: loss = 6.03662 (* 1 = 6.03662 loss)
I0422 04:04:26.885956  7583 sgd_solver.cpp:106] Iteration 760, lr = 0.0008
I0422 04:06:04.637907  7583 solver.cpp:228] Iteration 780, loss = 6.66871
I0422 04:06:04.638324  7583 solver.cpp:244]     Train net output #0: loss = 6.66871 (* 1 = 6.66871 loss)
I0422 04:06:04.638339  7583 sgd_solver.cpp:106] Iteration 780, lr = 0.0008
I0422 04:07:41.514406  7583 solver.cpp:228] Iteration 800, loss = 10.1785
I0422 04:07:41.514830  7583 solver.cpp:244]     Train net output #0: loss = 10.1785 (* 1 = 10.1785 loss)
I0422 04:07:41.514847  7583 sgd_solver.cpp:106] Iteration 800, lr = 0.0008
I0422 04:09:20.248900  7583 solver.cpp:228] Iteration 820, loss = 6.01653
I0422 04:09:20.249318  7583 solver.cpp:244]     Train net output #0: loss = 6.01653 (* 1 = 6.01653 loss)
I0422 04:09:20.249332  7583 sgd_solver.cpp:106] Iteration 820, lr = 0.0008
I0422 04:10:59.402794  7583 solver.cpp:228] Iteration 840, loss = 4.95807
I0422 04:10:59.403235  7583 solver.cpp:244]     Train net output #0: loss = 4.95807 (* 1 = 4.95807 loss)
I0422 04:10:59.403250  7583 sgd_solver.cpp:106] Iteration 840, lr = 0.0008
I0422 04:12:38.377192  7583 solver.cpp:228] Iteration 860, loss = 5.78566
I0422 04:12:38.377612  7583 solver.cpp:244]     Train net output #0: loss = 5.78566 (* 1 = 5.78566 loss)
I0422 04:12:38.377627  7583 sgd_solver.cpp:106] Iteration 860, lr = 0.0008
I0422 04:14:16.202631  7583 solver.cpp:228] Iteration 880, loss = 6.91364
I0422 04:14:16.203106  7583 solver.cpp:244]     Train net output #0: loss = 6.91364 (* 1 = 6.91364 loss)
I0422 04:14:16.203122  7583 sgd_solver.cpp:106] Iteration 880, lr = 0.0008
I0422 04:15:53.584336  7583 solver.cpp:228] Iteration 900, loss = 6.50799
I0422 04:15:53.584767  7583 solver.cpp:244]     Train net output #0: loss = 6.50799 (* 1 = 6.50799 loss)
I0422 04:15:53.584782  7583 sgd_solver.cpp:106] Iteration 900, lr = 0.0008
I0422 04:17:32.122442  7583 solver.cpp:228] Iteration 920, loss = 7.02768
I0422 04:17:32.122944  7583 solver.cpp:244]     Train net output #0: loss = 7.02768 (* 1 = 7.02768 loss)
I0422 04:17:32.122959  7583 sgd_solver.cpp:106] Iteration 920, lr = 0.0008
I0422 04:19:10.326498  7583 solver.cpp:228] Iteration 940, loss = 4.95112
I0422 04:19:10.326951  7583 solver.cpp:244]     Train net output #0: loss = 4.95112 (* 1 = 4.95112 loss)
I0422 04:19:10.326967  7583 sgd_solver.cpp:106] Iteration 940, lr = 0.0008
I0422 04:20:48.455106  7583 solver.cpp:228] Iteration 960, loss = 6.29466
I0422 04:20:48.455582  7583 solver.cpp:244]     Train net output #0: loss = 6.29466 (* 1 = 6.29466 loss)
I0422 04:20:48.455598  7583 sgd_solver.cpp:106] Iteration 960, lr = 0.0008
I0422 04:22:29.893354  7583 solver.cpp:228] Iteration 980, loss = 9.36272
I0422 04:22:29.893808  7583 solver.cpp:244]     Train net output #0: loss = 9.36272 (* 1 = 9.36272 loss)
I0422 04:22:29.893823  7583 sgd_solver.cpp:106] Iteration 980, lr = 0.0008
I0422 04:24:05.852823  7583 solver.cpp:337] Iteration 1000, Testing net (#0)
I0422 04:24:57.679790  7583 solver.cpp:404]     Test net output #0: loss = 6.83401 (* 1 = 6.83401 loss)
I0422 04:24:59.403317  7583 solver.cpp:228] Iteration 1000, loss = 6.48167
I0422 04:24:59.403383  7583 solver.cpp:244]     Train net output #0: loss = 6.48167 (* 1 = 6.48167 loss)
I0422 04:24:59.403396  7583 sgd_solver.cpp:106] Iteration 1000, lr = 0.0008
I0422 04:26:38.881989  7583 solver.cpp:228] Iteration 1020, loss = 8.24437
I0422 04:26:38.882444  7583 solver.cpp:244]     Train net output #0: loss = 8.24437 (* 1 = 8.24437 loss)
I0422 04:26:38.882459  7583 sgd_solver.cpp:106] Iteration 1020, lr = 0.0008
I0422 04:28:17.991616  7583 solver.cpp:228] Iteration 1040, loss = 7.54091
I0422 04:28:17.992076  7583 solver.cpp:244]     Train net output #0: loss = 7.54091 (* 1 = 7.54091 loss)
I0422 04:28:17.992091  7583 sgd_solver.cpp:106] Iteration 1040, lr = 0.0008
I0422 04:29:56.850867  7583 solver.cpp:228] Iteration 1060, loss = 8.80853
I0422 04:29:56.851284  7583 solver.cpp:244]     Train net output #0: loss = 8.80853 (* 1 = 8.80853 loss)
I0422 04:29:56.851310  7583 sgd_solver.cpp:106] Iteration 1060, lr = 0.0008
I0422 04:31:35.347412  7583 solver.cpp:228] Iteration 1080, loss = 10.7718
I0422 04:31:35.347897  7583 solver.cpp:244]     Train net output #0: loss = 10.7718 (* 1 = 10.7718 loss)
I0422 04:31:35.347913  7583 sgd_solver.cpp:106] Iteration 1080, lr = 0.0008
I0422 04:33:13.306149  7583 solver.cpp:228] Iteration 1100, loss = 6.92138
I0422 04:33:13.306547  7583 solver.cpp:244]     Train net output #0: loss = 6.92138 (* 1 = 6.92138 loss)
I0422 04:33:13.306562  7583 sgd_solver.cpp:106] Iteration 1100, lr = 0.0008
I0422 04:34:50.603725  7583 solver.cpp:228] Iteration 1120, loss = 8.95183
I0422 04:34:50.604172  7583 solver.cpp:244]     Train net output #0: loss = 8.95183 (* 1 = 8.95183 loss)
I0422 04:34:50.604187  7583 sgd_solver.cpp:106] Iteration 1120, lr = 0.0008
I0422 04:36:28.888437  7583 solver.cpp:228] Iteration 1140, loss = 6.53302
I0422 04:36:28.888890  7583 solver.cpp:244]     Train net output #0: loss = 6.53302 (* 1 = 6.53302 loss)
I0422 04:36:28.888905  7583 sgd_solver.cpp:106] Iteration 1140, lr = 0.0008
I0422 04:38:06.559520  7583 solver.cpp:228] Iteration 1160, loss = 9.06782
I0422 04:38:06.559962  7583 solver.cpp:244]     Train net output #0: loss = 9.06782 (* 1 = 9.06782 loss)
I0422 04:38:06.559976  7583 sgd_solver.cpp:106] Iteration 1160, lr = 0.0008
I0422 04:39:46.863661  7583 solver.cpp:228] Iteration 1180, loss = 5.66428
I0422 04:39:46.864397  7583 solver.cpp:244]     Train net output #0: loss = 5.66428 (* 1 = 5.66428 loss)
I0422 04:39:46.864413  7583 sgd_solver.cpp:106] Iteration 1180, lr = 0.0008
I0422 04:41:24.813500  7583 solver.cpp:228] Iteration 1200, loss = 4.72164
I0422 04:41:24.813946  7583 solver.cpp:244]     Train net output #0: loss = 4.72164 (* 1 = 4.72164 loss)
I0422 04:41:24.813961  7583 sgd_solver.cpp:106] Iteration 1200, lr = 0.0008
I0422 04:43:03.848134  7583 solver.cpp:228] Iteration 1220, loss = 6.21671
I0422 04:43:03.848531  7583 solver.cpp:244]     Train net output #0: loss = 6.21671 (* 1 = 6.21671 loss)
I0422 04:43:03.848546  7583 sgd_solver.cpp:106] Iteration 1220, lr = 0.0008
I0422 04:44:41.693545  7583 solver.cpp:228] Iteration 1240, loss = 7.7543
I0422 04:44:41.693969  7583 solver.cpp:244]     Train net output #0: loss = 7.7543 (* 1 = 7.7543 loss)
I0422 04:44:41.693984  7583 sgd_solver.cpp:106] Iteration 1240, lr = 0.0008
I0422 04:46:19.670423  7583 solver.cpp:228] Iteration 1260, loss = 8.05976
I0422 04:46:19.670776  7583 solver.cpp:244]     Train net output #0: loss = 8.05976 (* 1 = 8.05976 loss)
I0422 04:46:19.670791  7583 sgd_solver.cpp:106] Iteration 1260, lr = 0.0008
I0422 04:47:57.106115  7583 solver.cpp:228] Iteration 1280, loss = 10.4752
I0422 04:47:57.106552  7583 solver.cpp:244]     Train net output #0: loss = 10.4752 (* 1 = 10.4752 loss)
I0422 04:47:57.106567  7583 sgd_solver.cpp:106] Iteration 1280, lr = 0.0008
I0422 04:49:35.845182  7583 solver.cpp:228] Iteration 1300, loss = 6.12358
I0422 04:49:35.845631  7583 solver.cpp:244]     Train net output #0: loss = 6.12358 (* 1 = 6.12358 loss)
I0422 04:49:35.845646  7583 sgd_solver.cpp:106] Iteration 1300, lr = 0.0008
I0422 04:51:13.823866  7583 solver.cpp:228] Iteration 1320, loss = 6.76943
I0422 04:51:13.824326  7583 solver.cpp:244]     Train net output #0: loss = 6.76943 (* 1 = 6.76943 loss)
I0422 04:51:13.824344  7583 sgd_solver.cpp:106] Iteration 1320, lr = 0.0008
I0422 04:52:52.139453  7583 solver.cpp:228] Iteration 1340, loss = 6.78985
I0422 04:52:52.139896  7583 solver.cpp:244]     Train net output #0: loss = 6.78985 (* 1 = 6.78985 loss)
I0422 04:52:52.139911  7583 sgd_solver.cpp:106] Iteration 1340, lr = 0.0008
I0422 04:54:30.531618  7583 solver.cpp:228] Iteration 1360, loss = 5.77823
I0422 04:54:30.532065  7583 solver.cpp:244]     Train net output #0: loss = 5.77823 (* 1 = 5.77823 loss)
I0422 04:54:30.532079  7583 sgd_solver.cpp:106] Iteration 1360, lr = 0.0008
I0422 04:56:08.608098  7583 solver.cpp:228] Iteration 1380, loss = 5.64474
I0422 04:56:08.608588  7583 solver.cpp:244]     Train net output #0: loss = 5.64474 (* 1 = 5.64474 loss)
I0422 04:56:08.608615  7583 sgd_solver.cpp:106] Iteration 1380, lr = 0.0008
I0422 04:57:46.039692  7583 solver.cpp:228] Iteration 1400, loss = 5.46525
I0422 04:57:46.040047  7583 solver.cpp:244]     Train net output #0: loss = 5.46525 (* 1 = 5.46525 loss)
I0422 04:57:46.040061  7583 sgd_solver.cpp:106] Iteration 1400, lr = 0.0008
I0422 04:59:24.129371  7583 solver.cpp:228] Iteration 1420, loss = 5.77291
I0422 04:59:24.129811  7583 solver.cpp:244]     Train net output #0: loss = 5.77291 (* 1 = 5.77291 loss)
I0422 04:59:24.129827  7583 sgd_solver.cpp:106] Iteration 1420, lr = 0.0008
I0422 05:01:02.324393  7583 solver.cpp:228] Iteration 1440, loss = 7.81098
I0422 05:01:02.324848  7583 solver.cpp:244]     Train net output #0: loss = 7.81098 (* 1 = 7.81098 loss)
I0422 05:01:02.324864  7583 sgd_solver.cpp:106] Iteration 1440, lr = 0.0008
I0422 05:02:39.823128  7583 solver.cpp:228] Iteration 1460, loss = 6.1405
I0422 05:02:39.823655  7583 solver.cpp:244]     Train net output #0: loss = 6.1405 (* 1 = 6.1405 loss)
I0422 05:02:39.823676  7583 sgd_solver.cpp:106] Iteration 1460, lr = 0.0008
I0422 05:04:18.674516  7583 solver.cpp:228] Iteration 1480, loss = 6.87414
I0422 05:04:18.674896  7583 solver.cpp:244]     Train net output #0: loss = 6.87414 (* 1 = 6.87414 loss)
I0422 05:04:18.674911  7583 sgd_solver.cpp:106] Iteration 1480, lr = 0.0008
I0422 05:05:57.022919  7583 solver.cpp:228] Iteration 1500, loss = 8.87413
I0422 05:05:57.023427  7583 solver.cpp:244]     Train net output #0: loss = 8.87413 (* 1 = 8.87413 loss)
I0422 05:05:57.023442  7583 sgd_solver.cpp:106] Iteration 1500, lr = 0.0008
I0422 05:07:35.472563  7583 solver.cpp:228] Iteration 1520, loss = 8.68276
I0422 05:07:35.473016  7583 solver.cpp:244]     Train net output #0: loss = 8.68276 (* 1 = 8.68276 loss)
I0422 05:07:35.473031  7583 sgd_solver.cpp:106] Iteration 1520, lr = 0.0008
I0422 05:09:15.659473  7583 solver.cpp:228] Iteration 1540, loss = 8.42293
I0422 05:09:15.659934  7583 solver.cpp:244]     Train net output #0: loss = 8.42293 (* 1 = 8.42293 loss)
I0422 05:09:15.659950  7583 sgd_solver.cpp:106] Iteration 1540, lr = 0.0008
I0422 05:10:55.177979  7583 solver.cpp:228] Iteration 1560, loss = 5.51782
I0422 05:10:55.178467  7583 solver.cpp:244]     Train net output #0: loss = 5.51782 (* 1 = 5.51782 loss)
I0422 05:10:55.178483  7583 sgd_solver.cpp:106] Iteration 1560, lr = 0.0008
I0422 05:12:35.136241  7583 solver.cpp:228] Iteration 1580, loss = 6.2351
I0422 05:12:35.136716  7583 solver.cpp:244]     Train net output #0: loss = 6.2351 (* 1 = 6.2351 loss)
I0422 05:12:35.136734  7583 sgd_solver.cpp:106] Iteration 1580, lr = 0.0008
I0422 05:14:15.716001  7583 solver.cpp:228] Iteration 1600, loss = 4.70507
I0422 05:14:15.716470  7583 solver.cpp:244]     Train net output #0: loss = 4.70507 (* 1 = 4.70507 loss)
I0422 05:14:15.716485  7583 sgd_solver.cpp:106] Iteration 1600, lr = 0.0008
I0422 05:15:55.025374  7583 solver.cpp:228] Iteration 1620, loss = 6.1083
I0422 05:15:55.025823  7583 solver.cpp:244]     Train net output #0: loss = 6.1083 (* 1 = 6.1083 loss)
I0422 05:15:55.025838  7583 sgd_solver.cpp:106] Iteration 1620, lr = 0.0008
I0422 05:17:34.735503  7583 solver.cpp:228] Iteration 1640, loss = 7.88012
I0422 05:17:34.735949  7583 solver.cpp:244]     Train net output #0: loss = 7.88012 (* 1 = 7.88012 loss)
I0422 05:17:34.735965  7583 sgd_solver.cpp:106] Iteration 1640, lr = 0.0008
I0422 05:19:14.278249  7583 solver.cpp:228] Iteration 1660, loss = 6.95604
I0422 05:19:14.278688  7583 solver.cpp:244]     Train net output #0: loss = 6.95604 (* 1 = 6.95604 loss)
I0422 05:19:14.278703  7583 sgd_solver.cpp:106] Iteration 1660, lr = 0.0008
I0422 05:20:53.825716  7583 solver.cpp:228] Iteration 1680, loss = 6.42115
I0422 05:20:53.826144  7583 solver.cpp:244]     Train net output #0: loss = 6.42115 (* 1 = 6.42115 loss)
I0422 05:20:53.826159  7583 sgd_solver.cpp:106] Iteration 1680, lr = 0.0008
I0422 05:22:33.873379  7583 solver.cpp:228] Iteration 1700, loss = 5.10023
I0422 05:22:33.873831  7583 solver.cpp:244]     Train net output #0: loss = 5.10023 (* 1 = 5.10023 loss)
I0422 05:22:33.873857  7583 sgd_solver.cpp:106] Iteration 1700, lr = 0.0008
I0422 05:24:12.587709  7583 solver.cpp:228] Iteration 1720, loss = 4.8034
I0422 05:24:12.588165  7583 solver.cpp:244]     Train net output #0: loss = 4.8034 (* 1 = 4.8034 loss)
I0422 05:24:12.588181  7583 sgd_solver.cpp:106] Iteration 1720, lr = 0.0008
I0422 05:25:52.880249  7583 solver.cpp:228] Iteration 1740, loss = 6.84421
I0422 05:25:52.880754  7583 solver.cpp:244]     Train net output #0: loss = 6.84421 (* 1 = 6.84421 loss)
I0422 05:25:52.880769  7583 sgd_solver.cpp:106] Iteration 1740, lr = 0.0008
I0422 05:27:32.804523  7583 solver.cpp:228] Iteration 1760, loss = 5.66193
I0422 05:27:32.804939  7583 solver.cpp:244]     Train net output #0: loss = 5.66193 (* 1 = 5.66193 loss)
I0422 05:27:32.804955  7583 sgd_solver.cpp:106] Iteration 1760, lr = 0.0008
I0422 05:29:12.577267  7583 solver.cpp:228] Iteration 1780, loss = 5.24584
I0422 05:29:12.577740  7583 solver.cpp:244]     Train net output #0: loss = 5.24584 (* 1 = 5.24584 loss)
I0422 05:29:12.577756  7583 sgd_solver.cpp:106] Iteration 1780, lr = 0.0008
I0422 05:30:53.845185  7583 solver.cpp:228] Iteration 1800, loss = 5.2073
I0422 05:30:53.845626  7583 solver.cpp:244]     Train net output #0: loss = 5.2073 (* 1 = 5.2073 loss)
I0422 05:30:53.845641  7583 sgd_solver.cpp:106] Iteration 1800, lr = 0.0008
I0422 05:32:33.906643  7583 solver.cpp:228] Iteration 1820, loss = 5.40361
I0422 05:32:33.907078  7583 solver.cpp:244]     Train net output #0: loss = 5.40361 (* 1 = 5.40361 loss)
I0422 05:32:33.907093  7583 sgd_solver.cpp:106] Iteration 1820, lr = 0.0008
I0422 05:34:15.111759  7583 solver.cpp:228] Iteration 1840, loss = 5.15907
I0422 05:34:15.112213  7583 solver.cpp:244]     Train net output #0: loss = 5.15907 (* 1 = 5.15907 loss)
I0422 05:34:15.112228  7583 sgd_solver.cpp:106] Iteration 1840, lr = 0.0008
I0422 05:35:55.688478  7583 solver.cpp:228] Iteration 1860, loss = 5.12707
I0422 05:35:55.688930  7583 solver.cpp:244]     Train net output #0: loss = 5.12707 (* 1 = 5.12707 loss)
I0422 05:35:55.688946  7583 sgd_solver.cpp:106] Iteration 1860, lr = 0.0008
I0422 05:37:35.906163  7583 solver.cpp:228] Iteration 1880, loss = 8.44916
I0422 05:37:35.906584  7583 solver.cpp:244]     Train net output #0: loss = 8.44916 (* 1 = 8.44916 loss)
I0422 05:37:35.906600  7583 sgd_solver.cpp:106] Iteration 1880, lr = 0.0008
I0422 05:39:16.992818  7583 solver.cpp:228] Iteration 1900, loss = 5.20096
I0422 05:39:16.993263  7583 solver.cpp:244]     Train net output #0: loss = 5.20096 (* 1 = 5.20096 loss)
I0422 05:39:16.993278  7583 sgd_solver.cpp:106] Iteration 1900, lr = 0.0008
I0422 05:40:58.341907  7583 solver.cpp:228] Iteration 1920, loss = 6.33867
I0422 05:40:58.342368  7583 solver.cpp:244]     Train net output #0: loss = 6.33867 (* 1 = 6.33867 loss)
I0422 05:40:58.342386  7583 sgd_solver.cpp:106] Iteration 1920, lr = 0.0008
I0422 05:42:38.309885  7583 solver.cpp:228] Iteration 1940, loss = 6.91829
I0422 05:42:38.310354  7583 solver.cpp:244]     Train net output #0: loss = 6.91829 (* 1 = 6.91829 loss)
I0422 05:42:38.310369  7583 sgd_solver.cpp:106] Iteration 1940, lr = 0.0008
I0422 05:44:17.605324  7583 solver.cpp:228] Iteration 1960, loss = 7.82788
I0422 05:44:17.605772  7583 solver.cpp:244]     Train net output #0: loss = 7.82788 (* 1 = 7.82788 loss)
I0422 05:44:17.605790  7583 sgd_solver.cpp:106] Iteration 1960, lr = 0.0008
I0422 05:45:56.342145  7583 solver.cpp:228] Iteration 1980, loss = 5.74759
I0422 05:45:56.342532  7583 solver.cpp:244]     Train net output #0: loss = 5.74759 (* 1 = 5.74759 loss)
I0422 05:45:56.342545  7583 sgd_solver.cpp:106] Iteration 1980, lr = 0.0008
I0422 05:47:30.406827  7583 solver.cpp:337] Iteration 2000, Testing net (#0)
I0422 05:48:23.027627  7583 solver.cpp:404]     Test net output #0: loss = 4.99074 (* 1 = 4.99074 loss)
I0422 05:48:25.187980  7583 solver.cpp:228] Iteration 2000, loss = 4.44618
I0422 05:48:25.188045  7583 solver.cpp:244]     Train net output #0: loss = 4.44618 (* 1 = 4.44618 loss)
I0422 05:48:25.188058  7583 sgd_solver.cpp:106] Iteration 2000, lr = 0.0008
I0422 05:50:05.182454  7583 solver.cpp:228] Iteration 2020, loss = 5.84075
I0422 05:50:05.182934  7583 solver.cpp:244]     Train net output #0: loss = 5.84075 (* 1 = 5.84075 loss)
I0422 05:50:05.182950  7583 sgd_solver.cpp:106] Iteration 2020, lr = 0.0008
I0422 05:51:45.065747  7583 solver.cpp:228] Iteration 2040, loss = 7.90629
I0422 05:51:45.066293  7583 solver.cpp:244]     Train net output #0: loss = 7.90629 (* 1 = 7.90629 loss)
I0422 05:51:45.066315  7583 sgd_solver.cpp:106] Iteration 2040, lr = 0.0008
I0422 05:53:26.467864  7583 solver.cpp:228] Iteration 2060, loss = 7.26895
I0422 05:53:26.468413  7583 solver.cpp:244]     Train net output #0: loss = 7.26895 (* 1 = 7.26895 loss)
I0422 05:53:26.468441  7583 sgd_solver.cpp:106] Iteration 2060, lr = 0.0008
I0422 05:55:07.372886  7583 solver.cpp:228] Iteration 2080, loss = 5.71229
I0422 05:55:07.373406  7583 solver.cpp:244]     Train net output #0: loss = 5.71229 (* 1 = 5.71229 loss)
I0422 05:55:07.373426  7583 sgd_solver.cpp:106] Iteration 2080, lr = 0.0008
I0422 05:56:48.070103  7583 solver.cpp:228] Iteration 2100, loss = 5.13302
I0422 05:56:48.070610  7583 solver.cpp:244]     Train net output #0: loss = 5.13302 (* 1 = 5.13302 loss)
I0422 05:56:48.070632  7583 sgd_solver.cpp:106] Iteration 2100, lr = 0.0008
I0422 05:58:29.255542  7583 solver.cpp:228] Iteration 2120, loss = 5.32518
I0422 05:58:29.256021  7583 solver.cpp:244]     Train net output #0: loss = 5.32518 (* 1 = 5.32518 loss)
I0422 05:58:29.256044  7583 sgd_solver.cpp:106] Iteration 2120, lr = 0.0008
I0422 06:00:11.023109  7583 solver.cpp:228] Iteration 2140, loss = 5.86488
I0422 06:00:11.023548  7583 solver.cpp:244]     Train net output #0: loss = 5.86488 (* 1 = 5.86488 loss)
I0422 06:00:11.023574  7583 sgd_solver.cpp:106] Iteration 2140, lr = 0.0008
I0422 06:01:52.949326  7583 solver.cpp:228] Iteration 2160, loss = 6.01145
I0422 06:01:52.949810  7583 solver.cpp:244]     Train net output #0: loss = 6.01145 (* 1 = 6.01145 loss)
I0422 06:01:52.949833  7583 sgd_solver.cpp:106] Iteration 2160, lr = 0.0008
I0422 06:03:35.109794  7583 solver.cpp:228] Iteration 2180, loss = 5.47394
I0422 06:03:35.110173  7583 solver.cpp:244]     Train net output #0: loss = 5.47394 (* 1 = 5.47394 loss)
I0422 06:03:35.110193  7583 sgd_solver.cpp:106] Iteration 2180, lr = 0.0008
I0422 06:05:20.984024  7583 solver.cpp:228] Iteration 2200, loss = 6.10942
I0422 06:05:20.984580  7583 solver.cpp:244]     Train net output #0: loss = 6.10942 (* 1 = 6.10942 loss)
I0422 06:05:20.984606  7583 sgd_solver.cpp:106] Iteration 2200, lr = 0.0008
I0422 06:07:13.404321  7583 solver.cpp:228] Iteration 2220, loss = 6.66471
I0422 06:07:13.404884  7583 solver.cpp:244]     Train net output #0: loss = 6.66471 (* 1 = 6.66471 loss)
I0422 06:07:13.404919  7583 sgd_solver.cpp:106] Iteration 2220, lr = 0.0008
I0422 06:09:00.992854  7583 solver.cpp:228] Iteration 2240, loss = 8.17928
I0422 06:09:00.993365  7583 solver.cpp:244]     Train net output #0: loss = 8.17928 (* 1 = 8.17928 loss)
I0422 06:09:00.993381  7583 sgd_solver.cpp:106] Iteration 2240, lr = 0.0008
I0422 06:10:49.766204  7583 solver.cpp:228] Iteration 2260, loss = 8.80991
I0422 06:10:49.766726  7583 solver.cpp:244]     Train net output #0: loss = 8.80991 (* 1 = 8.80991 loss)
I0422 06:10:49.766743  7583 sgd_solver.cpp:106] Iteration 2260, lr = 0.0008
I0422 06:12:37.911427  7583 solver.cpp:228] Iteration 2280, loss = 16.4161
I0422 06:12:37.911903  7583 solver.cpp:244]     Train net output #0: loss = 16.4161 (* 1 = 16.4161 loss)
I0422 06:12:37.911921  7583 sgd_solver.cpp:106] Iteration 2280, lr = 0.0008
I0422 06:14:24.688668  7583 solver.cpp:228] Iteration 2300, loss = 17.6279
I0422 06:14:24.689164  7583 solver.cpp:244]     Train net output #0: loss = 17.6279 (* 1 = 17.6279 loss)
I0422 06:14:24.689178  7583 sgd_solver.cpp:106] Iteration 2300, lr = 0.0008
I0422 06:16:12.252084  7583 solver.cpp:228] Iteration 2320, loss = 40.7583
I0422 06:16:12.252619  7583 solver.cpp:244]     Train net output #0: loss = 40.7583 (* 1 = 40.7583 loss)
I0422 06:16:12.252635  7583 sgd_solver.cpp:106] Iteration 2320, lr = 0.0008
I0422 06:17:58.773382  7583 solver.cpp:228] Iteration 2340, loss = 13.1837
I0422 06:17:58.773895  7583 solver.cpp:244]     Train net output #0: loss = 13.1837 (* 1 = 13.1837 loss)
I0422 06:17:58.773910  7583 sgd_solver.cpp:106] Iteration 2340, lr = 0.0008
I0422 06:19:45.840453  7583 solver.cpp:228] Iteration 2360, loss = 7.5149
I0422 06:19:45.841083  7583 solver.cpp:244]     Train net output #0: loss = 7.5149 (* 1 = 7.5149 loss)
I0422 06:19:45.841100  7583 sgd_solver.cpp:106] Iteration 2360, lr = 0.0008
I0422 06:21:35.226922  7583 solver.cpp:228] Iteration 2380, loss = 6.84052
I0422 06:21:35.227475  7583 solver.cpp:244]     Train net output #0: loss = 6.84052 (* 1 = 6.84052 loss)
I0422 06:21:35.227491  7583 sgd_solver.cpp:106] Iteration 2380, lr = 0.0008
I0422 06:23:25.735641  7583 solver.cpp:228] Iteration 2400, loss = 5.4434
I0422 06:23:25.736165  7583 solver.cpp:244]     Train net output #0: loss = 5.4434 (* 1 = 5.4434 loss)
I0422 06:23:25.736178  7583 sgd_solver.cpp:106] Iteration 2400, lr = 0.0008
I0422 06:25:10.340204  7583 solver.cpp:228] Iteration 2420, loss = 5.46132
I0422 06:25:10.340735  7583 solver.cpp:244]     Train net output #0: loss = 5.46132 (* 1 = 5.46132 loss)
I0422 06:25:10.340750  7583 sgd_solver.cpp:106] Iteration 2420, lr = 0.0008
I0422 06:26:56.945081  7583 solver.cpp:228] Iteration 2440, loss = 5.48539
I0422 06:26:56.945632  7583 solver.cpp:244]     Train net output #0: loss = 5.48539 (* 1 = 5.48539 loss)
I0422 06:26:56.945648  7583 sgd_solver.cpp:106] Iteration 2440, lr = 0.0008
I0422 06:28:43.885341  7583 solver.cpp:228] Iteration 2460, loss = 6.82878
I0422 06:28:43.885856  7583 solver.cpp:244]     Train net output #0: loss = 6.82878 (* 1 = 6.82878 loss)
I0422 06:28:43.885870  7583 sgd_solver.cpp:106] Iteration 2460, lr = 0.0008
I0422 06:30:30.280414  7583 solver.cpp:228] Iteration 2480, loss = 5.32323
I0422 06:30:30.281007  7583 solver.cpp:244]     Train net output #0: loss = 5.32323 (* 1 = 5.32323 loss)
I0422 06:30:30.281024  7583 sgd_solver.cpp:106] Iteration 2480, lr = 0.0008
I0422 06:32:15.925050  7583 solver.cpp:228] Iteration 2500, loss = 5.99908
I0422 06:32:15.925592  7583 solver.cpp:244]     Train net output #0: loss = 5.99908 (* 1 = 5.99908 loss)
I0422 06:32:15.925614  7583 sgd_solver.cpp:106] Iteration 2500, lr = 0.0008
I0422 06:34:02.167315  7583 solver.cpp:228] Iteration 2520, loss = 9.42366
I0422 06:34:02.167794  7583 solver.cpp:244]     Train net output #0: loss = 9.42366 (* 1 = 9.42366 loss)
I0422 06:34:02.167809  7583 sgd_solver.cpp:106] Iteration 2520, lr = 0.0008
I0422 06:35:48.842463  7583 solver.cpp:228] Iteration 2540, loss = 10.4692
I0422 06:35:48.842917  7583 solver.cpp:244]     Train net output #0: loss = 10.4692 (* 1 = 10.4692 loss)
I0422 06:35:48.842932  7583 sgd_solver.cpp:106] Iteration 2540, lr = 0.0008
I0422 06:37:35.190242  7583 solver.cpp:228] Iteration 2560, loss = 5.83
I0422 06:37:35.190759  7583 solver.cpp:244]     Train net output #0: loss = 5.83 (* 1 = 5.83 loss)
I0422 06:37:35.190776  7583 sgd_solver.cpp:106] Iteration 2560, lr = 0.0008
I0422 06:39:21.298142  7583 solver.cpp:228] Iteration 2580, loss = 4.93876
I0422 06:39:21.298674  7583 solver.cpp:244]     Train net output #0: loss = 4.93876 (* 1 = 4.93876 loss)
I0422 06:39:21.298696  7583 sgd_solver.cpp:106] Iteration 2580, lr = 0.0008
I0422 06:41:06.947494  7583 solver.cpp:228] Iteration 2600, loss = 5.47687
I0422 06:41:06.947998  7583 solver.cpp:244]     Train net output #0: loss = 5.47687 (* 1 = 5.47687 loss)
I0422 06:41:06.948014  7583 sgd_solver.cpp:106] Iteration 2600, lr = 0.0008
I0422 06:42:54.453887  7583 solver.cpp:228] Iteration 2620, loss = 7.3411
I0422 06:42:54.454387  7583 solver.cpp:244]     Train net output #0: loss = 7.3411 (* 1 = 7.3411 loss)
I0422 06:42:54.454402  7583 sgd_solver.cpp:106] Iteration 2620, lr = 0.0008
I0422 06:44:41.439200  7583 solver.cpp:228] Iteration 2640, loss = 8.11791
I0422 06:44:41.439757  7583 solver.cpp:244]     Train net output #0: loss = 8.11791 (* 1 = 8.11791 loss)
I0422 06:44:41.439774  7583 sgd_solver.cpp:106] Iteration 2640, lr = 0.0008
I0422 06:46:28.657373  7583 solver.cpp:228] Iteration 2660, loss = 6.12013
I0422 06:46:28.657861  7583 solver.cpp:244]     Train net output #0: loss = 6.12013 (* 1 = 6.12013 loss)
I0422 06:46:28.657876  7583 sgd_solver.cpp:106] Iteration 2660, lr = 0.0008
I0422 06:48:15.715500  7583 solver.cpp:228] Iteration 2680, loss = 5.65086
I0422 06:48:15.715935  7583 solver.cpp:244]     Train net output #0: loss = 5.65086 (* 1 = 5.65086 loss)
I0422 06:48:15.715950  7583 sgd_solver.cpp:106] Iteration 2680, lr = 0.0008
I0422 06:50:03.371976  7583 solver.cpp:228] Iteration 2700, loss = 7.55886
I0422 06:50:03.372423  7583 solver.cpp:244]     Train net output #0: loss = 7.55886 (* 1 = 7.55886 loss)
I0422 06:50:03.372438  7583 sgd_solver.cpp:106] Iteration 2700, lr = 0.0008
I0422 06:51:53.171798  7583 solver.cpp:228] Iteration 2720, loss = 6.45612
I0422 06:51:53.172303  7583 solver.cpp:244]     Train net output #0: loss = 6.45612 (* 1 = 6.45612 loss)
I0422 06:51:53.172315  7583 sgd_solver.cpp:106] Iteration 2720, lr = 0.0008
I0422 06:53:41.758059  7583 solver.cpp:228] Iteration 2740, loss = 5.7828
I0422 06:53:41.758626  7583 solver.cpp:244]     Train net output #0: loss = 5.7828 (* 1 = 5.7828 loss)
I0422 06:53:41.758642  7583 sgd_solver.cpp:106] Iteration 2740, lr = 0.0008
I0422 06:55:30.181120  7583 solver.cpp:228] Iteration 2760, loss = 4.54523
I0422 06:55:30.181646  7583 solver.cpp:244]     Train net output #0: loss = 4.54523 (* 1 = 4.54523 loss)
I0422 06:55:30.181663  7583 sgd_solver.cpp:106] Iteration 2760, lr = 0.0008
I0422 06:57:19.151360  7583 solver.cpp:228] Iteration 2780, loss = 5.96219
I0422 06:57:19.151926  7583 solver.cpp:244]     Train net output #0: loss = 5.96219 (* 1 = 5.96219 loss)
I0422 06:57:19.151943  7583 sgd_solver.cpp:106] Iteration 2780, lr = 0.0008
I0422 06:59:08.419325  7583 solver.cpp:228] Iteration 2800, loss = 4.76075
I0422 06:59:08.419884  7583 solver.cpp:244]     Train net output #0: loss = 4.76075 (* 1 = 4.76075 loss)
I0422 06:59:08.419899  7583 sgd_solver.cpp:106] Iteration 2800, lr = 0.0008
I0422 07:00:57.984946  7583 solver.cpp:228] Iteration 2820, loss = 4.64587
I0422 07:00:57.985496  7583 solver.cpp:244]     Train net output #0: loss = 4.64587 (* 1 = 4.64587 loss)
I0422 07:00:57.985523  7583 sgd_solver.cpp:106] Iteration 2820, lr = 0.0008
I0422 07:02:45.055898  7583 solver.cpp:228] Iteration 2840, loss = 4.88091
I0422 07:02:45.056488  7583 solver.cpp:244]     Train net output #0: loss = 4.88091 (* 1 = 4.88091 loss)
I0422 07:02:45.056510  7583 sgd_solver.cpp:106] Iteration 2840, lr = 0.0008
I0422 07:04:35.053066  7583 solver.cpp:228] Iteration 2860, loss = 5.11888
I0422 07:04:35.053612  7583 solver.cpp:244]     Train net output #0: loss = 5.11888 (* 1 = 5.11888 loss)
I0422 07:04:35.053628  7583 sgd_solver.cpp:106] Iteration 2860, lr = 0.0008
I0422 07:06:22.831951  7583 solver.cpp:228] Iteration 2880, loss = 5.45558
I0422 07:06:22.832530  7583 solver.cpp:244]     Train net output #0: loss = 5.45558 (* 1 = 5.45558 loss)
I0422 07:06:22.832545  7583 sgd_solver.cpp:106] Iteration 2880, lr = 0.0008
I0422 07:08:11.812788  7583 solver.cpp:228] Iteration 2900, loss = 5.12419
I0422 07:08:11.813297  7583 solver.cpp:244]     Train net output #0: loss = 5.12419 (* 1 = 5.12419 loss)
I0422 07:08:11.813313  7583 sgd_solver.cpp:106] Iteration 2900, lr = 0.0008
I0422 07:10:02.123205  7583 solver.cpp:228] Iteration 2920, loss = 8.46243
I0422 07:10:02.123770  7583 solver.cpp:244]     Train net output #0: loss = 8.46243 (* 1 = 8.46243 loss)
I0422 07:10:02.123788  7583 sgd_solver.cpp:106] Iteration 2920, lr = 0.0008
I0422 07:11:50.940038  7583 solver.cpp:228] Iteration 2940, loss = 7.99634
I0422 07:11:50.940570  7583 solver.cpp:244]     Train net output #0: loss = 7.99634 (* 1 = 7.99634 loss)
I0422 07:11:50.940600  7583 sgd_solver.cpp:106] Iteration 2940, lr = 0.0008
I0422 07:13:31.934763  7583 solver.cpp:228] Iteration 2960, loss = 5.7992
I0422 07:13:31.935250  7583 solver.cpp:244]     Train net output #0: loss = 5.7992 (* 1 = 5.7992 loss)
I0422 07:13:31.935271  7583 sgd_solver.cpp:106] Iteration 2960, lr = 0.0008
I0422 07:15:14.295243  7583 solver.cpp:228] Iteration 2980, loss = 5.70709
I0422 07:15:14.295732  7583 solver.cpp:244]     Train net output #0: loss = 5.70709 (* 1 = 5.70709 loss)
I0422 07:15:14.295754  7583 sgd_solver.cpp:106] Iteration 2980, lr = 0.0008
I0422 07:16:52.438761  7583 solver.cpp:337] Iteration 3000, Testing net (#0)
I0422 07:17:49.240242  7583 solver.cpp:404]     Test net output #0: loss = 4.8708 (* 1 = 4.8708 loss)
I0422 07:17:51.685593  7583 solver.cpp:228] Iteration 3000, loss = 5.38411
I0422 07:17:51.685668  7583 solver.cpp:244]     Train net output #0: loss = 5.38411 (* 1 = 5.38411 loss)
I0422 07:17:51.685683  7583 sgd_solver.cpp:106] Iteration 3000, lr = 0.0008
I0422 07:19:33.810812  7583 solver.cpp:228] Iteration 3020, loss = 7.84551
I0422 07:19:33.811336  7583 solver.cpp:244]     Train net output #0: loss = 7.84551 (* 1 = 7.84551 loss)
I0422 07:19:33.811353  7583 sgd_solver.cpp:106] Iteration 3020, lr = 0.0008
I0422 07:21:15.667275  7583 solver.cpp:228] Iteration 3040, loss = 6.77501
I0422 07:21:15.667784  7583 solver.cpp:244]     Train net output #0: loss = 6.77501 (* 1 = 6.77501 loss)
I0422 07:21:15.667806  7583 sgd_solver.cpp:106] Iteration 3040, lr = 0.0008
I0422 07:22:58.719342  7583 solver.cpp:228] Iteration 3060, loss = 9.07936
I0422 07:22:58.719861  7583 solver.cpp:244]     Train net output #0: loss = 9.07936 (* 1 = 9.07936 loss)
I0422 07:22:58.719884  7583 sgd_solver.cpp:106] Iteration 3060, lr = 0.0008
I0422 07:24:41.994773  7583 solver.cpp:228] Iteration 3080, loss = 6.74428
I0422 07:24:41.995272  7583 solver.cpp:244]     Train net output #0: loss = 6.74428 (* 1 = 6.74428 loss)
I0422 07:24:41.995295  7583 sgd_solver.cpp:106] Iteration 3080, lr = 0.0008
I0422 07:26:24.680553  7583 solver.cpp:228] Iteration 3100, loss = 5.78768
I0422 07:26:24.681051  7583 solver.cpp:244]     Train net output #0: loss = 5.78768 (* 1 = 5.78768 loss)
I0422 07:26:24.681071  7583 sgd_solver.cpp:106] Iteration 3100, lr = 0.0008
I0422 07:28:06.944700  7583 solver.cpp:228] Iteration 3120, loss = 4.76818
I0422 07:28:06.945219  7583 solver.cpp:244]     Train net output #0: loss = 4.76818 (* 1 = 4.76818 loss)
I0422 07:28:06.945240  7583 sgd_solver.cpp:106] Iteration 3120, lr = 0.0008
I0422 07:29:48.323216  7583 solver.cpp:228] Iteration 3140, loss = 4.74779
I0422 07:29:48.323684  7583 solver.cpp:244]     Train net output #0: loss = 4.74779 (* 1 = 4.74779 loss)
I0422 07:29:48.323707  7583 sgd_solver.cpp:106] Iteration 3140, lr = 0.0008
I0422 07:31:30.767209  7583 solver.cpp:228] Iteration 3160, loss = 5.42501
I0422 07:31:30.767688  7583 solver.cpp:244]     Train net output #0: loss = 5.42501 (* 1 = 5.42501 loss)
I0422 07:31:30.767711  7583 sgd_solver.cpp:106] Iteration 3160, lr = 0.0008
I0422 07:33:11.840982  7583 solver.cpp:228] Iteration 3180, loss = 7.56946
I0422 07:33:11.841460  7583 solver.cpp:244]     Train net output #0: loss = 7.56946 (* 1 = 7.56946 loss)
I0422 07:33:11.841477  7583 sgd_solver.cpp:106] Iteration 3180, lr = 0.0008
I0422 07:34:54.662802  7583 solver.cpp:228] Iteration 3200, loss = 7.30702
I0422 07:34:54.663331  7583 solver.cpp:244]     Train net output #0: loss = 7.30702 (* 1 = 7.30702 loss)
I0422 07:34:54.663355  7583 sgd_solver.cpp:106] Iteration 3200, lr = 0.0008
I0422 07:36:37.667719  7583 solver.cpp:228] Iteration 3220, loss = 27.0241
I0422 07:36:37.668248  7583 solver.cpp:244]     Train net output #0: loss = 27.0241 (* 1 = 27.0241 loss)
I0422 07:36:37.668267  7583 sgd_solver.cpp:106] Iteration 3220, lr = 0.0008
I0422 07:38:20.872555  7583 solver.cpp:228] Iteration 3240, loss = 6.59015
I0422 07:38:20.873087  7583 solver.cpp:244]     Train net output #0: loss = 6.59015 (* 1 = 6.59015 loss)
I0422 07:38:20.873111  7583 sgd_solver.cpp:106] Iteration 3240, lr = 0.0008
I0422 07:40:01.867434  7583 solver.cpp:228] Iteration 3260, loss = 6.09617
I0422 07:40:01.867975  7583 solver.cpp:244]     Train net output #0: loss = 6.09617 (* 1 = 6.09617 loss)
I0422 07:40:01.867996  7583 sgd_solver.cpp:106] Iteration 3260, lr = 0.0008
I0422 07:41:43.141595  7583 solver.cpp:228] Iteration 3280, loss = 6.6265
I0422 07:41:43.142088  7583 solver.cpp:244]     Train net output #0: loss = 6.6265 (* 1 = 6.6265 loss)
I0422 07:41:43.142113  7583 sgd_solver.cpp:106] Iteration 3280, lr = 0.0008
I0422 07:43:25.923313  7583 solver.cpp:228] Iteration 3300, loss = 8.21201
I0422 07:43:25.923774  7583 solver.cpp:244]     Train net output #0: loss = 8.21201 (* 1 = 8.21201 loss)
I0422 07:43:25.923789  7583 sgd_solver.cpp:106] Iteration 3300, lr = 0.0008
I0422 07:45:08.376019  7583 solver.cpp:228] Iteration 3320, loss = 8.85399
I0422 07:45:08.376505  7583 solver.cpp:244]     Train net output #0: loss = 8.85399 (* 1 = 8.85399 loss)
I0422 07:45:08.376520  7583 sgd_solver.cpp:106] Iteration 3320, lr = 0.0008
I0422 07:46:50.957533  7583 solver.cpp:228] Iteration 3340, loss = 6.44317
I0422 07:46:50.958089  7583 solver.cpp:244]     Train net output #0: loss = 6.44317 (* 1 = 6.44317 loss)
I0422 07:46:50.958106  7583 sgd_solver.cpp:106] Iteration 3340, lr = 0.0008
I0422 07:48:32.219131  7583 solver.cpp:228] Iteration 3360, loss = 6.76937
I0422 07:48:32.219621  7583 solver.cpp:244]     Train net output #0: loss = 6.76937 (* 1 = 6.76937 loss)
I0422 07:48:32.219642  7583 sgd_solver.cpp:106] Iteration 3360, lr = 0.0008
I0422 07:50:15.206019  7583 solver.cpp:228] Iteration 3380, loss = 9.29031
I0422 07:50:15.206570  7583 solver.cpp:244]     Train net output #0: loss = 9.29031 (* 1 = 9.29031 loss)
I0422 07:50:15.206598  7583 sgd_solver.cpp:106] Iteration 3380, lr = 0.0008
I0422 07:51:58.527806  7583 solver.cpp:228] Iteration 3400, loss = 6.97947
I0422 07:51:58.528317  7583 solver.cpp:244]     Train net output #0: loss = 6.97947 (* 1 = 6.97947 loss)
I0422 07:51:58.528334  7583 sgd_solver.cpp:106] Iteration 3400, lr = 0.0008
I0422 07:53:40.213618  7583 solver.cpp:228] Iteration 3420, loss = 7.48651
I0422 07:53:40.214098  7583 solver.cpp:244]     Train net output #0: loss = 7.48651 (* 1 = 7.48651 loss)
I0422 07:53:40.214113  7583 sgd_solver.cpp:106] Iteration 3420, lr = 0.0008
I0422 07:55:22.443919  7583 solver.cpp:228] Iteration 3440, loss = 9.7316
I0422 07:55:22.444422  7583 solver.cpp:244]     Train net output #0: loss = 9.7316 (* 1 = 9.7316 loss)
I0422 07:55:22.444438  7583 sgd_solver.cpp:106] Iteration 3440, lr = 0.0008
I0422 07:57:05.676378  7583 solver.cpp:228] Iteration 3460, loss = 10.5245
I0422 07:57:05.676856  7583 solver.cpp:244]     Train net output #0: loss = 10.5245 (* 1 = 10.5245 loss)
I0422 07:57:05.676872  7583 sgd_solver.cpp:106] Iteration 3460, lr = 0.0008
I0422 07:58:48.794826  7583 solver.cpp:228] Iteration 3480, loss = 18.6695
I0422 07:58:48.795353  7583 solver.cpp:244]     Train net output #0: loss = 18.6695 (* 1 = 18.6695 loss)
I0422 07:58:48.795379  7583 sgd_solver.cpp:106] Iteration 3480, lr = 0.0008
I0422 08:00:32.191601  7583 solver.cpp:228] Iteration 3500, loss = 4.81615
I0422 08:00:32.192072  7583 solver.cpp:244]     Train net output #0: loss = 4.81615 (* 1 = 4.81615 loss)
I0422 08:00:32.192095  7583 sgd_solver.cpp:106] Iteration 3500, lr = 0.0008
I0422 08:02:16.293921  7583 solver.cpp:228] Iteration 3520, loss = 5.97816
I0422 08:02:16.294445  7583 solver.cpp:244]     Train net output #0: loss = 5.97816 (* 1 = 5.97816 loss)
I0422 08:02:16.294466  7583 sgd_solver.cpp:106] Iteration 3520, lr = 0.0008
I0422 08:04:00.636904  7583 solver.cpp:228] Iteration 3540, loss = 5.2529
I0422 08:04:00.637451  7583 solver.cpp:244]     Train net output #0: loss = 5.2529 (* 1 = 5.2529 loss)
I0422 08:04:00.637470  7583 sgd_solver.cpp:106] Iteration 3540, lr = 0.0008
I0422 08:05:46.126791  7583 solver.cpp:228] Iteration 3560, loss = 5.73947
I0422 08:05:46.127328  7583 solver.cpp:244]     Train net output #0: loss = 5.73947 (* 1 = 5.73947 loss)
I0422 08:05:46.127358  7583 sgd_solver.cpp:106] Iteration 3560, lr = 0.0008
I0422 08:07:28.762027  7583 solver.cpp:228] Iteration 3580, loss = 6.43355
I0422 08:07:28.762495  7583 solver.cpp:244]     Train net output #0: loss = 6.43355 (* 1 = 6.43355 loss)
I0422 08:07:28.762511  7583 sgd_solver.cpp:106] Iteration 3580, lr = 0.0008
I0422 08:09:08.770380  7583 solver.cpp:228] Iteration 3600, loss = 6.88982
I0422 08:09:08.770896  7583 solver.cpp:244]     Train net output #0: loss = 6.88982 (* 1 = 6.88982 loss)
I0422 08:09:08.770920  7583 sgd_solver.cpp:106] Iteration 3600, lr = 0.0008
I0422 08:10:50.249335  7583 solver.cpp:228] Iteration 3620, loss = 46.3836
I0422 08:10:50.249892  7583 solver.cpp:244]     Train net output #0: loss = 46.3836 (* 1 = 46.3836 loss)
I0422 08:10:50.249912  7583 sgd_solver.cpp:106] Iteration 3620, lr = 0.0008
I0422 08:12:31.845266  7583 solver.cpp:228] Iteration 3640, loss = 12.8483
I0422 08:12:31.845715  7583 solver.cpp:244]     Train net output #0: loss = 12.8483 (* 1 = 12.8483 loss)
I0422 08:12:31.845736  7583 sgd_solver.cpp:106] Iteration 3640, lr = 0.0008
I0422 08:14:14.232848  7583 solver.cpp:228] Iteration 3660, loss = 6.16935
I0422 08:14:14.233355  7583 solver.cpp:244]     Train net output #0: loss = 6.16935 (* 1 = 6.16935 loss)
I0422 08:14:14.233371  7583 sgd_solver.cpp:106] Iteration 3660, lr = 0.0008
I0422 08:15:56.979046  7583 solver.cpp:228] Iteration 3680, loss = 5.21437
I0422 08:15:56.979430  7583 solver.cpp:244]     Train net output #0: loss = 5.21437 (* 1 = 5.21437 loss)
I0422 08:15:56.979445  7583 sgd_solver.cpp:106] Iteration 3680, lr = 0.0008
I0422 08:17:36.730780  7583 solver.cpp:228] Iteration 3700, loss = 4.96285
I0422 08:17:36.731339  7583 solver.cpp:244]     Train net output #0: loss = 4.96285 (* 1 = 4.96285 loss)
I0422 08:17:36.731365  7583 sgd_solver.cpp:106] Iteration 3700, lr = 0.0008
I0422 08:19:15.743810  7583 solver.cpp:228] Iteration 3720, loss = 4.8949
I0422 08:19:15.744282  7583 solver.cpp:244]     Train net output #0: loss = 4.8949 (* 1 = 4.8949 loss)
I0422 08:19:15.744300  7583 sgd_solver.cpp:106] Iteration 3720, lr = 0.0008
I0422 08:20:55.778982  7583 solver.cpp:228] Iteration 3740, loss = 5.75137
I0422 08:20:55.779424  7583 solver.cpp:244]     Train net output #0: loss = 5.75137 (* 1 = 5.75137 loss)
I0422 08:20:55.779439  7583 sgd_solver.cpp:106] Iteration 3740, lr = 0.0008
I0422 08:22:35.168478  7583 solver.cpp:228] Iteration 3760, loss = 7.41491
I0422 08:22:35.168856  7583 solver.cpp:244]     Train net output #0: loss = 7.41491 (* 1 = 7.41491 loss)
I0422 08:22:35.168872  7583 sgd_solver.cpp:106] Iteration 3760, lr = 0.0008
I0422 08:24:14.978070  7583 solver.cpp:228] Iteration 3780, loss = 7.18667
I0422 08:24:14.978502  7583 solver.cpp:244]     Train net output #0: loss = 7.18667 (* 1 = 7.18667 loss)
I0422 08:24:14.978516  7583 sgd_solver.cpp:106] Iteration 3780, lr = 0.0008
I0422 08:25:54.528866  7583 solver.cpp:228] Iteration 3800, loss = 5.20376
I0422 08:25:54.529311  7583 solver.cpp:244]     Train net output #0: loss = 5.20376 (* 1 = 5.20376 loss)
I0422 08:25:54.529326  7583 sgd_solver.cpp:106] Iteration 3800, lr = 0.0008
I0422 08:27:32.699769  7583 solver.cpp:228] Iteration 3820, loss = 5.70517
I0422 08:27:32.700240  7583 solver.cpp:244]     Train net output #0: loss = 5.70517 (* 1 = 5.70517 loss)
I0422 08:27:32.700256  7583 sgd_solver.cpp:106] Iteration 3820, lr = 0.0008
I0422 08:29:12.341111  7583 solver.cpp:228] Iteration 3840, loss = 6.4492
I0422 08:29:12.341558  7583 solver.cpp:244]     Train net output #0: loss = 6.4492 (* 1 = 6.4492 loss)
I0422 08:29:12.341573  7583 sgd_solver.cpp:106] Iteration 3840, lr = 0.0008
I0422 08:30:52.481001  7583 solver.cpp:228] Iteration 3860, loss = 5.08478
I0422 08:30:52.481477  7583 solver.cpp:244]     Train net output #0: loss = 5.08478 (* 1 = 5.08478 loss)
I0422 08:30:52.481492  7583 sgd_solver.cpp:106] Iteration 3860, lr = 0.0008
I0422 08:32:31.926439  7583 solver.cpp:228] Iteration 3880, loss = 4.80931
I0422 08:32:31.926877  7583 solver.cpp:244]     Train net output #0: loss = 4.80931 (* 1 = 4.80931 loss)
I0422 08:32:31.926890  7583 sgd_solver.cpp:106] Iteration 3880, lr = 0.0008
I0422 08:34:11.680449  7583 solver.cpp:228] Iteration 3900, loss = 8.11039
I0422 08:34:11.680917  7583 solver.cpp:244]     Train net output #0: loss = 8.11039 (* 1 = 8.11039 loss)
I0422 08:34:11.680932  7583 sgd_solver.cpp:106] Iteration 3900, lr = 0.0008
I0422 08:35:51.931577  7583 solver.cpp:228] Iteration 3920, loss = 6.28272
I0422 08:35:51.932004  7583 solver.cpp:244]     Train net output #0: loss = 6.28272 (* 1 = 6.28272 loss)
I0422 08:35:51.932025  7583 sgd_solver.cpp:106] Iteration 3920, lr = 0.0008
I0422 08:37:31.293138  7583 solver.cpp:228] Iteration 3940, loss = 7.13623
I0422 08:37:31.293601  7583 solver.cpp:244]     Train net output #0: loss = 7.13623 (* 1 = 7.13623 loss)
I0422 08:37:31.293617  7583 sgd_solver.cpp:106] Iteration 3940, lr = 0.0008
I0422 08:39:09.269855  7583 solver.cpp:228] Iteration 3960, loss = 5.0163
I0422 08:39:09.270295  7583 solver.cpp:244]     Train net output #0: loss = 5.0163 (* 1 = 5.0163 loss)
I0422 08:39:09.270310  7583 sgd_solver.cpp:106] Iteration 3960, lr = 0.0008
I0422 08:40:48.414160  7583 solver.cpp:228] Iteration 3980, loss = 8.77973
I0422 08:40:48.414629  7583 solver.cpp:244]     Train net output #0: loss = 8.77973 (* 1 = 8.77973 loss)
I0422 08:40:48.414645  7583 sgd_solver.cpp:106] Iteration 3980, lr = 0.0008
I0422 08:42:22.211210  7583 solver.cpp:337] Iteration 4000, Testing net (#0)
I0422 08:43:15.042325  7583 solver.cpp:404]     Test net output #0: loss = 32.6814 (* 1 = 32.6814 loss)
I0422 08:43:17.417549  7583 solver.cpp:228] Iteration 4000, loss = 26.561
I0422 08:43:17.417606  7583 solver.cpp:244]     Train net output #0: loss = 26.561 (* 1 = 26.561 loss)
I0422 08:43:17.417619  7583 sgd_solver.cpp:106] Iteration 4000, lr = 0.0008
I0422 08:44:56.468516  7583 solver.cpp:228] Iteration 4020, loss = 93.984
I0422 08:44:56.468967  7583 solver.cpp:244]     Train net output #0: loss = 93.984 (* 1 = 93.984 loss)
I0422 08:44:56.468982  7583 sgd_solver.cpp:106] Iteration 4020, lr = 0.0008
I0422 08:46:36.009037  7583 solver.cpp:228] Iteration 4040, loss = 253.546
I0422 08:46:36.009410  7583 solver.cpp:244]     Train net output #0: loss = 253.546 (* 1 = 253.546 loss)
I0422 08:46:36.009426  7583 sgd_solver.cpp:106] Iteration 4040, lr = 0.0008
I0422 08:48:15.620885  7583 solver.cpp:228] Iteration 4060, loss = 25.2658
I0422 08:48:15.621325  7583 solver.cpp:244]     Train net output #0: loss = 25.2658 (* 1 = 25.2658 loss)
I0422 08:48:15.621343  7583 sgd_solver.cpp:106] Iteration 4060, lr = 0.0008
I0422 08:49:57.139981  7583 solver.cpp:228] Iteration 4080, loss = 9.92493
I0422 08:49:57.140465  7583 solver.cpp:244]     Train net output #0: loss = 9.92493 (* 1 = 9.92493 loss)
I0422 08:49:57.140480  7583 sgd_solver.cpp:106] Iteration 4080, lr = 0.0008
I0422 08:51:37.144752  7583 solver.cpp:228] Iteration 4100, loss = 8.9408
I0422 08:51:37.145232  7583 solver.cpp:244]     Train net output #0: loss = 8.9408 (* 1 = 8.9408 loss)
I0422 08:51:37.145247  7583 sgd_solver.cpp:106] Iteration 4100, lr = 0.0008
I0422 08:53:16.137529  7583 solver.cpp:228] Iteration 4120, loss = 7.49296
I0422 08:53:16.138027  7583 solver.cpp:244]     Train net output #0: loss = 7.49296 (* 1 = 7.49296 loss)
I0422 08:53:16.138041  7583 sgd_solver.cpp:106] Iteration 4120, lr = 0.0008
I0422 08:54:55.874341  7583 solver.cpp:228] Iteration 4140, loss = 11.8617
I0422 08:54:55.874784  7583 solver.cpp:244]     Train net output #0: loss = 11.8617 (* 1 = 11.8617 loss)
I0422 08:54:55.874799  7583 sgd_solver.cpp:106] Iteration 4140, lr = 0.0008
I0422 08:56:34.593374  7583 solver.cpp:228] Iteration 4160, loss = 5.70494
I0422 08:56:34.593875  7583 solver.cpp:244]     Train net output #0: loss = 5.70494 (* 1 = 5.70494 loss)
I0422 08:56:34.593890  7583 sgd_solver.cpp:106] Iteration 4160, lr = 0.0008
I0422 08:58:14.020867  7583 solver.cpp:228] Iteration 4180, loss = 4.94861
I0422 08:58:14.021389  7583 solver.cpp:244]     Train net output #0: loss = 4.94861 (* 1 = 4.94861 loss)
I0422 08:58:14.021410  7583 sgd_solver.cpp:106] Iteration 4180, lr = 0.0008
I0422 08:59:54.276854  7583 solver.cpp:228] Iteration 4200, loss = 5.99589
I0422 08:59:54.277395  7583 solver.cpp:244]     Train net output #0: loss = 5.99589 (* 1 = 5.99589 loss)
I0422 08:59:54.277412  7583 sgd_solver.cpp:106] Iteration 4200, lr = 0.0008
I0422 09:01:34.267539  7583 solver.cpp:228] Iteration 4220, loss = 8.97165
I0422 09:01:34.268100  7583 solver.cpp:244]     Train net output #0: loss = 8.97165 (* 1 = 8.97165 loss)
I0422 09:01:34.268121  7583 sgd_solver.cpp:106] Iteration 4220, lr = 0.0008
I0422 09:03:23.258747  7583 solver.cpp:228] Iteration 4240, loss = 6.26807
I0422 09:03:23.259318  7583 solver.cpp:244]     Train net output #0: loss = 6.26807 (* 1 = 6.26807 loss)
I0422 09:03:23.259335  7583 sgd_solver.cpp:106] Iteration 4240, lr = 0.0008
I0422 09:05:15.330333  7583 solver.cpp:228] Iteration 4260, loss = 5.49405
I0422 09:05:15.330838  7583 solver.cpp:244]     Train net output #0: loss = 5.49405 (* 1 = 5.49405 loss)
I0422 09:05:15.330852  7583 sgd_solver.cpp:106] Iteration 4260, lr = 0.0008
I0422 09:07:07.111332  7583 solver.cpp:228] Iteration 4280, loss = 9.35681
I0422 09:07:07.111940  7583 solver.cpp:244]     Train net output #0: loss = 9.35681 (* 1 = 9.35681 loss)
I0422 09:07:07.111974  7583 sgd_solver.cpp:106] Iteration 4280, lr = 0.0008
I0422 09:08:51.351449  7583 solver.cpp:228] Iteration 4300, loss = 17.9123
I0422 09:08:51.351966  7583 solver.cpp:244]     Train net output #0: loss = 17.9123 (* 1 = 17.9123 loss)
I0422 09:08:51.351984  7583 sgd_solver.cpp:106] Iteration 4300, lr = 0.0008
I0422 09:10:34.504108  7583 solver.cpp:228] Iteration 4320, loss = 14.9471
I0422 09:10:34.504734  7583 solver.cpp:244]     Train net output #0: loss = 14.9471 (* 1 = 14.9471 loss)
I0422 09:10:34.504765  7583 sgd_solver.cpp:106] Iteration 4320, lr = 0.0008
I0422 09:12:16.694963  7583 solver.cpp:228] Iteration 4340, loss = 6.16333
I0422 09:12:16.695433  7583 solver.cpp:244]     Train net output #0: loss = 6.16333 (* 1 = 6.16333 loss)
I0422 09:12:16.695451  7583 sgd_solver.cpp:106] Iteration 4340, lr = 0.0008
I0422 09:13:59.889583  7583 solver.cpp:228] Iteration 4360, loss = 4.89239
I0422 09:13:59.890105  7583 solver.cpp:244]     Train net output #0: loss = 4.89239 (* 1 = 4.89239 loss)
I0422 09:13:59.890130  7583 sgd_solver.cpp:106] Iteration 4360, lr = 0.0008
I0422 09:15:41.350805  7583 solver.cpp:228] Iteration 4380, loss = 5.73405
I0422 09:15:41.351229  7583 solver.cpp:244]     Train net output #0: loss = 5.73405 (* 1 = 5.73405 loss)
I0422 09:15:41.351246  7583 sgd_solver.cpp:106] Iteration 4380, lr = 0.0008
I0422 09:17:21.288295  7583 solver.cpp:228] Iteration 4400, loss = 7.21617
I0422 09:17:21.288732  7583 solver.cpp:244]     Train net output #0: loss = 7.21617 (* 1 = 7.21617 loss)
I0422 09:17:21.288746  7583 sgd_solver.cpp:106] Iteration 4400, lr = 0.0008
I0422 09:19:02.618953  7583 solver.cpp:228] Iteration 4420, loss = 5.19105
I0422 09:19:02.619318  7583 solver.cpp:244]     Train net output #0: loss = 5.19105 (* 1 = 5.19105 loss)
I0422 09:19:02.619333  7583 sgd_solver.cpp:106] Iteration 4420, lr = 0.0008
I0422 09:20:41.157522  7583 solver.cpp:228] Iteration 4440, loss = 6.15008
I0422 09:20:41.158004  7583 solver.cpp:244]     Train net output #0: loss = 6.15008 (* 1 = 6.15008 loss)
I0422 09:20:41.158020  7583 sgd_solver.cpp:106] Iteration 4440, lr = 0.0008
I0422 09:22:18.671633  7583 solver.cpp:228] Iteration 4460, loss = 5.78103
I0422 09:22:18.672097  7583 solver.cpp:244]     Train net output #0: loss = 5.78103 (* 1 = 5.78103 loss)
I0422 09:22:18.672112  7583 sgd_solver.cpp:106] Iteration 4460, lr = 0.0008
I0422 09:23:58.891599  7583 solver.cpp:228] Iteration 4480, loss = 6.61135
I0422 09:23:58.891978  7583 solver.cpp:244]     Train net output #0: loss = 6.61135 (* 1 = 6.61135 loss)
I0422 09:23:58.891993  7583 sgd_solver.cpp:106] Iteration 4480, lr = 0.0008
I0422 09:25:39.649061  7583 solver.cpp:228] Iteration 4500, loss = 8.03268
I0422 09:25:39.649508  7583 solver.cpp:244]     Train net output #0: loss = 8.03268 (* 1 = 8.03268 loss)
I0422 09:25:39.649523  7583 sgd_solver.cpp:106] Iteration 4500, lr = 0.0008
I0422 09:27:18.901774  7583 solver.cpp:228] Iteration 4520, loss = 6.87276
I0422 09:27:18.902297  7583 solver.cpp:244]     Train net output #0: loss = 6.87276 (* 1 = 6.87276 loss)
I0422 09:27:18.902313  7583 sgd_solver.cpp:106] Iteration 4520, lr = 0.0008
I0422 09:28:59.062273  7583 solver.cpp:228] Iteration 4540, loss = 8.46042
I0422 09:28:59.062744  7583 solver.cpp:244]     Train net output #0: loss = 8.46042 (* 1 = 8.46042 loss)
I0422 09:28:59.062760  7583 sgd_solver.cpp:106] Iteration 4540, lr = 0.0008
I0422 09:30:37.962600  7583 solver.cpp:228] Iteration 4560, loss = 6.31663
I0422 09:30:37.963109  7583 solver.cpp:244]     Train net output #0: loss = 6.31663 (* 1 = 6.31663 loss)
I0422 09:30:37.963124  7583 sgd_solver.cpp:106] Iteration 4560, lr = 0.0008
I0422 09:32:17.782392  7583 solver.cpp:228] Iteration 4580, loss = 4.768
I0422 09:32:17.782868  7583 solver.cpp:244]     Train net output #0: loss = 4.768 (* 1 = 4.768 loss)
I0422 09:32:17.782884  7583 sgd_solver.cpp:106] Iteration 4580, lr = 0.0008
I0422 09:33:57.210662  7583 solver.cpp:228] Iteration 4600, loss = 6.3141
I0422 09:33:57.211110  7583 solver.cpp:244]     Train net output #0: loss = 6.3141 (* 1 = 6.3141 loss)
I0422 09:33:57.211127  7583 sgd_solver.cpp:106] Iteration 4600, lr = 0.0008
I0422 09:35:37.096730  7583 solver.cpp:228] Iteration 4620, loss = 5.14854
I0422 09:35:37.097239  7583 solver.cpp:244]     Train net output #0: loss = 5.14854 (* 1 = 5.14854 loss)
I0422 09:35:37.097254  7583 sgd_solver.cpp:106] Iteration 4620, lr = 0.0008
I0422 09:37:16.736675  7583 solver.cpp:228] Iteration 4640, loss = 6.317
I0422 09:37:16.737165  7583 solver.cpp:244]     Train net output #0: loss = 6.317 (* 1 = 6.317 loss)
I0422 09:37:16.737180  7583 sgd_solver.cpp:106] Iteration 4640, lr = 0.0008
I0422 09:38:56.480080  7583 solver.cpp:228] Iteration 4660, loss = 5.74218
I0422 09:38:56.480444  7583 solver.cpp:244]     Train net output #0: loss = 5.74218 (* 1 = 5.74218 loss)
I0422 09:38:56.480459  7583 sgd_solver.cpp:106] Iteration 4660, lr = 0.0008
I0422 09:40:37.178484  7583 solver.cpp:228] Iteration 4680, loss = 5.24106
I0422 09:40:37.179018  7583 solver.cpp:244]     Train net output #0: loss = 5.24106 (* 1 = 5.24106 loss)
I0422 09:40:37.179034  7583 sgd_solver.cpp:106] Iteration 4680, lr = 0.0008
I0422 09:42:17.973258  7583 solver.cpp:228] Iteration 4700, loss = 5.2644
I0422 09:42:17.973702  7583 solver.cpp:244]     Train net output #0: loss = 5.2644 (* 1 = 5.2644 loss)
I0422 09:42:17.973717  7583 sgd_solver.cpp:106] Iteration 4700, lr = 0.0008
I0422 09:43:57.395530  7583 solver.cpp:228] Iteration 4720, loss = 10.7626
I0422 09:43:57.396033  7583 solver.cpp:244]     Train net output #0: loss = 10.7626 (* 1 = 10.7626 loss)
I0422 09:43:57.396047  7583 sgd_solver.cpp:106] Iteration 4720, lr = 0.0008
I0422 09:45:38.284184  7583 solver.cpp:228] Iteration 4740, loss = 8.75409
I0422 09:45:38.284631  7583 solver.cpp:244]     Train net output #0: loss = 8.75409 (* 1 = 8.75409 loss)
I0422 09:45:38.284646  7583 sgd_solver.cpp:106] Iteration 4740, lr = 0.0008
I0422 09:47:18.584113  7583 solver.cpp:228] Iteration 4760, loss = 5.04122
I0422 09:47:18.584579  7583 solver.cpp:244]     Train net output #0: loss = 5.04122 (* 1 = 5.04122 loss)
I0422 09:47:18.584596  7583 sgd_solver.cpp:106] Iteration 4760, lr = 0.0008
I0422 09:48:58.833356  7583 solver.cpp:228] Iteration 4780, loss = 4.6187
I0422 09:48:58.833799  7583 solver.cpp:244]     Train net output #0: loss = 4.6187 (* 1 = 4.6187 loss)
I0422 09:48:58.833813  7583 sgd_solver.cpp:106] Iteration 4780, lr = 0.0008
I0422 09:50:39.029273  7583 solver.cpp:228] Iteration 4800, loss = 6.22577
I0422 09:50:39.029712  7583 solver.cpp:244]     Train net output #0: loss = 6.22577 (* 1 = 6.22577 loss)
I0422 09:50:39.029727  7583 sgd_solver.cpp:106] Iteration 4800, lr = 0.0008
I0422 09:52:19.692543  7583 solver.cpp:228] Iteration 4820, loss = 5.97366
I0422 09:52:19.693011  7583 solver.cpp:244]     Train net output #0: loss = 5.97366 (* 1 = 5.97366 loss)
I0422 09:52:19.693030  7583 sgd_solver.cpp:106] Iteration 4820, lr = 0.0008
I0422 09:53:59.944093  7583 solver.cpp:228] Iteration 4840, loss = 8.22602
I0422 09:53:59.944617  7583 solver.cpp:244]     Train net output #0: loss = 8.22602 (* 1 = 8.22602 loss)
I0422 09:53:59.944633  7583 sgd_solver.cpp:106] Iteration 4840, lr = 0.0008
I0422 09:55:38.860055  7583 solver.cpp:228] Iteration 4860, loss = 6.7129
I0422 09:55:38.860544  7583 solver.cpp:244]     Train net output #0: loss = 6.7129 (* 1 = 6.7129 loss)
I0422 09:55:38.860561  7583 sgd_solver.cpp:106] Iteration 4860, lr = 0.0008
I0422 09:57:18.694607  7583 solver.cpp:228] Iteration 4880, loss = 6.50923
I0422 09:57:18.695065  7583 solver.cpp:244]     Train net output #0: loss = 6.50923 (* 1 = 6.50923 loss)
I0422 09:57:18.695080  7583 sgd_solver.cpp:106] Iteration 4880, lr = 0.0008
I0422 09:58:59.711612  7583 solver.cpp:228] Iteration 4900, loss = 4.98396
I0422 09:58:59.712019  7583 solver.cpp:244]     Train net output #0: loss = 4.98396 (* 1 = 4.98396 loss)
I0422 09:58:59.712033  7583 sgd_solver.cpp:106] Iteration 4900, lr = 0.0008
I0422 10:00:38.868355  7583 solver.cpp:228] Iteration 4920, loss = 11.349
I0422 10:00:38.868832  7583 solver.cpp:244]     Train net output #0: loss = 11.349 (* 1 = 11.349 loss)
I0422 10:00:38.868849  7583 sgd_solver.cpp:106] Iteration 4920, lr = 0.0008
I0422 10:02:18.514168  7583 solver.cpp:228] Iteration 4940, loss = 15.1366
I0422 10:02:18.514698  7583 solver.cpp:244]     Train net output #0: loss = 15.1366 (* 1 = 15.1366 loss)
I0422 10:02:18.514714  7583 sgd_solver.cpp:106] Iteration 4940, lr = 0.0008
I0422 10:03:57.939000  7583 solver.cpp:228] Iteration 4960, loss = 7.55904
I0422 10:03:57.939467  7583 solver.cpp:244]     Train net output #0: loss = 7.55904 (* 1 = 7.55904 loss)
I0422 10:03:57.939483  7583 sgd_solver.cpp:106] Iteration 4960, lr = 0.0008
I0422 10:05:38.129138  7583 solver.cpp:228] Iteration 4980, loss = 6.08314
I0422 10:05:38.129585  7583 solver.cpp:244]     Train net output #0: loss = 6.08314 (* 1 = 6.08314 loss)
I0422 10:05:38.129601  7583 sgd_solver.cpp:106] Iteration 4980, lr = 0.0008
I0422 10:07:15.974285  7583 solver.cpp:337] Iteration 5000, Testing net (#0)
I0422 10:08:10.191426  7583 solver.cpp:404]     Test net output #0: loss = 5.54893 (* 1 = 5.54893 loss)
I0422 10:08:12.615370  7583 solver.cpp:228] Iteration 5000, loss = 5.67853
I0422 10:08:12.615429  7583 solver.cpp:244]     Train net output #0: loss = 5.67853 (* 1 = 5.67853 loss)
I0422 10:08:12.615442  7583 sgd_solver.cpp:106] Iteration 5000, lr = 0.0008
I0422 10:09:54.594689  7583 solver.cpp:228] Iteration 5020, loss = 5.33841
I0422 10:09:54.595099  7583 solver.cpp:244]     Train net output #0: loss = 5.33841 (* 1 = 5.33841 loss)
I0422 10:09:54.595115  7583 sgd_solver.cpp:106] Iteration 5020, lr = 0.0008
I0422 10:11:32.063171  7583 solver.cpp:228] Iteration 5040, loss = 5.05876
I0422 10:11:32.063555  7583 solver.cpp:244]     Train net output #0: loss = 5.05876 (* 1 = 5.05876 loss)
I0422 10:11:32.063570  7583 sgd_solver.cpp:106] Iteration 5040, lr = 0.0008
I0422 10:13:03.443337  7583 solver.cpp:228] Iteration 5060, loss = 6.54415
I0422 10:13:03.443703  7583 solver.cpp:244]     Train net output #0: loss = 6.54415 (* 1 = 6.54415 loss)
I0422 10:13:03.443717  7583 sgd_solver.cpp:106] Iteration 5060, lr = 0.0008
I0422 10:14:34.603925  7583 solver.cpp:228] Iteration 5080, loss = 6.60921
I0422 10:14:34.604270  7583 solver.cpp:244]     Train net output #0: loss = 6.60921 (* 1 = 6.60921 loss)
I0422 10:14:34.604285  7583 sgd_solver.cpp:106] Iteration 5080, lr = 0.0008
I0422 10:16:08.709000  7583 solver.cpp:228] Iteration 5100, loss = 5.41687
I0422 10:16:08.709463  7583 solver.cpp:244]     Train net output #0: loss = 5.41687 (* 1 = 5.41687 loss)
I0422 10:16:08.709480  7583 sgd_solver.cpp:106] Iteration 5100, lr = 0.0008
I0422 10:17:45.769467  7583 solver.cpp:228] Iteration 5120, loss = 10.4287
I0422 10:17:45.769908  7583 solver.cpp:244]     Train net output #0: loss = 10.4287 (* 1 = 10.4287 loss)
I0422 10:17:45.769923  7583 sgd_solver.cpp:106] Iteration 5120, lr = 0.0008
I0422 10:19:23.886823  7583 solver.cpp:228] Iteration 5140, loss = 5.08137
I0422 10:19:23.887186  7583 solver.cpp:244]     Train net output #0: loss = 5.08137 (* 1 = 5.08137 loss)
I0422 10:19:23.887200  7583 sgd_solver.cpp:106] Iteration 5140, lr = 0.0008
I0422 10:21:02.210441  7583 solver.cpp:228] Iteration 5160, loss = 4.54405
I0422 10:21:02.210875  7583 solver.cpp:244]     Train net output #0: loss = 4.54405 (* 1 = 4.54405 loss)
I0422 10:21:02.210889  7583 sgd_solver.cpp:106] Iteration 5160, lr = 0.0008
I0422 10:22:40.249922  7583 solver.cpp:228] Iteration 5180, loss = 9.80493
I0422 10:22:40.250370  7583 solver.cpp:244]     Train net output #0: loss = 9.80493 (* 1 = 9.80493 loss)
I0422 10:22:40.250385  7583 sgd_solver.cpp:106] Iteration 5180, lr = 0.0008
I0422 10:24:18.719641  7583 solver.cpp:228] Iteration 5200, loss = 7.14924
I0422 10:24:18.720019  7583 solver.cpp:244]     Train net output #0: loss = 7.14924 (* 1 = 7.14924 loss)
I0422 10:24:18.720034  7583 sgd_solver.cpp:106] Iteration 5200, lr = 0.0008
I0422 10:25:56.819730  7583 solver.cpp:228] Iteration 5220, loss = 9.26462
I0422 10:25:56.820185  7583 solver.cpp:244]     Train net output #0: loss = 9.26462 (* 1 = 9.26462 loss)
I0422 10:25:56.820199  7583 sgd_solver.cpp:106] Iteration 5220, lr = 0.0008
I0422 10:27:33.141513  7583 solver.cpp:228] Iteration 5240, loss = 5.25653
I0422 10:27:33.141991  7583 solver.cpp:244]     Train net output #0: loss = 5.25653 (* 1 = 5.25653 loss)
I0422 10:27:33.142006  7583 sgd_solver.cpp:106] Iteration 5240, lr = 0.0008
I0422 10:29:09.406229  7583 solver.cpp:228] Iteration 5260, loss = 5.11507
I0422 10:29:09.406679  7583 solver.cpp:244]     Train net output #0: loss = 5.11507 (* 1 = 5.11507 loss)
I0422 10:29:09.406694  7583 sgd_solver.cpp:106] Iteration 5260, lr = 0.0008
I0422 10:30:45.396450  7583 solver.cpp:228] Iteration 5280, loss = 6.46893
I0422 10:30:45.396934  7583 solver.cpp:244]     Train net output #0: loss = 6.46893 (* 1 = 6.46893 loss)
I0422 10:30:45.396949  7583 sgd_solver.cpp:106] Iteration 5280, lr = 0.0008
I0422 10:32:20.958158  7583 solver.cpp:228] Iteration 5300, loss = 9.41493
I0422 10:32:20.958573  7583 solver.cpp:244]     Train net output #0: loss = 9.41493 (* 1 = 9.41493 loss)
I0422 10:32:20.958587  7583 sgd_solver.cpp:106] Iteration 5300, lr = 0.0008
I0422 10:33:56.493461  7583 solver.cpp:228] Iteration 5320, loss = 9.37568
I0422 10:33:56.493911  7583 solver.cpp:244]     Train net output #0: loss = 9.37568 (* 1 = 9.37568 loss)
I0422 10:33:56.493926  7583 sgd_solver.cpp:106] Iteration 5320, lr = 0.0008
I0422 10:35:31.574224  7583 solver.cpp:228] Iteration 5340, loss = 6.31954
I0422 10:35:31.574656  7583 solver.cpp:244]     Train net output #0: loss = 6.31954 (* 1 = 6.31954 loss)
I0422 10:35:31.574671  7583 sgd_solver.cpp:106] Iteration 5340, lr = 0.0008
I0422 10:37:07.665017  7583 solver.cpp:228] Iteration 5360, loss = 6.502
I0422 10:37:07.665488  7583 solver.cpp:244]     Train net output #0: loss = 6.502 (* 1 = 6.502 loss)
I0422 10:37:07.665504  7583 sgd_solver.cpp:106] Iteration 5360, lr = 0.0008
I0422 10:38:43.465052  7583 solver.cpp:228] Iteration 5380, loss = 7.01246
I0422 10:38:43.465484  7583 solver.cpp:244]     Train net output #0: loss = 7.01246 (* 1 = 7.01246 loss)
I0422 10:38:43.465498  7583 sgd_solver.cpp:106] Iteration 5380, lr = 0.0008
I0422 10:40:17.460959  7583 solver.cpp:228] Iteration 5400, loss = 6.48979
I0422 10:40:17.461410  7583 solver.cpp:244]     Train net output #0: loss = 6.48979 (* 1 = 6.48979 loss)
I0422 10:40:17.461424  7583 sgd_solver.cpp:106] Iteration 5400, lr = 0.0008
I0422 10:41:53.002694  7583 solver.cpp:228] Iteration 5420, loss = 6.5138
I0422 10:41:53.003132  7583 solver.cpp:244]     Train net output #0: loss = 6.5138 (* 1 = 6.5138 loss)
I0422 10:41:53.003147  7583 sgd_solver.cpp:106] Iteration 5420, lr = 0.0008
I0422 10:43:28.125592  7583 solver.cpp:228] Iteration 5440, loss = 5.19405
I0422 10:43:28.126073  7583 solver.cpp:244]     Train net output #0: loss = 5.19405 (* 1 = 5.19405 loss)
I0422 10:43:28.126088  7583 sgd_solver.cpp:106] Iteration 5440, lr = 0.0008
I0422 10:45:03.327332  7583 solver.cpp:228] Iteration 5460, loss = 6.51146
I0422 10:45:03.327791  7583 solver.cpp:244]     Train net output #0: loss = 6.51146 (* 1 = 6.51146 loss)
I0422 10:45:03.327806  7583 sgd_solver.cpp:106] Iteration 5460, lr = 0.0008
I0422 10:46:38.767820  7583 solver.cpp:228] Iteration 5480, loss = 7.50162
I0422 10:46:38.768358  7583 solver.cpp:244]     Train net output #0: loss = 7.50162 (* 1 = 7.50162 loss)
I0422 10:46:38.768373  7583 sgd_solver.cpp:106] Iteration 5480, lr = 0.0008
I0422 10:48:14.693277  7583 solver.cpp:228] Iteration 5500, loss = 6.34684
I0422 10:48:14.693753  7583 solver.cpp:244]     Train net output #0: loss = 6.34684 (* 1 = 6.34684 loss)
I0422 10:48:14.693768  7583 sgd_solver.cpp:106] Iteration 5500, lr = 0.0008
I0422 10:49:48.555498  7583 solver.cpp:228] Iteration 5520, loss = 4.94961
I0422 10:49:48.555932  7583 solver.cpp:244]     Train net output #0: loss = 4.94961 (* 1 = 4.94961 loss)
I0422 10:49:48.555945  7583 sgd_solver.cpp:106] Iteration 5520, lr = 0.0008
I0422 10:51:25.487948  7583 solver.cpp:228] Iteration 5540, loss = 5.39224
I0422 10:51:25.488386  7583 solver.cpp:244]     Train net output #0: loss = 5.39224 (* 1 = 5.39224 loss)
I0422 10:51:25.488401  7583 sgd_solver.cpp:106] Iteration 5540, lr = 0.0008
I0422 10:53:03.349644  7583 solver.cpp:228] Iteration 5560, loss = 4.66464
I0422 10:53:03.350076  7583 solver.cpp:244]     Train net output #0: loss = 4.66464 (* 1 = 4.66464 loss)
I0422 10:53:03.350090  7583 sgd_solver.cpp:106] Iteration 5560, lr = 0.0008
I0422 10:54:41.580597  7583 solver.cpp:228] Iteration 5580, loss = 6.03696
I0422 10:54:41.581040  7583 solver.cpp:244]     Train net output #0: loss = 6.03696 (* 1 = 6.03696 loss)
I0422 10:54:41.581055  7583 sgd_solver.cpp:106] Iteration 5580, lr = 0.0008
I0422 10:56:20.329552  7583 solver.cpp:228] Iteration 5600, loss = 4.18524
I0422 10:56:20.330016  7583 solver.cpp:244]     Train net output #0: loss = 4.18524 (* 1 = 4.18524 loss)
I0422 10:56:20.330032  7583 sgd_solver.cpp:106] Iteration 5600, lr = 0.0008
I0422 10:57:57.941954  7583 solver.cpp:228] Iteration 5620, loss = 6.39586
I0422 10:57:57.942420  7583 solver.cpp:244]     Train net output #0: loss = 6.39586 (* 1 = 6.39586 loss)
I0422 10:57:57.942435  7583 sgd_solver.cpp:106] Iteration 5620, lr = 0.0008
I0422 10:59:38.916476  7583 solver.cpp:228] Iteration 5640, loss = 5.70076
I0422 10:59:38.916956  7583 solver.cpp:244]     Train net output #0: loss = 5.70076 (* 1 = 5.70076 loss)
I0422 10:59:38.916987  7583 sgd_solver.cpp:106] Iteration 5640, lr = 0.0008
I0422 11:01:15.244864  7583 solver.cpp:228] Iteration 5660, loss = 5.89318
I0422 11:01:15.245307  7583 solver.cpp:244]     Train net output #0: loss = 5.89318 (* 1 = 5.89318 loss)
I0422 11:01:15.245322  7583 sgd_solver.cpp:106] Iteration 5660, lr = 0.0008
I0422 11:02:52.394809  7583 solver.cpp:228] Iteration 5680, loss = 6.11481
I0422 11:02:52.395273  7583 solver.cpp:244]     Train net output #0: loss = 6.11481 (* 1 = 6.11481 loss)
I0422 11:02:52.395288  7583 sgd_solver.cpp:106] Iteration 5680, lr = 0.0008
I0422 11:04:31.470469  7583 solver.cpp:228] Iteration 5700, loss = 5.34793
I0422 11:04:31.470914  7583 solver.cpp:244]     Train net output #0: loss = 5.34793 (* 1 = 5.34793 loss)
I0422 11:04:31.470929  7583 sgd_solver.cpp:106] Iteration 5700, lr = 0.0008
I0422 11:06:10.262778  7583 solver.cpp:228] Iteration 5720, loss = 5.47112
I0422 11:06:10.263211  7583 solver.cpp:244]     Train net output #0: loss = 5.47112 (* 1 = 5.47112 loss)
I0422 11:06:10.263224  7583 sgd_solver.cpp:106] Iteration 5720, lr = 0.0008
I0422 11:07:49.370543  7583 solver.cpp:228] Iteration 5740, loss = 8.58532
I0422 11:07:49.370981  7583 solver.cpp:244]     Train net output #0: loss = 8.58532 (* 1 = 8.58532 loss)
I0422 11:07:49.370996  7583 sgd_solver.cpp:106] Iteration 5740, lr = 0.0008
I0422 11:09:31.064800  7583 solver.cpp:228] Iteration 5760, loss = 4.74133
I0422 11:09:31.065294  7583 solver.cpp:244]     Train net output #0: loss = 4.74133 (* 1 = 4.74133 loss)
I0422 11:09:31.065310  7583 sgd_solver.cpp:106] Iteration 5760, lr = 0.0008
I0422 11:11:12.499049  7583 solver.cpp:228] Iteration 5780, loss = 4.62053
I0422 11:11:12.499567  7583 solver.cpp:244]     Train net output #0: loss = 4.62053 (* 1 = 4.62053 loss)
I0422 11:11:12.499582  7583 sgd_solver.cpp:106] Iteration 5780, lr = 0.0008
I0422 11:12:53.973968  7583 solver.cpp:228] Iteration 5800, loss = 4.46901
I0422 11:12:53.974442  7583 solver.cpp:244]     Train net output #0: loss = 4.46901 (* 1 = 4.46901 loss)
I0422 11:12:53.974457  7583 sgd_solver.cpp:106] Iteration 5800, lr = 0.0008
I0422 11:14:35.045351  7583 solver.cpp:228] Iteration 5820, loss = 4.36666
I0422 11:14:35.045882  7583 solver.cpp:244]     Train net output #0: loss = 4.36666 (* 1 = 4.36666 loss)
I0422 11:14:35.045897  7583 sgd_solver.cpp:106] Iteration 5820, lr = 0.0008
I0422 11:16:18.013474  7583 solver.cpp:228] Iteration 5840, loss = 5.68705
I0422 11:16:18.014032  7583 solver.cpp:244]     Train net output #0: loss = 5.68705 (* 1 = 5.68705 loss)
I0422 11:16:18.014052  7583 sgd_solver.cpp:106] Iteration 5840, lr = 0.0008
I0422 11:18:00.033187  7583 solver.cpp:228] Iteration 5860, loss = 5.73652
I0422 11:18:00.033718  7583 solver.cpp:244]     Train net output #0: loss = 5.73652 (* 1 = 5.73652 loss)
I0422 11:18:00.033742  7583 sgd_solver.cpp:106] Iteration 5860, lr = 0.0008
I0422 11:19:42.489037  7583 solver.cpp:228] Iteration 5880, loss = 5.19253
I0422 11:19:42.489575  7583 solver.cpp:244]     Train net output #0: loss = 5.19253 (* 1 = 5.19253 loss)
I0422 11:19:42.489595  7583 sgd_solver.cpp:106] Iteration 5880, lr = 0.0008
I0422 11:21:22.878777  7583 solver.cpp:228] Iteration 5900, loss = 6.32022
I0422 11:21:22.879278  7583 solver.cpp:244]     Train net output #0: loss = 6.32022 (* 1 = 6.32022 loss)
I0422 11:21:22.879294  7583 sgd_solver.cpp:106] Iteration 5900, lr = 0.0008
I0422 11:23:04.798390  7583 solver.cpp:228] Iteration 5920, loss = 5.22855
I0422 11:23:04.798863  7583 solver.cpp:244]     Train net output #0: loss = 5.22855 (* 1 = 5.22855 loss)
I0422 11:23:04.798880  7583 sgd_solver.cpp:106] Iteration 5920, lr = 0.0008
I0422 11:24:47.274636  7583 solver.cpp:228] Iteration 5940, loss = 4.86906
I0422 11:24:47.275257  7583 solver.cpp:244]     Train net output #0: loss = 4.86906 (* 1 = 4.86906 loss)
I0422 11:24:47.275285  7583 sgd_solver.cpp:106] Iteration 5940, lr = 0.0008
I0422 11:26:30.667884  7583 solver.cpp:228] Iteration 5960, loss = 6.38447
I0422 11:26:30.668421  7583 solver.cpp:244]     Train net output #0: loss = 6.38447 (* 1 = 6.38447 loss)
I0422 11:26:30.668444  7583 sgd_solver.cpp:106] Iteration 5960, lr = 0.0008
I0422 11:28:11.693054  7583 solver.cpp:228] Iteration 5980, loss = 4.37643
I0422 11:28:11.693642  7583 solver.cpp:244]     Train net output #0: loss = 4.37643 (* 1 = 4.37643 loss)
I0422 11:28:11.693665  7583 sgd_solver.cpp:106] Iteration 5980, lr = 0.0008
I0422 11:29:50.295466  7583 solver.cpp:337] Iteration 6000, Testing net (#0)
I0422 11:30:48.045724  7583 solver.cpp:404]     Test net output #0: loss = 5.40942 (* 1 = 5.40942 loss)
I0422 11:30:50.816038  7583 solver.cpp:228] Iteration 6000, loss = 4.61322
I0422 11:30:50.816112  7583 solver.cpp:244]     Train net output #0: loss = 4.61322 (* 1 = 4.61322 loss)
I0422 11:30:50.816123  7583 sgd_solver.cpp:106] Iteration 6000, lr = 0.0008
I0422 11:32:32.719005  7583 solver.cpp:228] Iteration 6020, loss = 4.44114
I0422 11:32:32.719535  7583 solver.cpp:244]     Train net output #0: loss = 4.44114 (* 1 = 4.44114 loss)
I0422 11:32:32.719558  7583 sgd_solver.cpp:106] Iteration 6020, lr = 0.0008
I0422 11:34:15.203182  7583 solver.cpp:228] Iteration 6040, loss = 7.15793
I0422 11:34:15.203658  7583 solver.cpp:244]     Train net output #0: loss = 7.15793 (* 1 = 7.15793 loss)
I0422 11:34:15.203671  7583 sgd_solver.cpp:106] Iteration 6040, lr = 0.0008
I0422 11:35:59.058728  7583 solver.cpp:228] Iteration 6060, loss = 5.61384
I0422 11:35:59.059190  7583 solver.cpp:244]     Train net output #0: loss = 5.61384 (* 1 = 5.61384 loss)
I0422 11:35:59.059211  7583 sgd_solver.cpp:106] Iteration 6060, lr = 0.0008
I0422 11:37:42.656473  7583 solver.cpp:228] Iteration 6080, loss = 5.60806
I0422 11:37:42.657054  7583 solver.cpp:244]     Train net output #0: loss = 5.60806 (* 1 = 5.60806 loss)
I0422 11:37:42.657084  7583 sgd_solver.cpp:106] Iteration 6080, lr = 0.0008
I0422 11:39:23.691884  7583 solver.cpp:228] Iteration 6100, loss = 7.0243
I0422 11:39:23.692376  7583 solver.cpp:244]     Train net output #0: loss = 7.0243 (* 1 = 7.0243 loss)
I0422 11:39:23.692415  7583 sgd_solver.cpp:106] Iteration 6100, lr = 0.0008
I0422 11:41:02.061533  7583 solver.cpp:228] Iteration 6120, loss = 6.53157
I0422 11:41:02.062011  7583 solver.cpp:244]     Train net output #0: loss = 6.53157 (* 1 = 6.53157 loss)
I0422 11:41:02.062034  7583 sgd_solver.cpp:106] Iteration 6120, lr = 0.0008
I0422 11:42:42.446162  7583 solver.cpp:228] Iteration 6140, loss = 6.14568
I0422 11:42:42.446596  7583 solver.cpp:244]     Train net output #0: loss = 6.14568 (* 1 = 6.14568 loss)
I0422 11:42:42.446611  7583 sgd_solver.cpp:106] Iteration 6140, lr = 0.0008
I0422 11:44:21.257228  7583 solver.cpp:228] Iteration 6160, loss = 5.27996
I0422 11:44:21.257658  7583 solver.cpp:244]     Train net output #0: loss = 5.27996 (* 1 = 5.27996 loss)
I0422 11:44:21.257673  7583 sgd_solver.cpp:106] Iteration 6160, lr = 0.0008
I0422 11:45:59.987054  7583 solver.cpp:228] Iteration 6180, loss = 5.52197
I0422 11:45:59.987434  7583 solver.cpp:244]     Train net output #0: loss = 5.52197 (* 1 = 5.52197 loss)
I0422 11:45:59.987449  7583 sgd_solver.cpp:106] Iteration 6180, lr = 0.0008
I0422 11:47:40.714913  7583 solver.cpp:228] Iteration 6200, loss = 5.85112
I0422 11:47:40.715373  7583 solver.cpp:244]     Train net output #0: loss = 5.85112 (* 1 = 5.85112 loss)
I0422 11:47:40.715395  7583 sgd_solver.cpp:106] Iteration 6200, lr = 0.0008
I0422 11:49:20.216315  7583 solver.cpp:228] Iteration 6220, loss = 5.05248
I0422 11:49:20.216727  7583 solver.cpp:244]     Train net output #0: loss = 5.05248 (* 1 = 5.05248 loss)
I0422 11:49:20.216742  7583 sgd_solver.cpp:106] Iteration 6220, lr = 0.0008
I0422 11:50:59.541653  7583 solver.cpp:228] Iteration 6240, loss = 5.35703
I0422 11:50:59.542110  7583 solver.cpp:244]     Train net output #0: loss = 5.35703 (* 1 = 5.35703 loss)
I0422 11:50:59.542126  7583 sgd_solver.cpp:106] Iteration 6240, lr = 0.0008
I0422 11:52:37.950759  7583 solver.cpp:228] Iteration 6260, loss = 8.25158
I0422 11:52:37.951249  7583 solver.cpp:244]     Train net output #0: loss = 8.25158 (* 1 = 8.25158 loss)
I0422 11:52:37.951267  7583 sgd_solver.cpp:106] Iteration 6260, lr = 0.0008
I0422 11:54:16.874179  7583 solver.cpp:228] Iteration 6280, loss = 5.13852
I0422 11:54:16.874583  7583 solver.cpp:244]     Train net output #0: loss = 5.13852 (* 1 = 5.13852 loss)
I0422 11:54:16.874603  7583 sgd_solver.cpp:106] Iteration 6280, lr = 0.0008
I0422 11:55:56.429183  7583 solver.cpp:228] Iteration 6300, loss = 5.77018
I0422 11:55:56.429661  7583 solver.cpp:244]     Train net output #0: loss = 5.77018 (* 1 = 5.77018 loss)
I0422 11:55:56.429683  7583 sgd_solver.cpp:106] Iteration 6300, lr = 0.0008
I0422 11:57:38.723884  7583 solver.cpp:228] Iteration 6320, loss = 5.54373
I0422 11:57:38.724392  7583 solver.cpp:244]     Train net output #0: loss = 5.54373 (* 1 = 5.54373 loss)
I0422 11:57:38.724412  7583 sgd_solver.cpp:106] Iteration 6320, lr = 0.0008
I0422 11:59:22.229163  7583 solver.cpp:228] Iteration 6340, loss = 6.35989
I0422 11:59:22.229679  7583 solver.cpp:244]     Train net output #0: loss = 6.35989 (* 1 = 6.35989 loss)
I0422 11:59:22.229708  7583 sgd_solver.cpp:106] Iteration 6340, lr = 0.0008
I0422 12:01:03.678480  7583 solver.cpp:228] Iteration 6360, loss = 4.46731
I0422 12:01:03.678985  7583 solver.cpp:244]     Train net output #0: loss = 4.46731 (* 1 = 4.46731 loss)
I0422 12:01:03.679016  7583 sgd_solver.cpp:106] Iteration 6360, lr = 0.0008
I0422 12:02:45.550673  7583 solver.cpp:228] Iteration 6380, loss = 7.44718
I0422 12:02:45.551192  7583 solver.cpp:244]     Train net output #0: loss = 7.44718 (* 1 = 7.44718 loss)
I0422 12:02:45.551213  7583 sgd_solver.cpp:106] Iteration 6380, lr = 0.0008
I0422 12:04:28.452476  7583 solver.cpp:228] Iteration 6400, loss = 4.77111
I0422 12:04:28.452872  7583 solver.cpp:244]     Train net output #0: loss = 4.77111 (* 1 = 4.77111 loss)
I0422 12:04:28.452893  7583 sgd_solver.cpp:106] Iteration 6400, lr = 0.0008
I0422 12:06:11.345201  7583 solver.cpp:228] Iteration 6420, loss = 5.40194
I0422 12:06:11.345675  7583 solver.cpp:244]     Train net output #0: loss = 5.40194 (* 1 = 5.40194 loss)
I0422 12:06:11.345693  7583 sgd_solver.cpp:106] Iteration 6420, lr = 0.0008
I0422 12:07:53.970633  7583 solver.cpp:228] Iteration 6440, loss = 4.71616
I0422 12:07:53.971144  7583 solver.cpp:244]     Train net output #0: loss = 4.71616 (* 1 = 4.71616 loss)
I0422 12:07:53.971173  7583 sgd_solver.cpp:106] Iteration 6440, lr = 0.0008
I0422 12:09:38.575326  7583 solver.cpp:228] Iteration 6460, loss = 5.72557
I0422 12:09:38.575819  7583 solver.cpp:244]     Train net output #0: loss = 5.72557 (* 1 = 5.72557 loss)
I0422 12:09:38.575837  7583 sgd_solver.cpp:106] Iteration 6460, lr = 0.0008
I0422 12:11:22.068841  7583 solver.cpp:228] Iteration 6480, loss = 6.23148
I0422 12:11:22.069383  7583 solver.cpp:244]     Train net output #0: loss = 6.23148 (* 1 = 6.23148 loss)
I0422 12:11:22.069406  7583 sgd_solver.cpp:106] Iteration 6480, lr = 0.0008
I0422 12:13:05.299953  7583 solver.cpp:228] Iteration 6500, loss = 5.8821
I0422 12:13:05.300470  7583 solver.cpp:244]     Train net output #0: loss = 5.8821 (* 1 = 5.8821 loss)
I0422 12:13:05.300496  7583 sgd_solver.cpp:106] Iteration 6500, lr = 0.0008
I0422 12:14:49.361340  7583 solver.cpp:228] Iteration 6520, loss = 7.39791
I0422 12:14:49.361798  7583 solver.cpp:244]     Train net output #0: loss = 7.39791 (* 1 = 7.39791 loss)
I0422 12:14:49.361817  7583 sgd_solver.cpp:106] Iteration 6520, lr = 0.0008
I0422 12:16:33.072247  7583 solver.cpp:228] Iteration 6540, loss = 6.50822
I0422 12:16:33.072721  7583 solver.cpp:244]     Train net output #0: loss = 6.50822 (* 1 = 6.50822 loss)
I0422 12:16:33.073038  7583 sgd_solver.cpp:106] Iteration 6540, lr = 0.0008
I0422 12:18:17.609689  7583 solver.cpp:228] Iteration 6560, loss = 4.03113
I0422 12:18:17.610189  7583 solver.cpp:244]     Train net output #0: loss = 4.03113 (* 1 = 4.03113 loss)
I0422 12:18:17.610210  7583 sgd_solver.cpp:106] Iteration 6560, lr = 0.0008
I0422 12:20:04.797894  7583 solver.cpp:228] Iteration 6580, loss = 5.75941
I0422 12:20:04.798398  7583 solver.cpp:244]     Train net output #0: loss = 5.75941 (* 1 = 5.75941 loss)
I0422 12:20:04.798413  7583 sgd_solver.cpp:106] Iteration 6580, lr = 0.0008
I0422 12:21:55.179760  7583 solver.cpp:228] Iteration 6600, loss = 4.7229
I0422 12:21:55.180253  7583 solver.cpp:244]     Train net output #0: loss = 4.7229 (* 1 = 4.7229 loss)
I0422 12:21:55.180269  7583 sgd_solver.cpp:106] Iteration 6600, lr = 0.0008
I0422 12:23:45.343322  7583 solver.cpp:228] Iteration 6620, loss = 6.351
I0422 12:23:45.343816  7583 solver.cpp:244]     Train net output #0: loss = 6.351 (* 1 = 6.351 loss)
I0422 12:23:45.343832  7583 sgd_solver.cpp:106] Iteration 6620, lr = 0.0008
I0422 12:25:33.106431  7583 solver.cpp:228] Iteration 6640, loss = 5.3533
I0422 12:25:33.106959  7583 solver.cpp:244]     Train net output #0: loss = 5.3533 (* 1 = 5.3533 loss)
I0422 12:25:33.106988  7583 sgd_solver.cpp:106] Iteration 6640, lr = 0.0008
I0422 12:27:17.019820  7583 solver.cpp:228] Iteration 6660, loss = 4.75424
I0422 12:27:17.020262  7583 solver.cpp:244]     Train net output #0: loss = 4.75424 (* 1 = 4.75424 loss)
I0422 12:27:17.020277  7583 sgd_solver.cpp:106] Iteration 6660, lr = 0.0008
I0422 12:29:00.073858  7583 solver.cpp:228] Iteration 6680, loss = 4.66468
I0422 12:29:00.074425  7583 solver.cpp:244]     Train net output #0: loss = 4.66468 (* 1 = 4.66468 loss)
I0422 12:29:00.074452  7583 sgd_solver.cpp:106] Iteration 6680, lr = 0.0008
I0422 12:30:42.819438  7583 solver.cpp:228] Iteration 6700, loss = 4.54704
I0422 12:30:42.819939  7583 solver.cpp:244]     Train net output #0: loss = 4.54704 (* 1 = 4.54704 loss)
I0422 12:30:42.819962  7583 sgd_solver.cpp:106] Iteration 6700, lr = 0.0008
I0422 12:32:24.111565  7583 solver.cpp:228] Iteration 6720, loss = 4.45683
I0422 12:32:24.112035  7583 solver.cpp:244]     Train net output #0: loss = 4.45683 (* 1 = 4.45683 loss)
I0422 12:32:24.112051  7583 sgd_solver.cpp:106] Iteration 6720, lr = 0.0008
I0422 12:34:06.158092  7583 solver.cpp:228] Iteration 6740, loss = 5.96048
I0422 12:34:06.158537  7583 solver.cpp:244]     Train net output #0: loss = 5.96048 (* 1 = 5.96048 loss)
I0422 12:34:06.158552  7583 sgd_solver.cpp:106] Iteration 6740, lr = 0.0008
I0422 12:35:47.642608  7583 solver.cpp:228] Iteration 6760, loss = 4.97247
I0422 12:35:47.642976  7583 solver.cpp:244]     Train net output #0: loss = 4.97247 (* 1 = 4.97247 loss)
I0422 12:35:47.642998  7583 sgd_solver.cpp:106] Iteration 6760, lr = 0.0008
I0422 12:37:27.166370  7583 solver.cpp:228] Iteration 6780, loss = 4.5894
I0422 12:37:27.166682  7583 solver.cpp:244]     Train net output #0: loss = 4.5894 (* 1 = 4.5894 loss)
I0422 12:37:27.166697  7583 sgd_solver.cpp:106] Iteration 6780, lr = 0.0008
I0422 12:39:07.977762  7583 solver.cpp:228] Iteration 6800, loss = 4.71579
I0422 12:39:07.978198  7583 solver.cpp:244]     Train net output #0: loss = 4.71579 (* 1 = 4.71579 loss)
I0422 12:39:07.978212  7583 sgd_solver.cpp:106] Iteration 6800, lr = 0.0008
I0422 12:40:50.126435  7583 solver.cpp:228] Iteration 6820, loss = 5.29498
I0422 12:40:50.126893  7583 solver.cpp:244]     Train net output #0: loss = 5.29498 (* 1 = 5.29498 loss)
I0422 12:40:50.126912  7583 sgd_solver.cpp:106] Iteration 6820, lr = 0.0008
I0422 12:42:30.800984  7583 solver.cpp:228] Iteration 6840, loss = 4.76057
I0422 12:42:30.801414  7583 solver.cpp:244]     Train net output #0: loss = 4.76057 (* 1 = 4.76057 loss)
I0422 12:42:30.801429  7583 sgd_solver.cpp:106] Iteration 6840, lr = 0.0008
I0422 12:44:12.456323  7583 solver.cpp:228] Iteration 6860, loss = 5.67772
I0422 12:44:12.456755  7583 solver.cpp:244]     Train net output #0: loss = 5.67772 (* 1 = 5.67772 loss)
I0422 12:44:12.456770  7583 sgd_solver.cpp:106] Iteration 6860, lr = 0.0008
I0422 12:45:54.169265  7583 solver.cpp:228] Iteration 6880, loss = 6.08276
I0422 12:45:54.169706  7583 solver.cpp:244]     Train net output #0: loss = 6.08276 (* 1 = 6.08276 loss)
I0422 12:45:54.169720  7583 sgd_solver.cpp:106] Iteration 6880, lr = 0.0008
I0422 12:47:34.988176  7583 solver.cpp:228] Iteration 6900, loss = 8.10901
I0422 12:47:34.988533  7583 solver.cpp:244]     Train net output #0: loss = 8.10901 (* 1 = 8.10901 loss)
I0422 12:47:34.988548  7583 sgd_solver.cpp:106] Iteration 6900, lr = 0.0008
I0422 12:49:14.912084  7583 solver.cpp:228] Iteration 6920, loss = 8.1391
I0422 12:49:14.912554  7583 solver.cpp:244]     Train net output #0: loss = 8.1391 (* 1 = 8.1391 loss)
I0422 12:49:14.912571  7583 sgd_solver.cpp:106] Iteration 6920, lr = 0.0008
I0422 12:50:56.019511  7583 solver.cpp:228] Iteration 6940, loss = 7.12894
I0422 12:50:56.019973  7583 solver.cpp:244]     Train net output #0: loss = 7.12894 (* 1 = 7.12894 loss)
I0422 12:50:56.019989  7583 sgd_solver.cpp:106] Iteration 6940, lr = 0.0008
I0422 12:52:37.402400  7583 solver.cpp:228] Iteration 6960, loss = 6.08082
I0422 12:52:37.402859  7583 solver.cpp:244]     Train net output #0: loss = 6.08082 (* 1 = 6.08082 loss)
I0422 12:52:37.402874  7583 sgd_solver.cpp:106] Iteration 6960, lr = 0.0008
I0422 12:54:18.493139  7583 solver.cpp:228] Iteration 6980, loss = 4.58238
I0422 12:54:18.493587  7583 solver.cpp:244]     Train net output #0: loss = 4.58238 (* 1 = 4.58238 loss)
I0422 12:54:18.493605  7583 sgd_solver.cpp:106] Iteration 6980, lr = 0.0008
I0422 12:55:54.234241  7583 solver.cpp:337] Iteration 7000, Testing net (#0)
I0422 12:56:47.174374  7583 solver.cpp:404]     Test net output #0: loss = 5.1025 (* 1 = 5.1025 loss)
I0422 12:56:49.621640  7583 solver.cpp:228] Iteration 7000, loss = 4.38034
I0422 12:56:49.621704  7583 solver.cpp:244]     Train net output #0: loss = 4.38034 (* 1 = 4.38034 loss)
I0422 12:56:49.621717  7583 sgd_solver.cpp:106] Iteration 7000, lr = 0.0008
I0422 12:58:30.687304  7583 solver.cpp:228] Iteration 7020, loss = 4.27106
I0422 12:58:30.687762  7583 solver.cpp:244]     Train net output #0: loss = 4.27106 (* 1 = 4.27106 loss)
I0422 12:58:30.687777  7583 sgd_solver.cpp:106] Iteration 7020, lr = 0.0008
I0422 13:00:11.785835  7583 solver.cpp:228] Iteration 7040, loss = 5.39716
I0422 13:00:11.786259  7583 solver.cpp:244]     Train net output #0: loss = 5.39716 (* 1 = 5.39716 loss)
I0422 13:00:11.786274  7583 sgd_solver.cpp:106] Iteration 7040, lr = 0.0008
I0422 13:01:52.899976  7583 solver.cpp:228] Iteration 7060, loss = 6.13184
I0422 13:01:52.900436  7583 solver.cpp:244]     Train net output #0: loss = 6.13184 (* 1 = 6.13184 loss)
I0422 13:01:52.900451  7583 sgd_solver.cpp:106] Iteration 7060, lr = 0.0008
I0422 13:03:31.497185  7583 solver.cpp:228] Iteration 7080, loss = 8.14106
I0422 13:03:31.497637  7583 solver.cpp:244]     Train net output #0: loss = 8.14106 (* 1 = 8.14106 loss)
I0422 13:03:31.497653  7583 sgd_solver.cpp:106] Iteration 7080, lr = 0.0008
I0422 13:05:12.656388  7583 solver.cpp:228] Iteration 7100, loss = 6.94569
I0422 13:05:12.656838  7583 solver.cpp:244]     Train net output #0: loss = 6.94569 (* 1 = 6.94569 loss)
I0422 13:05:12.656853  7583 sgd_solver.cpp:106] Iteration 7100, lr = 0.0008
I0422 13:06:54.030272  7583 solver.cpp:228] Iteration 7120, loss = 6.23044
I0422 13:06:54.030710  7583 solver.cpp:244]     Train net output #0: loss = 6.23044 (* 1 = 6.23044 loss)
I0422 13:06:54.030724  7583 sgd_solver.cpp:106] Iteration 7120, lr = 0.0008
I0422 13:08:34.006635  7583 solver.cpp:228] Iteration 7140, loss = 6.49114
I0422 13:08:34.007076  7583 solver.cpp:244]     Train net output #0: loss = 6.49114 (* 1 = 6.49114 loss)
I0422 13:08:34.007091  7583 sgd_solver.cpp:106] Iteration 7140, lr = 0.0008
I0422 13:10:14.531790  7583 solver.cpp:228] Iteration 7160, loss = 7.84507
I0422 13:10:14.532222  7583 solver.cpp:244]     Train net output #0: loss = 7.84507 (* 1 = 7.84507 loss)
I0422 13:10:14.532238  7583 sgd_solver.cpp:106] Iteration 7160, lr = 0.0008
I0422 13:11:55.563097  7583 solver.cpp:228] Iteration 7180, loss = 6.21548
I0422 13:11:55.563544  7583 solver.cpp:244]     Train net output #0: loss = 6.21548 (* 1 = 6.21548 loss)
I0422 13:11:55.563558  7583 sgd_solver.cpp:106] Iteration 7180, lr = 0.0008
I0422 13:13:35.951743  7583 solver.cpp:228] Iteration 7200, loss = 7.10019
I0422 13:13:35.952133  7583 solver.cpp:244]     Train net output #0: loss = 7.10019 (* 1 = 7.10019 loss)
I0422 13:13:35.952147  7583 sgd_solver.cpp:106] Iteration 7200, lr = 0.0008
I0422 13:15:16.908336  7583 solver.cpp:228] Iteration 7220, loss = 5.50095
I0422 13:15:16.908660  7583 solver.cpp:244]     Train net output #0: loss = 5.50095 (* 1 = 5.50095 loss)
I0422 13:15:16.908674  7583 sgd_solver.cpp:106] Iteration 7220, lr = 0.0008
I0422 13:16:57.235379  7583 solver.cpp:228] Iteration 7240, loss = 7.67166
I0422 13:16:57.235816  7583 solver.cpp:244]     Train net output #0: loss = 7.67166 (* 1 = 7.67166 loss)
I0422 13:16:57.235831  7583 sgd_solver.cpp:106] Iteration 7240, lr = 0.0008
I0422 13:18:36.945595  7583 solver.cpp:228] Iteration 7260, loss = 9.11407
I0422 13:18:36.946038  7583 solver.cpp:244]     Train net output #0: loss = 9.11407 (* 1 = 9.11407 loss)
I0422 13:18:36.946051  7583 sgd_solver.cpp:106] Iteration 7260, lr = 0.0008
I0422 13:20:17.482952  7583 solver.cpp:228] Iteration 7280, loss = 4.78422
I0422 13:20:17.483398  7583 solver.cpp:244]     Train net output #0: loss = 4.78422 (* 1 = 4.78422 loss)
I0422 13:20:17.483410  7583 sgd_solver.cpp:106] Iteration 7280, lr = 0.0008
I0422 13:21:59.569368  7583 solver.cpp:228] Iteration 7300, loss = 4.40655
I0422 13:21:59.569787  7583 solver.cpp:244]     Train net output #0: loss = 4.40655 (* 1 = 4.40655 loss)
I0422 13:21:59.569802  7583 sgd_solver.cpp:106] Iteration 7300, lr = 0.0008
I0422 13:23:42.883277  7583 solver.cpp:228] Iteration 7320, loss = 4.68105
I0422 13:23:42.883698  7583 solver.cpp:244]     Train net output #0: loss = 4.68105 (* 1 = 4.68105 loss)
I0422 13:23:42.883718  7583 sgd_solver.cpp:106] Iteration 7320, lr = 0.0008
I0422 13:25:24.294019  7583 solver.cpp:228] Iteration 7340, loss = 5.97704
I0422 13:25:24.294466  7583 solver.cpp:244]     Train net output #0: loss = 5.97704 (* 1 = 5.97704 loss)
I0422 13:25:24.294481  7583 sgd_solver.cpp:106] Iteration 7340, lr = 0.0008
I0422 13:27:05.351136  7583 solver.cpp:228] Iteration 7360, loss = 5.34072
I0422 13:27:05.351614  7583 solver.cpp:244]     Train net output #0: loss = 5.34072 (* 1 = 5.34072 loss)
I0422 13:27:05.351629  7583 sgd_solver.cpp:106] Iteration 7360, lr = 0.0008
I0422 13:28:48.122907  7583 solver.cpp:228] Iteration 7380, loss = 5.5601
I0422 13:28:48.123342  7583 solver.cpp:244]     Train net output #0: loss = 5.5601 (* 1 = 5.5601 loss)
I0422 13:28:48.123355  7583 sgd_solver.cpp:106] Iteration 7380, lr = 0.0008
I0422 13:30:29.893642  7583 solver.cpp:228] Iteration 7400, loss = 6.88681
I0422 13:30:29.894022  7583 solver.cpp:244]     Train net output #0: loss = 6.88681 (* 1 = 6.88681 loss)
I0422 13:30:29.894039  7583 sgd_solver.cpp:106] Iteration 7400, lr = 0.0008
I0422 13:32:10.383976  7583 solver.cpp:228] Iteration 7420, loss = 5.5591
I0422 13:32:10.384433  7583 solver.cpp:244]     Train net output #0: loss = 5.5591 (* 1 = 5.5591 loss)
I0422 13:32:10.384449  7583 sgd_solver.cpp:106] Iteration 7420, lr = 0.0008
I0422 13:33:52.205662  7583 solver.cpp:228] Iteration 7440, loss = 5.92021
I0422 13:33:52.206174  7583 solver.cpp:244]     Train net output #0: loss = 5.92021 (* 1 = 5.92021 loss)
I0422 13:33:52.206190  7583 sgd_solver.cpp:106] Iteration 7440, lr = 0.0008
I0422 13:35:34.523999  7583 solver.cpp:228] Iteration 7460, loss = 5.78235
I0422 13:35:34.524448  7583 solver.cpp:244]     Train net output #0: loss = 5.78235 (* 1 = 5.78235 loss)
I0422 13:35:34.524463  7583 sgd_solver.cpp:106] Iteration 7460, lr = 0.0008
I0422 13:37:17.202045  7583 solver.cpp:228] Iteration 7480, loss = 4.87459
I0422 13:37:17.202471  7583 solver.cpp:244]     Train net output #0: loss = 4.87459 (* 1 = 4.87459 loss)
I0422 13:37:17.202486  7583 sgd_solver.cpp:106] Iteration 7480, lr = 0.0008
I0422 13:38:58.908602  7583 solver.cpp:228] Iteration 7500, loss = 5.42909
I0422 13:38:58.909065  7583 solver.cpp:244]     Train net output #0: loss = 5.42909 (* 1 = 5.42909 loss)
I0422 13:38:58.909083  7583 sgd_solver.cpp:106] Iteration 7500, lr = 0.0008
I0422 13:40:41.019345  7583 solver.cpp:228] Iteration 7520, loss = 4.23414
I0422 13:40:41.019798  7583 solver.cpp:244]     Train net output #0: loss = 4.23414 (* 1 = 4.23414 loss)
I0422 13:40:41.019812  7583 sgd_solver.cpp:106] Iteration 7520, lr = 0.0008
I0422 13:42:21.431996  7583 solver.cpp:228] Iteration 7540, loss = 7.47027
I0422 13:42:21.432409  7583 solver.cpp:244]     Train net output #0: loss = 7.47027 (* 1 = 7.47027 loss)
I0422 13:42:21.432423  7583 sgd_solver.cpp:106] Iteration 7540, lr = 0.0008
I0422 13:44:05.464490  7583 solver.cpp:228] Iteration 7560, loss = 5.49909
I0422 13:44:05.464917  7583 solver.cpp:244]     Train net output #0: loss = 5.49909 (* 1 = 5.49909 loss)
I0422 13:44:05.464933  7583 sgd_solver.cpp:106] Iteration 7560, lr = 0.0008
I0422 13:45:49.267714  7583 solver.cpp:228] Iteration 7580, loss = 4.42381
I0422 13:45:49.268115  7583 solver.cpp:244]     Train net output #0: loss = 4.42381 (* 1 = 4.42381 loss)
I0422 13:45:49.268128  7583 sgd_solver.cpp:106] Iteration 7580, lr = 0.0008
I0422 13:47:32.206153  7583 solver.cpp:228] Iteration 7600, loss = 4.66766
I0422 13:47:32.206593  7583 solver.cpp:244]     Train net output #0: loss = 4.66766 (* 1 = 4.66766 loss)
I0422 13:47:32.206607  7583 sgd_solver.cpp:106] Iteration 7600, lr = 0.0008
I0422 13:49:16.002007  7583 solver.cpp:228] Iteration 7620, loss = 4.85409
I0422 13:49:16.002449  7583 solver.cpp:244]     Train net output #0: loss = 4.85409 (* 1 = 4.85409 loss)
I0422 13:49:16.002465  7583 sgd_solver.cpp:106] Iteration 7620, lr = 0.0008
I0422 13:50:59.449040  7583 solver.cpp:228] Iteration 7640, loss = 5.20457
I0422 13:50:59.449453  7583 solver.cpp:244]     Train net output #0: loss = 5.20457 (* 1 = 5.20457 loss)
I0422 13:50:59.449468  7583 sgd_solver.cpp:106] Iteration 7640, lr = 0.0008
I0422 13:52:43.459137  7583 solver.cpp:228] Iteration 7660, loss = 4.51461
I0422 13:52:43.459533  7583 solver.cpp:244]     Train net output #0: loss = 4.51461 (* 1 = 4.51461 loss)
I0422 13:52:43.459550  7583 sgd_solver.cpp:106] Iteration 7660, lr = 0.0008
I0422 13:54:30.848408  7583 solver.cpp:228] Iteration 7680, loss = 5.04996
I0422 13:54:30.848820  7583 solver.cpp:244]     Train net output #0: loss = 5.04996 (* 1 = 5.04996 loss)
I0422 13:54:30.848835  7583 sgd_solver.cpp:106] Iteration 7680, lr = 0.0008
I0422 13:56:19.231130  7583 solver.cpp:228] Iteration 7700, loss = 5.78794
I0422 13:56:19.231565  7583 solver.cpp:244]     Train net output #0: loss = 5.78794 (* 1 = 5.78794 loss)
I0422 13:56:19.231580  7583 sgd_solver.cpp:106] Iteration 7700, lr = 0.0008
I0422 13:58:09.549010  7583 solver.cpp:228] Iteration 7720, loss = 4.15645
I0422 13:58:09.549470  7583 solver.cpp:244]     Train net output #0: loss = 4.15645 (* 1 = 4.15645 loss)
I0422 13:58:09.549491  7583 sgd_solver.cpp:106] Iteration 7720, lr = 0.0008
I0422 14:00:00.735864  7583 solver.cpp:228] Iteration 7740, loss = 4.61098
I0422 14:00:00.736304  7583 solver.cpp:244]     Train net output #0: loss = 4.61098 (* 1 = 4.61098 loss)
I0422 14:00:00.736320  7583 sgd_solver.cpp:106] Iteration 7740, lr = 0.0008
I0422 14:01:50.174240  7583 solver.cpp:228] Iteration 7760, loss = 5.45445
I0422 14:01:50.174643  7583 solver.cpp:244]     Train net output #0: loss = 5.45445 (* 1 = 5.45445 loss)
I0422 14:01:50.174656  7583 sgd_solver.cpp:106] Iteration 7760, lr = 0.0008
I0422 14:03:41.122159  7583 solver.cpp:228] Iteration 7780, loss = 5.24203
I0422 14:03:41.122701  7583 solver.cpp:244]     Train net output #0: loss = 5.24203 (* 1 = 5.24203 loss)
I0422 14:03:41.122722  7583 sgd_solver.cpp:106] Iteration 7780, lr = 0.0008
I0422 14:05:32.560302  7583 solver.cpp:228] Iteration 7800, loss = 4.81562
I0422 14:05:32.560636  7583 solver.cpp:244]     Train net output #0: loss = 4.81562 (* 1 = 4.81562 loss)
I0422 14:05:32.560650  7583 sgd_solver.cpp:106] Iteration 7800, lr = 0.0008
I0422 14:07:19.330065  7583 solver.cpp:228] Iteration 7820, loss = 7.9353
I0422 14:07:19.330482  7583 solver.cpp:244]     Train net output #0: loss = 7.9353 (* 1 = 7.9353 loss)
I0422 14:07:19.330497  7583 sgd_solver.cpp:106] Iteration 7820, lr = 0.0008
I0422 14:09:08.531688  7583 solver.cpp:228] Iteration 7840, loss = 4.68959
I0422 14:09:08.532124  7583 solver.cpp:244]     Train net output #0: loss = 4.68959 (* 1 = 4.68959 loss)
I0422 14:09:08.532137  7583 sgd_solver.cpp:106] Iteration 7840, lr = 0.0008
I0422 14:10:57.035095  7583 solver.cpp:228] Iteration 7860, loss = 5.28715
I0422 14:10:57.035598  7583 solver.cpp:244]     Train net output #0: loss = 5.28715 (* 1 = 5.28715 loss)
I0422 14:10:57.035614  7583 sgd_solver.cpp:106] Iteration 7860, lr = 0.0008
I0422 14:12:45.559087  7583 solver.cpp:228] Iteration 7880, loss = 6.16895
I0422 14:12:45.559567  7583 solver.cpp:244]     Train net output #0: loss = 6.16895 (* 1 = 6.16895 loss)
I0422 14:12:45.559581  7583 sgd_solver.cpp:106] Iteration 7880, lr = 0.0008
I0422 14:14:33.349051  7583 solver.cpp:228] Iteration 7900, loss = 7.06328
I0422 14:14:33.349604  7583 solver.cpp:244]     Train net output #0: loss = 7.06328 (* 1 = 7.06328 loss)
I0422 14:14:33.349623  7583 sgd_solver.cpp:106] Iteration 7900, lr = 0.0008
I0422 14:16:17.849393  7583 solver.cpp:228] Iteration 7920, loss = 5.85668
I0422 14:16:17.849822  7583 solver.cpp:244]     Train net output #0: loss = 5.85668 (* 1 = 5.85668 loss)
I0422 14:16:17.849838  7583 sgd_solver.cpp:106] Iteration 7920, lr = 0.0008
I0422 14:18:03.643523  7583 solver.cpp:228] Iteration 7940, loss = 5.26627
I0422 14:18:03.644002  7583 solver.cpp:244]     Train net output #0: loss = 5.26627 (* 1 = 5.26627 loss)
I0422 14:18:03.644017  7583 sgd_solver.cpp:106] Iteration 7940, lr = 0.0008
I0422 14:19:51.891140  7583 solver.cpp:228] Iteration 7960, loss = 4.29487
I0422 14:19:51.891589  7583 solver.cpp:244]     Train net output #0: loss = 4.29487 (* 1 = 4.29487 loss)
I0422 14:19:51.891607  7583 sgd_solver.cpp:106] Iteration 7960, lr = 0.0008
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0422 14:30:35.800019 23268 solver.cpp:48] Initializing solver from parameters: 
test_iter: 25
test_interval: 1000
base_lr: 0.0008
display: 20
max_iter: 100000
lr_policy: "step"
gamma: 0.3
momentum: 0.9
weight_decay: 0.0005
stepsize: 20000
snapshot: 10000
snapshot_prefix: "models/model"
net: "fcn_alxnet_train_val.prototxt"
I0422 14:30:35.813482 23268 solver.cpp:91] Creating training net from net file: fcn_alxnet_train_val.prototxt
I0422 14:30:35.826685 23268 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0422 14:30:35.827028 23268 net.cpp:49] Initializing net from parameters: 
name: "FCN-AlexNet-joint"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "jointLocation"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 104
    mean_value: 116
    mean_value: 122
  }
  image_data_param {
    source: "/home/mengxin1/HumanPoseDetect/preprocess/train_label_resized.txt"
    batch_size: 10
    shuffle: true
    root_folder: "/home/mengxin1/mpii_human_pose_v1_images_resized/"
    label_num: 48
  }
}
layer {
  name: "jointMap"
  type: "PositionToMap"
  bottom: "data"
  bottom: "jointLocation"
  top: "jointMap"
  gaussian_param {
    std: 25
    half_window_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 100
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "cconv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "cconv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fcc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fcc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 6
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fcc6"
  top: "fcc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fcc6"
  top: "fcc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fcc7"
  type: "Convolution"
  bottom: "fcc6"
  top: "fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fcc7"
  top: "fcc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fcc7"
  top: "fcc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score-fr"
  type: "Convolution"
  bottom: "fcc7"
  top: "score-fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    engine: CAFFE
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "score-fcc7"
  top: "bigscore"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 63
    group: 16
    stride: 32
  }
}
layer {
  name: "crop"
  type: "Crop"
  bottom: "bigscore"
  bottom: "data"
  top: "score"
}
layer {
  name: "balance"
  type: "Balance"
  bottom: "score"
  bottom: "jointMap"
  top: "bscore"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "bscore"
  bottom: "jointMap"
  top: "loss"
}
I0422 14:30:35.827183 23268 layer_factory.hpp:77] Creating layer data
I0422 14:30:35.827215 23268 net.cpp:91] Creating Layer data
I0422 14:30:35.827226 23268 net.cpp:399] data -> data
I0422 14:30:35.827244 23268 net.cpp:399] data -> jointLocation
I0422 14:30:35.827265 23268 image_data_layer.cpp:40] Opening file /home/mengxin1/HumanPoseDetect/preprocess/train_label_resized.txt
I0422 14:30:35.934830 23268 image_data_layer.cpp:57] Shuffling data
I0422 14:30:35.937871 23268 image_data_layer.cpp:62] A total of 9580 images.
I0422 14:30:36.156168 23268 image_data_layer.cpp:89] output data size: 10,3,512,512
I0422 14:30:36.255436 23268 net.cpp:141] Setting up data
I0422 14:30:36.255499 23268 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 14:30:36.255508 23268 net.cpp:148] Top shape: 10 48 1 1 (480)
I0422 14:30:36.255513 23268 net.cpp:156] Memory required for data: 31459200
I0422 14:30:36.255524 23268 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 14:30:36.255544 23268 net.cpp:91] Creating Layer data_data_0_split
I0422 14:30:36.255551 23268 net.cpp:425] data_data_0_split <- data
I0422 14:30:36.255563 23268 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 14:30:36.255579 23268 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 14:30:36.255587 23268 net.cpp:399] data_data_0_split -> data_data_0_split_2
I0422 14:30:36.255743 23268 net.cpp:141] Setting up data_data_0_split
I0422 14:30:36.255755 23268 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 14:30:36.255761 23268 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 14:30:36.255766 23268 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 14:30:36.255770 23268 net.cpp:156] Memory required for data: 125831040
I0422 14:30:36.255780 23268 layer_factory.hpp:77] Creating layer jointMap
I0422 14:30:36.255791 23268 net.cpp:91] Creating Layer jointMap
I0422 14:30:36.255795 23268 net.cpp:425] jointMap <- data_data_0_split_0
I0422 14:30:36.255801 23268 net.cpp:425] jointMap <- jointLocation
I0422 14:30:36.255808 23268 net.cpp:399] jointMap -> jointMap
I0422 14:30:36.255844 23268 net.cpp:141] Setting up jointMap
I0422 14:30:36.255853 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:36.255858 23268 net.cpp:156] Memory required for data: 293603200
I0422 14:30:36.255862 23268 layer_factory.hpp:77] Creating layer jointMap_jointMap_0_split
I0422 14:30:36.255869 23268 net.cpp:91] Creating Layer jointMap_jointMap_0_split
I0422 14:30:36.255873 23268 net.cpp:425] jointMap_jointMap_0_split <- jointMap
I0422 14:30:36.255880 23268 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_0
I0422 14:30:36.255888 23268 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_1
I0422 14:30:36.255937 23268 net.cpp:141] Setting up jointMap_jointMap_0_split
I0422 14:30:36.255945 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:36.255950 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:36.255954 23268 net.cpp:156] Memory required for data: 629147520
I0422 14:30:36.255959 23268 layer_factory.hpp:77] Creating layer conv1
I0422 14:30:36.255977 23268 net.cpp:91] Creating Layer conv1
I0422 14:30:36.255982 23268 net.cpp:425] conv1 <- data_data_0_split_1
I0422 14:30:36.255990 23268 net.cpp:399] conv1 -> conv1
I0422 14:30:36.827067 23268 net.cpp:141] Setting up conv1
I0422 14:30:36.827128 23268 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 14:30:36.827134 23268 net.cpp:156] Memory required for data: 748095360
I0422 14:30:36.827158 23268 layer_factory.hpp:77] Creating layer relu1
I0422 14:30:36.827178 23268 net.cpp:91] Creating Layer relu1
I0422 14:30:36.827184 23268 net.cpp:425] relu1 <- conv1
I0422 14:30:36.827194 23268 net.cpp:386] relu1 -> conv1 (in-place)
I0422 14:30:36.832087 23268 net.cpp:141] Setting up relu1
I0422 14:30:36.832120 23268 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 14:30:36.832124 23268 net.cpp:156] Memory required for data: 867043200
I0422 14:30:36.832131 23268 layer_factory.hpp:77] Creating layer pool1
I0422 14:30:36.832150 23268 net.cpp:91] Creating Layer pool1
I0422 14:30:36.832155 23268 net.cpp:425] pool1 <- conv1
I0422 14:30:36.832165 23268 net.cpp:399] pool1 -> pool1
I0422 14:30:36.832264 23268 net.cpp:141] Setting up pool1
I0422 14:30:36.832274 23268 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 14:30:36.832278 23268 net.cpp:156] Memory required for data: 896780160
I0422 14:30:36.832283 23268 layer_factory.hpp:77] Creating layer norm1
I0422 14:30:36.832296 23268 net.cpp:91] Creating Layer norm1
I0422 14:30:36.832301 23268 net.cpp:425] norm1 <- pool1
I0422 14:30:36.832309 23268 net.cpp:399] norm1 -> norm1
I0422 14:30:36.837149 23268 net.cpp:141] Setting up norm1
I0422 14:30:36.837194 23268 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 14:30:36.837199 23268 net.cpp:156] Memory required for data: 926517120
I0422 14:30:36.837208 23268 layer_factory.hpp:77] Creating layer conv2
I0422 14:30:36.837237 23268 net.cpp:91] Creating Layer conv2
I0422 14:30:36.837244 23268 net.cpp:425] conv2 <- norm1
I0422 14:30:36.837255 23268 net.cpp:399] conv2 -> conv2
I0422 14:30:36.880458 23268 net.cpp:141] Setting up conv2
I0422 14:30:36.880513 23268 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 14:30:36.880518 23268 net.cpp:156] Memory required for data: 1005815680
I0422 14:30:36.880540 23268 layer_factory.hpp:77] Creating layer relu2
I0422 14:30:36.880558 23268 net.cpp:91] Creating Layer relu2
I0422 14:30:36.880564 23268 net.cpp:425] relu2 <- conv2
I0422 14:30:36.880574 23268 net.cpp:386] relu2 -> conv2 (in-place)
I0422 14:30:36.886281 23268 net.cpp:141] Setting up relu2
I0422 14:30:36.886324 23268 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 14:30:36.886329 23268 net.cpp:156] Memory required for data: 1085114240
I0422 14:30:36.886343 23268 layer_factory.hpp:77] Creating layer pool2
I0422 14:30:36.886359 23268 net.cpp:91] Creating Layer pool2
I0422 14:30:36.886364 23268 net.cpp:425] pool2 <- conv2
I0422 14:30:36.886373 23268 net.cpp:399] pool2 -> pool2
I0422 14:30:36.886478 23268 net.cpp:141] Setting up pool2
I0422 14:30:36.886490 23268 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 14:30:36.886494 23268 net.cpp:156] Memory required for data: 1104938880
I0422 14:30:36.886499 23268 layer_factory.hpp:77] Creating layer norm2
I0422 14:30:36.886509 23268 net.cpp:91] Creating Layer norm2
I0422 14:30:36.886514 23268 net.cpp:425] norm2 <- pool2
I0422 14:30:36.886523 23268 net.cpp:399] norm2 -> norm2
I0422 14:30:36.891821 23268 net.cpp:141] Setting up norm2
I0422 14:30:36.891849 23268 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 14:30:36.891854 23268 net.cpp:156] Memory required for data: 1124763520
I0422 14:30:36.891860 23268 layer_factory.hpp:77] Creating layer conv3
I0422 14:30:36.891885 23268 net.cpp:91] Creating Layer conv3
I0422 14:30:36.891893 23268 net.cpp:425] conv3 <- norm2
I0422 14:30:36.891903 23268 net.cpp:399] conv3 -> conv3
I0422 14:30:36.917349 23268 net.cpp:141] Setting up conv3
I0422 14:30:36.917403 23268 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 14:30:36.917408 23268 net.cpp:156] Memory required for data: 1154500480
I0422 14:30:36.917433 23268 layer_factory.hpp:77] Creating layer relu3
I0422 14:30:36.917449 23268 net.cpp:91] Creating Layer relu3
I0422 14:30:36.917456 23268 net.cpp:425] relu3 <- conv3
I0422 14:30:36.917466 23268 net.cpp:386] relu3 -> conv3 (in-place)
I0422 14:30:36.921082 23268 net.cpp:141] Setting up relu3
I0422 14:30:36.921108 23268 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 14:30:36.921113 23268 net.cpp:156] Memory required for data: 1184237440
I0422 14:30:36.921119 23268 layer_factory.hpp:77] Creating layer cconv4
I0422 14:30:36.921141 23268 net.cpp:91] Creating Layer cconv4
I0422 14:30:36.921146 23268 net.cpp:425] cconv4 <- conv3
I0422 14:30:36.921157 23268 net.cpp:399] cconv4 -> conv4
I0422 14:30:36.949116 23268 net.cpp:141] Setting up cconv4
I0422 14:30:36.949178 23268 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 14:30:36.949183 23268 net.cpp:156] Memory required for data: 1213974400
I0422 14:30:36.949198 23268 layer_factory.hpp:77] Creating layer relu4
I0422 14:30:36.949216 23268 net.cpp:91] Creating Layer relu4
I0422 14:30:36.949223 23268 net.cpp:425] relu4 <- conv4
I0422 14:30:36.949232 23268 net.cpp:386] relu4 -> conv4 (in-place)
I0422 14:30:36.953047 23268 net.cpp:141] Setting up relu4
I0422 14:30:36.953073 23268 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 14:30:36.953078 23268 net.cpp:156] Memory required for data: 1243711360
I0422 14:30:36.953083 23268 layer_factory.hpp:77] Creating layer cconv5
I0422 14:30:36.953110 23268 net.cpp:91] Creating Layer cconv5
I0422 14:30:36.953116 23268 net.cpp:425] cconv5 <- conv4
I0422 14:30:36.953125 23268 net.cpp:399] cconv5 -> conv5
I0422 14:30:36.983845 23268 net.cpp:141] Setting up cconv5
I0422 14:30:36.983907 23268 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 14:30:36.983912 23268 net.cpp:156] Memory required for data: 1263536000
I0422 14:30:36.983934 23268 layer_factory.hpp:77] Creating layer relu5
I0422 14:30:36.983952 23268 net.cpp:91] Creating Layer relu5
I0422 14:30:36.983958 23268 net.cpp:425] relu5 <- conv5
I0422 14:30:36.983968 23268 net.cpp:386] relu5 -> conv5 (in-place)
I0422 14:30:36.991291 23268 net.cpp:141] Setting up relu5
I0422 14:30:36.991348 23268 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 14:30:36.991353 23268 net.cpp:156] Memory required for data: 1283360640
I0422 14:30:36.991363 23268 layer_factory.hpp:77] Creating layer pool5
I0422 14:30:36.991381 23268 net.cpp:91] Creating Layer pool5
I0422 14:30:36.991387 23268 net.cpp:425] pool5 <- conv5
I0422 14:30:36.991400 23268 net.cpp:399] pool5 -> pool5
I0422 14:30:36.991521 23268 net.cpp:141] Setting up pool5
I0422 14:30:36.991533 23268 net.cpp:148] Top shape: 10 256 22 22 (1239040)
I0422 14:30:36.991539 23268 net.cpp:156] Memory required for data: 1288316800
I0422 14:30:36.991550 23268 layer_factory.hpp:77] Creating layer fcc6
I0422 14:30:36.991570 23268 net.cpp:91] Creating Layer fcc6
I0422 14:30:36.991575 23268 net.cpp:425] fcc6 <- pool5
I0422 14:30:36.991583 23268 net.cpp:399] fcc6 -> fcc6
I0422 14:30:37.519657 23268 net.cpp:141] Setting up fcc6
I0422 14:30:37.519714 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:37.519721 23268 net.cpp:156] Memory required for data: 1335666560
I0422 14:30:37.519736 23268 layer_factory.hpp:77] Creating layer relu6
I0422 14:30:37.519752 23268 net.cpp:91] Creating Layer relu6
I0422 14:30:37.519759 23268 net.cpp:425] relu6 <- fcc6
I0422 14:30:37.519772 23268 net.cpp:386] relu6 -> fcc6 (in-place)
I0422 14:30:37.522467 23268 net.cpp:141] Setting up relu6
I0422 14:30:37.522511 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:37.522516 23268 net.cpp:156] Memory required for data: 1383016320
I0422 14:30:37.522522 23268 layer_factory.hpp:77] Creating layer drop6
I0422 14:30:37.522547 23268 net.cpp:91] Creating Layer drop6
I0422 14:30:37.522553 23268 net.cpp:425] drop6 <- fcc6
I0422 14:30:37.522562 23268 net.cpp:386] drop6 -> fcc6 (in-place)
I0422 14:30:37.522647 23268 net.cpp:141] Setting up drop6
I0422 14:30:37.522661 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:37.522668 23268 net.cpp:156] Memory required for data: 1430366080
I0422 14:30:37.522672 23268 layer_factory.hpp:77] Creating layer fcc7
I0422 14:30:37.522688 23268 net.cpp:91] Creating Layer fcc7
I0422 14:30:37.522693 23268 net.cpp:425] fcc7 <- fcc6
I0422 14:30:37.522703 23268 net.cpp:399] fcc7 -> fcc7
I0422 14:30:37.748986 23268 net.cpp:141] Setting up fcc7
I0422 14:30:37.749039 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:37.749044 23268 net.cpp:156] Memory required for data: 1477715840
I0422 14:30:37.749055 23268 layer_factory.hpp:77] Creating layer relu7
I0422 14:30:37.749071 23268 net.cpp:91] Creating Layer relu7
I0422 14:30:37.749078 23268 net.cpp:425] relu7 <- fcc7
I0422 14:30:37.749090 23268 net.cpp:386] relu7 -> fcc7 (in-place)
I0422 14:30:37.749925 23268 net.cpp:141] Setting up relu7
I0422 14:30:37.749938 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:37.749941 23268 net.cpp:156] Memory required for data: 1525065600
I0422 14:30:37.749946 23268 layer_factory.hpp:77] Creating layer drop7
I0422 14:30:37.749955 23268 net.cpp:91] Creating Layer drop7
I0422 14:30:37.749960 23268 net.cpp:425] drop7 <- fcc7
I0422 14:30:37.749969 23268 net.cpp:386] drop7 -> fcc7 (in-place)
I0422 14:30:37.750015 23268 net.cpp:141] Setting up drop7
I0422 14:30:37.750023 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:37.750026 23268 net.cpp:156] Memory required for data: 1572415360
I0422 14:30:37.750031 23268 layer_factory.hpp:77] Creating layer score-fr
I0422 14:30:37.750048 23268 net.cpp:91] Creating Layer score-fr
I0422 14:30:37.750053 23268 net.cpp:425] score-fr <- fcc7
I0422 14:30:37.750063 23268 net.cpp:399] score-fr -> score-fcc7
I0422 14:30:37.751152 23268 net.cpp:141] Setting up score-fr
I0422 14:30:37.751163 23268 net.cpp:148] Top shape: 10 16 17 17 (46240)
I0422 14:30:37.751168 23268 net.cpp:156] Memory required for data: 1572600320
I0422 14:30:37.751175 23268 layer_factory.hpp:77] Creating layer upsample
I0422 14:30:37.751185 23268 net.cpp:91] Creating Layer upsample
I0422 14:30:37.751190 23268 net.cpp:425] upsample <- score-fcc7
I0422 14:30:37.751199 23268 net.cpp:399] upsample -> bigscore
I0422 14:30:37.753187 23268 net.cpp:141] Setting up upsample
I0422 14:30:37.753201 23268 net.cpp:148] Top shape: 10 16 575 575 (52900000)
I0422 14:30:37.753206 23268 net.cpp:156] Memory required for data: 1784200320
I0422 14:30:37.753221 23268 layer_factory.hpp:77] Creating layer crop
I0422 14:30:37.753237 23268 net.cpp:91] Creating Layer crop
I0422 14:30:37.753242 23268 net.cpp:425] crop <- bigscore
I0422 14:30:37.753247 23268 net.cpp:425] crop <- data_data_0_split_2
I0422 14:30:37.753253 23268 net.cpp:399] crop -> score
I0422 14:30:37.753306 23268 net.cpp:141] Setting up crop
I0422 14:30:37.753321 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:37.753325 23268 net.cpp:156] Memory required for data: 1951972480
I0422 14:30:37.753330 23268 layer_factory.hpp:77] Creating layer balance
I0422 14:30:37.753341 23268 net.cpp:91] Creating Layer balance
I0422 14:30:37.753346 23268 net.cpp:425] balance <- score
I0422 14:30:37.753351 23268 net.cpp:425] balance <- jointMap_jointMap_0_split_0
I0422 14:30:37.753357 23268 net.cpp:399] balance -> bscore
I0422 14:30:37.753419 23268 net.cpp:141] Setting up balance
I0422 14:30:37.753428 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:37.753432 23268 net.cpp:156] Memory required for data: 2119744640
I0422 14:30:37.753437 23268 layer_factory.hpp:77] Creating layer loss
I0422 14:30:37.753446 23268 net.cpp:91] Creating Layer loss
I0422 14:30:37.753450 23268 net.cpp:425] loss <- bscore
I0422 14:30:37.753455 23268 net.cpp:425] loss <- jointMap_jointMap_0_split_1
I0422 14:30:37.753464 23268 net.cpp:399] loss -> loss
I0422 14:30:37.753528 23268 net.cpp:141] Setting up loss
I0422 14:30:37.753537 23268 net.cpp:148] Top shape: (1)
I0422 14:30:37.753541 23268 net.cpp:151]     with loss weight 1
I0422 14:30:37.753578 23268 net.cpp:156] Memory required for data: 2119744644
I0422 14:30:37.753584 23268 net.cpp:217] loss needs backward computation.
I0422 14:30:37.753589 23268 net.cpp:217] balance needs backward computation.
I0422 14:30:37.753594 23268 net.cpp:217] crop needs backward computation.
I0422 14:30:37.753598 23268 net.cpp:217] upsample needs backward computation.
I0422 14:30:37.753603 23268 net.cpp:217] score-fr needs backward computation.
I0422 14:30:37.753607 23268 net.cpp:217] drop7 needs backward computation.
I0422 14:30:37.753610 23268 net.cpp:217] relu7 needs backward computation.
I0422 14:30:37.753614 23268 net.cpp:217] fcc7 needs backward computation.
I0422 14:30:37.753618 23268 net.cpp:217] drop6 needs backward computation.
I0422 14:30:37.753623 23268 net.cpp:217] relu6 needs backward computation.
I0422 14:30:37.753626 23268 net.cpp:217] fcc6 needs backward computation.
I0422 14:30:37.753630 23268 net.cpp:217] pool5 needs backward computation.
I0422 14:30:37.753634 23268 net.cpp:217] relu5 needs backward computation.
I0422 14:30:37.753638 23268 net.cpp:217] cconv5 needs backward computation.
I0422 14:30:37.753643 23268 net.cpp:217] relu4 needs backward computation.
I0422 14:30:37.753648 23268 net.cpp:217] cconv4 needs backward computation.
I0422 14:30:37.753655 23268 net.cpp:217] relu3 needs backward computation.
I0422 14:30:37.753660 23268 net.cpp:217] conv3 needs backward computation.
I0422 14:30:37.753664 23268 net.cpp:217] norm2 needs backward computation.
I0422 14:30:37.753669 23268 net.cpp:217] pool2 needs backward computation.
I0422 14:30:37.753674 23268 net.cpp:217] relu2 needs backward computation.
I0422 14:30:37.753677 23268 net.cpp:217] conv2 needs backward computation.
I0422 14:30:37.753681 23268 net.cpp:217] norm1 needs backward computation.
I0422 14:30:37.753685 23268 net.cpp:217] pool1 needs backward computation.
I0422 14:30:37.753690 23268 net.cpp:217] relu1 needs backward computation.
I0422 14:30:37.753693 23268 net.cpp:217] conv1 needs backward computation.
I0422 14:30:37.753698 23268 net.cpp:219] jointMap_jointMap_0_split does not need backward computation.
I0422 14:30:37.753703 23268 net.cpp:219] jointMap does not need backward computation.
I0422 14:30:37.753708 23268 net.cpp:219] data_data_0_split does not need backward computation.
I0422 14:30:37.753713 23268 net.cpp:219] data does not need backward computation.
I0422 14:30:37.753717 23268 net.cpp:261] This network produces output loss
I0422 14:30:37.753741 23268 net.cpp:274] Network initialization done.
I0422 14:30:37.754853 23268 solver.cpp:181] Creating test net (#0) specified by net file: fcn_alxnet_train_val.prototxt
I0422 14:30:37.754904 23268 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0422 14:30:37.755213 23268 net.cpp:49] Initializing net from parameters: 
name: "FCN-AlexNet-joint"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "jointLocation"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 104
    mean_value: 116
    mean_value: 122
  }
  image_data_param {
    source: "/home/mengxin1/HumanPoseDetect/preprocess/val_label_resized.txt"
    batch_size: 10
    root_folder: "/home/mengxin1/mpii_human_pose_v1_images_resized/"
    label_num: 48
  }
}
layer {
  name: "jointMap"
  type: "PositionToMap"
  bottom: "data"
  bottom: "jointLocation"
  top: "jointMap"
  gaussian_param {
    std: 25
    half_window_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 100
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "cconv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "cconv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fcc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fcc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 6
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fcc6"
  top: "fcc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fcc6"
  top: "fcc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fcc7"
  type: "Convolution"
  bottom: "fcc6"
  top: "fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fcc7"
  top: "fcc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fcc7"
  top: "fcc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score-fr"
  type: "Convolution"
  bottom: "fcc7"
  top: "score-fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    engine: CAFFE
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "score-fcc7"
  top: "bigscore"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 63
    group: 16
    stride: 32
  }
}
layer {
  name: "crop"
  type: "Crop"
  bottom: "bigscore"
  bottom: "data"
  top: "score"
}
layer {
  name: "balance"
  type: "Balance"
  bottom: "score"
  bottom: "jointMap"
  top: "bscore"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "bscore"
  bottom: "jointMap"
  top: "loss"
}
I0422 14:30:37.755338 23268 layer_factory.hpp:77] Creating layer data
I0422 14:30:37.755357 23268 net.cpp:91] Creating Layer data
I0422 14:30:37.755363 23268 net.cpp:399] data -> data
I0422 14:30:37.755373 23268 net.cpp:399] data -> jointLocation
I0422 14:30:37.755385 23268 image_data_layer.cpp:40] Opening file /home/mengxin1/HumanPoseDetect/preprocess/val_label_resized.txt
I0422 14:30:37.781332 23268 image_data_layer.cpp:62] A total of 2424 images.
I0422 14:30:37.787521 23268 image_data_layer.cpp:89] output data size: 10,3,512,512
I0422 14:30:37.873746 23268 net.cpp:141] Setting up data
I0422 14:30:37.873801 23268 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 14:30:37.873810 23268 net.cpp:148] Top shape: 10 48 1 1 (480)
I0422 14:30:37.873813 23268 net.cpp:156] Memory required for data: 31459200
I0422 14:30:37.873823 23268 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 14:30:37.873843 23268 net.cpp:91] Creating Layer data_data_0_split
I0422 14:30:37.873849 23268 net.cpp:425] data_data_0_split <- data
I0422 14:30:37.873860 23268 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 14:30:37.873877 23268 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 14:30:37.873883 23268 net.cpp:399] data_data_0_split -> data_data_0_split_2
I0422 14:30:37.874074 23268 net.cpp:141] Setting up data_data_0_split
I0422 14:30:37.874085 23268 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 14:30:37.874090 23268 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 14:30:37.874095 23268 net.cpp:148] Top shape: 10 3 512 512 (7864320)
I0422 14:30:37.874099 23268 net.cpp:156] Memory required for data: 125831040
I0422 14:30:37.874104 23268 layer_factory.hpp:77] Creating layer jointMap
I0422 14:30:37.874114 23268 net.cpp:91] Creating Layer jointMap
I0422 14:30:37.874119 23268 net.cpp:425] jointMap <- data_data_0_split_0
I0422 14:30:37.874125 23268 net.cpp:425] jointMap <- jointLocation
I0422 14:30:37.874131 23268 net.cpp:399] jointMap -> jointMap
I0422 14:30:37.874176 23268 net.cpp:141] Setting up jointMap
I0422 14:30:37.874184 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:37.874188 23268 net.cpp:156] Memory required for data: 293603200
I0422 14:30:37.874193 23268 layer_factory.hpp:77] Creating layer jointMap_jointMap_0_split
I0422 14:30:37.874200 23268 net.cpp:91] Creating Layer jointMap_jointMap_0_split
I0422 14:30:37.874205 23268 net.cpp:425] jointMap_jointMap_0_split <- jointMap
I0422 14:30:37.874217 23268 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_0
I0422 14:30:37.874227 23268 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_1
I0422 14:30:37.874285 23268 net.cpp:141] Setting up jointMap_jointMap_0_split
I0422 14:30:37.874294 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:37.874300 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:37.874303 23268 net.cpp:156] Memory required for data: 629147520
I0422 14:30:37.874308 23268 layer_factory.hpp:77] Creating layer conv1
I0422 14:30:37.874327 23268 net.cpp:91] Creating Layer conv1
I0422 14:30:37.874333 23268 net.cpp:425] conv1 <- data_data_0_split_1
I0422 14:30:37.874341 23268 net.cpp:399] conv1 -> conv1
I0422 14:30:37.880084 23268 net.cpp:141] Setting up conv1
I0422 14:30:37.880100 23268 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 14:30:37.880105 23268 net.cpp:156] Memory required for data: 748095360
I0422 14:30:37.880118 23268 layer_factory.hpp:77] Creating layer relu1
I0422 14:30:37.880127 23268 net.cpp:91] Creating Layer relu1
I0422 14:30:37.880132 23268 net.cpp:425] relu1 <- conv1
I0422 14:30:37.880139 23268 net.cpp:386] relu1 -> conv1 (in-place)
I0422 14:30:37.883707 23268 net.cpp:141] Setting up relu1
I0422 14:30:37.883719 23268 net.cpp:148] Top shape: 10 96 176 176 (29736960)
I0422 14:30:37.883723 23268 net.cpp:156] Memory required for data: 867043200
I0422 14:30:37.883728 23268 layer_factory.hpp:77] Creating layer pool1
I0422 14:30:37.883738 23268 net.cpp:91] Creating Layer pool1
I0422 14:30:37.883744 23268 net.cpp:425] pool1 <- conv1
I0422 14:30:37.883750 23268 net.cpp:399] pool1 -> pool1
I0422 14:30:37.883824 23268 net.cpp:141] Setting up pool1
I0422 14:30:37.883834 23268 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 14:30:37.883838 23268 net.cpp:156] Memory required for data: 896780160
I0422 14:30:37.883843 23268 layer_factory.hpp:77] Creating layer norm1
I0422 14:30:37.883853 23268 net.cpp:91] Creating Layer norm1
I0422 14:30:37.883858 23268 net.cpp:425] norm1 <- pool1
I0422 14:30:37.883865 23268 net.cpp:399] norm1 -> norm1
I0422 14:30:37.885918 23268 net.cpp:141] Setting up norm1
I0422 14:30:37.885931 23268 net.cpp:148] Top shape: 10 96 88 88 (7434240)
I0422 14:30:37.885934 23268 net.cpp:156] Memory required for data: 926517120
I0422 14:30:37.885939 23268 layer_factory.hpp:77] Creating layer conv2
I0422 14:30:37.885953 23268 net.cpp:91] Creating Layer conv2
I0422 14:30:37.885958 23268 net.cpp:425] conv2 <- norm1
I0422 14:30:37.885967 23268 net.cpp:399] conv2 -> conv2
I0422 14:30:37.909258 23268 net.cpp:141] Setting up conv2
I0422 14:30:37.909276 23268 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 14:30:37.909281 23268 net.cpp:156] Memory required for data: 1005815680
I0422 14:30:37.909293 23268 layer_factory.hpp:77] Creating layer relu2
I0422 14:30:37.909302 23268 net.cpp:91] Creating Layer relu2
I0422 14:30:37.909307 23268 net.cpp:425] relu2 <- conv2
I0422 14:30:37.909314 23268 net.cpp:386] relu2 -> conv2 (in-place)
I0422 14:30:37.928166 23268 net.cpp:141] Setting up relu2
I0422 14:30:37.928180 23268 net.cpp:148] Top shape: 10 256 88 88 (19824640)
I0422 14:30:37.928184 23268 net.cpp:156] Memory required for data: 1085114240
I0422 14:30:37.928189 23268 layer_factory.hpp:77] Creating layer pool2
I0422 14:30:37.928199 23268 net.cpp:91] Creating Layer pool2
I0422 14:30:37.928202 23268 net.cpp:425] pool2 <- conv2
I0422 14:30:37.928210 23268 net.cpp:399] pool2 -> pool2
I0422 14:30:37.928282 23268 net.cpp:141] Setting up pool2
I0422 14:30:37.928290 23268 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 14:30:37.928294 23268 net.cpp:156] Memory required for data: 1104938880
I0422 14:30:37.928299 23268 layer_factory.hpp:77] Creating layer norm2
I0422 14:30:37.928308 23268 net.cpp:91] Creating Layer norm2
I0422 14:30:37.928313 23268 net.cpp:425] norm2 <- pool2
I0422 14:30:37.928319 23268 net.cpp:399] norm2 -> norm2
I0422 14:30:37.930629 23268 net.cpp:141] Setting up norm2
I0422 14:30:37.930641 23268 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 14:30:37.930654 23268 net.cpp:156] Memory required for data: 1124763520
I0422 14:30:37.930660 23268 layer_factory.hpp:77] Creating layer conv3
I0422 14:30:37.930671 23268 net.cpp:91] Creating Layer conv3
I0422 14:30:37.930676 23268 net.cpp:425] conv3 <- norm2
I0422 14:30:37.930685 23268 net.cpp:399] conv3 -> conv3
I0422 14:30:37.950161 23268 net.cpp:141] Setting up conv3
I0422 14:30:37.950184 23268 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 14:30:37.950189 23268 net.cpp:156] Memory required for data: 1154500480
I0422 14:30:37.950201 23268 layer_factory.hpp:77] Creating layer relu3
I0422 14:30:37.950212 23268 net.cpp:91] Creating Layer relu3
I0422 14:30:37.950217 23268 net.cpp:425] relu3 <- conv3
I0422 14:30:37.950224 23268 net.cpp:386] relu3 -> conv3 (in-place)
I0422 14:30:37.950404 23268 net.cpp:141] Setting up relu3
I0422 14:30:37.950415 23268 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 14:30:37.950419 23268 net.cpp:156] Memory required for data: 1184237440
I0422 14:30:37.950423 23268 layer_factory.hpp:77] Creating layer cconv4
I0422 14:30:37.950436 23268 net.cpp:91] Creating Layer cconv4
I0422 14:30:37.950440 23268 net.cpp:425] cconv4 <- conv3
I0422 14:30:37.950449 23268 net.cpp:399] cconv4 -> conv4
I0422 14:30:37.971168 23268 net.cpp:141] Setting up cconv4
I0422 14:30:37.971197 23268 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 14:30:37.971202 23268 net.cpp:156] Memory required for data: 1213974400
I0422 14:30:37.971211 23268 layer_factory.hpp:77] Creating layer relu4
I0422 14:30:37.971221 23268 net.cpp:91] Creating Layer relu4
I0422 14:30:37.971227 23268 net.cpp:425] relu4 <- conv4
I0422 14:30:37.971235 23268 net.cpp:386] relu4 -> conv4 (in-place)
I0422 14:30:37.973979 23268 net.cpp:141] Setting up relu4
I0422 14:30:37.973991 23268 net.cpp:148] Top shape: 10 384 44 44 (7434240)
I0422 14:30:37.973995 23268 net.cpp:156] Memory required for data: 1243711360
I0422 14:30:37.974000 23268 layer_factory.hpp:77] Creating layer cconv5
I0422 14:30:37.974020 23268 net.cpp:91] Creating Layer cconv5
I0422 14:30:37.974025 23268 net.cpp:425] cconv5 <- conv4
I0422 14:30:37.974033 23268 net.cpp:399] cconv5 -> conv5
I0422 14:30:37.982606 23268 net.cpp:141] Setting up cconv5
I0422 14:30:37.982621 23268 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 14:30:37.982626 23268 net.cpp:156] Memory required for data: 1263536000
I0422 14:30:37.982638 23268 layer_factory.hpp:77] Creating layer relu5
I0422 14:30:37.982650 23268 net.cpp:91] Creating Layer relu5
I0422 14:30:37.982659 23268 net.cpp:425] relu5 <- conv5
I0422 14:30:37.982666 23268 net.cpp:386] relu5 -> conv5 (in-place)
I0422 14:30:37.983577 23268 net.cpp:141] Setting up relu5
I0422 14:30:37.983588 23268 net.cpp:148] Top shape: 10 256 44 44 (4956160)
I0422 14:30:37.983592 23268 net.cpp:156] Memory required for data: 1283360640
I0422 14:30:37.983597 23268 layer_factory.hpp:77] Creating layer pool5
I0422 14:30:37.983608 23268 net.cpp:91] Creating Layer pool5
I0422 14:30:37.983613 23268 net.cpp:425] pool5 <- conv5
I0422 14:30:37.983619 23268 net.cpp:399] pool5 -> pool5
I0422 14:30:37.983703 23268 net.cpp:141] Setting up pool5
I0422 14:30:37.983713 23268 net.cpp:148] Top shape: 10 256 22 22 (1239040)
I0422 14:30:37.983717 23268 net.cpp:156] Memory required for data: 1288316800
I0422 14:30:37.983722 23268 layer_factory.hpp:77] Creating layer fcc6
I0422 14:30:37.983734 23268 net.cpp:91] Creating Layer fcc6
I0422 14:30:37.983741 23268 net.cpp:425] fcc6 <- pool5
I0422 14:30:37.983749 23268 net.cpp:399] fcc6 -> fcc6
I0422 14:30:38.479965 23268 net.cpp:141] Setting up fcc6
I0422 14:30:38.480015 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:38.480020 23268 net.cpp:156] Memory required for data: 1335666560
I0422 14:30:38.480032 23268 layer_factory.hpp:77] Creating layer relu6
I0422 14:30:38.480051 23268 net.cpp:91] Creating Layer relu6
I0422 14:30:38.480057 23268 net.cpp:425] relu6 <- fcc6
I0422 14:30:38.480067 23268 net.cpp:386] relu6 -> fcc6 (in-place)
I0422 14:30:38.481454 23268 net.cpp:141] Setting up relu6
I0422 14:30:38.481473 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:38.481478 23268 net.cpp:156] Memory required for data: 1383016320
I0422 14:30:38.481482 23268 layer_factory.hpp:77] Creating layer drop6
I0422 14:30:38.481492 23268 net.cpp:91] Creating Layer drop6
I0422 14:30:38.481499 23268 net.cpp:425] drop6 <- fcc6
I0422 14:30:38.481506 23268 net.cpp:386] drop6 -> fcc6 (in-place)
I0422 14:30:38.481555 23268 net.cpp:141] Setting up drop6
I0422 14:30:38.481564 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:38.481569 23268 net.cpp:156] Memory required for data: 1430366080
I0422 14:30:38.481573 23268 layer_factory.hpp:77] Creating layer fcc7
I0422 14:30:38.481590 23268 net.cpp:91] Creating Layer fcc7
I0422 14:30:38.481595 23268 net.cpp:425] fcc7 <- fcc6
I0422 14:30:38.481605 23268 net.cpp:399] fcc7 -> fcc7
I0422 14:30:38.729296 23268 net.cpp:141] Setting up fcc7
I0422 14:30:38.729351 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:38.729357 23268 net.cpp:156] Memory required for data: 1477715840
I0422 14:30:38.729372 23268 layer_factory.hpp:77] Creating layer relu7
I0422 14:30:38.729387 23268 net.cpp:91] Creating Layer relu7
I0422 14:30:38.729393 23268 net.cpp:425] relu7 <- fcc7
I0422 14:30:38.729406 23268 net.cpp:386] relu7 -> fcc7 (in-place)
I0422 14:30:38.731679 23268 net.cpp:141] Setting up relu7
I0422 14:30:38.731698 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:38.731701 23268 net.cpp:156] Memory required for data: 1525065600
I0422 14:30:38.731709 23268 layer_factory.hpp:77] Creating layer drop7
I0422 14:30:38.731719 23268 net.cpp:91] Creating Layer drop7
I0422 14:30:38.731724 23268 net.cpp:425] drop7 <- fcc7
I0422 14:30:38.731734 23268 net.cpp:386] drop7 -> fcc7 (in-place)
I0422 14:30:38.731794 23268 net.cpp:141] Setting up drop7
I0422 14:30:38.731804 23268 net.cpp:148] Top shape: 10 4096 17 17 (11837440)
I0422 14:30:38.731808 23268 net.cpp:156] Memory required for data: 1572415360
I0422 14:30:38.731812 23268 layer_factory.hpp:77] Creating layer score-fr
I0422 14:30:38.731832 23268 net.cpp:91] Creating Layer score-fr
I0422 14:30:38.731837 23268 net.cpp:425] score-fr <- fcc7
I0422 14:30:38.731847 23268 net.cpp:399] score-fr -> score-fcc7
I0422 14:30:38.734383 23268 net.cpp:141] Setting up score-fr
I0422 14:30:38.734421 23268 net.cpp:148] Top shape: 10 16 17 17 (46240)
I0422 14:30:38.734426 23268 net.cpp:156] Memory required for data: 1572600320
I0422 14:30:38.734437 23268 layer_factory.hpp:77] Creating layer upsample
I0422 14:30:38.734457 23268 net.cpp:91] Creating Layer upsample
I0422 14:30:38.734462 23268 net.cpp:425] upsample <- score-fcc7
I0422 14:30:38.734473 23268 net.cpp:399] upsample -> bigscore
I0422 14:30:38.736615 23268 net.cpp:141] Setting up upsample
I0422 14:30:38.736641 23268 net.cpp:148] Top shape: 10 16 575 575 (52900000)
I0422 14:30:38.736645 23268 net.cpp:156] Memory required for data: 1784200320
I0422 14:30:38.736670 23268 layer_factory.hpp:77] Creating layer crop
I0422 14:30:38.736681 23268 net.cpp:91] Creating Layer crop
I0422 14:30:38.736686 23268 net.cpp:425] crop <- bigscore
I0422 14:30:38.736693 23268 net.cpp:425] crop <- data_data_0_split_2
I0422 14:30:38.736702 23268 net.cpp:399] crop -> score
I0422 14:30:38.736764 23268 net.cpp:141] Setting up crop
I0422 14:30:38.736774 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:38.736778 23268 net.cpp:156] Memory required for data: 1951972480
I0422 14:30:38.736783 23268 layer_factory.hpp:77] Creating layer balance
I0422 14:30:38.736791 23268 net.cpp:91] Creating Layer balance
I0422 14:30:38.736795 23268 net.cpp:425] balance <- score
I0422 14:30:38.736801 23268 net.cpp:425] balance <- jointMap_jointMap_0_split_0
I0422 14:30:38.736809 23268 net.cpp:399] balance -> bscore
I0422 14:30:38.736886 23268 net.cpp:141] Setting up balance
I0422 14:30:38.736896 23268 net.cpp:148] Top shape: 10 16 512 512 (41943040)
I0422 14:30:38.736899 23268 net.cpp:156] Memory required for data: 2119744640
I0422 14:30:38.736904 23268 layer_factory.hpp:77] Creating layer loss
I0422 14:30:38.736917 23268 net.cpp:91] Creating Layer loss
I0422 14:30:38.736922 23268 net.cpp:425] loss <- bscore
I0422 14:30:38.736927 23268 net.cpp:425] loss <- jointMap_jointMap_0_split_1
I0422 14:30:38.736937 23268 net.cpp:399] loss -> loss
I0422 14:30:38.737005 23268 net.cpp:141] Setting up loss
I0422 14:30:38.737017 23268 net.cpp:148] Top shape: (1)
I0422 14:30:38.737021 23268 net.cpp:151]     with loss weight 1
I0422 14:30:38.737035 23268 net.cpp:156] Memory required for data: 2119744644
I0422 14:30:38.737040 23268 net.cpp:217] loss needs backward computation.
I0422 14:30:38.737046 23268 net.cpp:217] balance needs backward computation.
I0422 14:30:38.737051 23268 net.cpp:217] crop needs backward computation.
I0422 14:30:38.737054 23268 net.cpp:217] upsample needs backward computation.
I0422 14:30:38.737059 23268 net.cpp:217] score-fr needs backward computation.
I0422 14:30:38.737063 23268 net.cpp:217] drop7 needs backward computation.
I0422 14:30:38.737067 23268 net.cpp:217] relu7 needs backward computation.
I0422 14:30:38.737071 23268 net.cpp:217] fcc7 needs backward computation.
I0422 14:30:38.737076 23268 net.cpp:217] drop6 needs backward computation.
I0422 14:30:38.737079 23268 net.cpp:217] relu6 needs backward computation.
I0422 14:30:38.737083 23268 net.cpp:217] fcc6 needs backward computation.
I0422 14:30:38.737088 23268 net.cpp:217] pool5 needs backward computation.
I0422 14:30:38.737092 23268 net.cpp:217] relu5 needs backward computation.
I0422 14:30:38.737097 23268 net.cpp:217] cconv5 needs backward computation.
I0422 14:30:38.737100 23268 net.cpp:217] relu4 needs backward computation.
I0422 14:30:38.737104 23268 net.cpp:217] cconv4 needs backward computation.
I0422 14:30:38.737108 23268 net.cpp:217] relu3 needs backward computation.
I0422 14:30:38.737112 23268 net.cpp:217] conv3 needs backward computation.
I0422 14:30:38.737117 23268 net.cpp:217] norm2 needs backward computation.
I0422 14:30:38.737120 23268 net.cpp:217] pool2 needs backward computation.
I0422 14:30:38.737124 23268 net.cpp:217] relu2 needs backward computation.
I0422 14:30:38.737128 23268 net.cpp:217] conv2 needs backward computation.
I0422 14:30:38.737133 23268 net.cpp:217] norm1 needs backward computation.
I0422 14:30:38.737136 23268 net.cpp:217] pool1 needs backward computation.
I0422 14:30:38.737140 23268 net.cpp:217] relu1 needs backward computation.
I0422 14:30:38.737144 23268 net.cpp:217] conv1 needs backward computation.
I0422 14:30:38.737149 23268 net.cpp:219] jointMap_jointMap_0_split does not need backward computation.
I0422 14:30:38.737154 23268 net.cpp:219] jointMap does not need backward computation.
I0422 14:30:38.737160 23268 net.cpp:219] data_data_0_split does not need backward computation.
I0422 14:30:38.737165 23268 net.cpp:219] data does not need backward computation.
I0422 14:30:38.737169 23268 net.cpp:261] This network produces output loss
I0422 14:30:38.737192 23268 net.cpp:274] Network initialization done.
I0422 14:30:38.737292 23268 solver.cpp:60] Solver scaffolding done.
I0422 14:30:40.053717 23268 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: /home/mengxin1/HumanPoseDetect/fcn_joint_predict/bvlc_reference_caffenet.caffemodel
I0422 14:30:40.053764 23268 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.
W0422 14:30:40.053769 23268 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0422 14:30:40.053773 23268 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/mengxin1/HumanPoseDetect/fcn_joint_predict/bvlc_reference_caffenet.caffemodel
I0422 14:30:43.064291 23268 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I0422 14:30:43.066936 23268 net.cpp:752] Ignoring source layer conv4
I0422 14:30:43.066975 23268 net.cpp:752] Ignoring source layer conv5
I0422 14:30:43.066980 23268 net.cpp:752] Ignoring source layer fc6
I0422 14:30:43.066995 23268 net.cpp:752] Ignoring source layer fc7
I0422 14:30:43.067000 23268 net.cpp:752] Ignoring source layer fc8
I0422 14:30:43.076931 23268 solver.cpp:337] Iteration 0, Testing net (#0)
I0422 14:31:37.188599 23268 solver.cpp:404]     Test net output #0: loss = 8.57076 (* 1 = 8.57076 loss)
I0422 14:31:39.948642 23268 solver.cpp:228] Iteration 0, loss = 9.66251
I0422 14:31:39.948695 23268 solver.cpp:244]     Train net output #0: loss = 9.66251 (* 1 = 9.66251 loss)
I0422 14:31:39.948709 23268 sgd_solver.cpp:106] Iteration 0, lr = 0.0008
I0422 14:33:25.991255 23268 solver.cpp:228] Iteration 20, loss = 9.81943
I0422 14:33:25.991690 23268 solver.cpp:244]     Train net output #0: loss = 9.81943 (* 1 = 9.81943 loss)
I0422 14:33:25.991719 23268 sgd_solver.cpp:106] Iteration 20, lr = 0.0008
I0422 14:35:12.245802 23268 solver.cpp:228] Iteration 40, loss = 6.6359
I0422 14:35:12.246301 23268 solver.cpp:244]     Train net output #0: loss = 6.6359 (* 1 = 6.6359 loss)
I0422 14:35:12.246318 23268 sgd_solver.cpp:106] Iteration 40, lr = 0.0008
I0422 14:36:58.470087 23268 solver.cpp:228] Iteration 60, loss = 6.58144
I0422 14:36:58.470613 23268 solver.cpp:244]     Train net output #0: loss = 6.58144 (* 1 = 6.58144 loss)
I0422 14:36:58.470633 23268 sgd_solver.cpp:106] Iteration 60, lr = 0.0008
I0422 14:38:46.504243 23268 solver.cpp:228] Iteration 80, loss = 5.77171
I0422 14:38:46.504771 23268 solver.cpp:244]     Train net output #0: loss = 5.77171 (* 1 = 5.77171 loss)
I0422 14:38:46.504789 23268 sgd_solver.cpp:106] Iteration 80, lr = 0.0008
I0422 14:40:33.029834 23268 solver.cpp:228] Iteration 100, loss = 5.88421
I0422 14:40:33.030268 23268 solver.cpp:244]     Train net output #0: loss = 5.88421 (* 1 = 5.88421 loss)
I0422 14:40:33.030282 23268 sgd_solver.cpp:106] Iteration 100, lr = 0.0008
I0422 14:42:19.606483 23268 solver.cpp:228] Iteration 120, loss = 6.18998
I0422 14:42:19.607007 23268 solver.cpp:244]     Train net output #0: loss = 6.18998 (* 1 = 6.18998 loss)
I0422 14:42:19.607023 23268 sgd_solver.cpp:106] Iteration 120, lr = 0.0008
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/mengxin1/caffe/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0422 14:46:10.753547 32015 solver.cpp:48] Initializing solver from parameters: 
test_iter: 25
test_interval: 1000
base_lr: 0.0008
display: 20
max_iter: 100000
lr_policy: "step"
gamma: 0.3
momentum: 0.9
weight_decay: 0.0005
stepsize: 20000
snapshot: 10000
snapshot_prefix: "models/model"
net: "fcn_alxnet_train_val.prototxt"
I0422 14:46:10.767086 32015 solver.cpp:91] Creating training net from net file: fcn_alxnet_train_val.prototxt
I0422 14:46:10.768409 32015 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0422 14:46:10.768913 32015 net.cpp:49] Initializing net from parameters: 
name: "FCN-AlexNet-joint"
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "jointLocation"
  include {
    phase: TRAIN
  }
  transform_param {
    mean_value: 104
    mean_value: 116
    mean_value: 122
  }
  image_data_param {
    source: "/home/mengxin1/HumanPoseDetect/preprocess/train_label_resized.txt"
    batch_size: 15
    shuffle: true
    root_folder: "/home/mengxin1/mpii_human_pose_v1_images_resized/"
    label_num: 48
  }
}
layer {
  name: "jointMap"
  type: "PositionToMap"
  bottom: "data"
  bottom: "jointLocation"
  top: "jointMap"
  gaussian_param {
    std: 25
    half_window_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 100
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "cconv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "cconv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fcc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fcc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 6
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fcc6"
  top: "fcc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fcc6"
  top: "fcc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fcc7"
  type: "Convolution"
  bottom: "fcc6"
  top: "fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fcc7"
  top: "fcc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fcc7"
  top: "fcc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score-fr"
  type: "Convolution"
  bottom: "fcc7"
  top: "score-fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    engine: CAFFE
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "score-fcc7"
  top: "bigscore"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 63
    group: 16
    stride: 32
  }
}
layer {
  name: "crop"
  type: "Crop"
  bottom: "bigscore"
  bottom: "data"
  top: "score"
}
layer {
  name: "balance"
  type: "Balance"
  bottom: "score"
  bottom: "jointMap"
  top: "bscore"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "bscore"
  bottom: "jointMap"
  top: "loss"
}
I0422 14:46:10.769145 32015 layer_factory.hpp:77] Creating layer data
I0422 14:46:10.769183 32015 net.cpp:91] Creating Layer data
I0422 14:46:10.769197 32015 net.cpp:399] data -> data
I0422 14:46:10.769219 32015 net.cpp:399] data -> jointLocation
I0422 14:46:10.769242 32015 image_data_layer.cpp:40] Opening file /home/mengxin1/HumanPoseDetect/preprocess/train_label_resized.txt
I0422 14:46:10.877128 32015 image_data_layer.cpp:57] Shuffling data
I0422 14:46:10.880591 32015 image_data_layer.cpp:62] A total of 9580 images.
I0422 14:46:10.996608 32015 image_data_layer.cpp:89] output data size: 15,3,512,512
I0422 14:46:11.126932 32015 net.cpp:141] Setting up data
I0422 14:46:11.126994 32015 net.cpp:148] Top shape: 15 3 512 512 (11796480)
I0422 14:46:11.127003 32015 net.cpp:148] Top shape: 15 48 1 1 (720)
I0422 14:46:11.127007 32015 net.cpp:156] Memory required for data: 47188800
I0422 14:46:11.127017 32015 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 14:46:11.127034 32015 net.cpp:91] Creating Layer data_data_0_split
I0422 14:46:11.127040 32015 net.cpp:425] data_data_0_split <- data
I0422 14:46:11.127050 32015 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 14:46:11.127065 32015 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 14:46:11.127073 32015 net.cpp:399] data_data_0_split -> data_data_0_split_2
I0422 14:46:11.127285 32015 net.cpp:141] Setting up data_data_0_split
I0422 14:46:11.127296 32015 net.cpp:148] Top shape: 15 3 512 512 (11796480)
I0422 14:46:11.127302 32015 net.cpp:148] Top shape: 15 3 512 512 (11796480)
I0422 14:46:11.127307 32015 net.cpp:148] Top shape: 15 3 512 512 (11796480)
I0422 14:46:11.127311 32015 net.cpp:156] Memory required for data: 188746560
I0422 14:46:11.127315 32015 layer_factory.hpp:77] Creating layer jointMap
I0422 14:46:11.127331 32015 net.cpp:91] Creating Layer jointMap
I0422 14:46:11.127336 32015 net.cpp:425] jointMap <- data_data_0_split_0
I0422 14:46:11.127341 32015 net.cpp:425] jointMap <- jointLocation
I0422 14:46:11.127347 32015 net.cpp:399] jointMap -> jointMap
I0422 14:46:11.127382 32015 net.cpp:141] Setting up jointMap
I0422 14:46:11.127391 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:11.127395 32015 net.cpp:156] Memory required for data: 440404800
I0422 14:46:11.127399 32015 layer_factory.hpp:77] Creating layer jointMap_jointMap_0_split
I0422 14:46:11.127406 32015 net.cpp:91] Creating Layer jointMap_jointMap_0_split
I0422 14:46:11.127410 32015 net.cpp:425] jointMap_jointMap_0_split <- jointMap
I0422 14:46:11.127418 32015 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_0
I0422 14:46:11.127424 32015 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_1
I0422 14:46:11.127473 32015 net.cpp:141] Setting up jointMap_jointMap_0_split
I0422 14:46:11.127482 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:11.127487 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:11.127491 32015 net.cpp:156] Memory required for data: 943721280
I0422 14:46:11.127502 32015 layer_factory.hpp:77] Creating layer conv1
I0422 14:46:11.127521 32015 net.cpp:91] Creating Layer conv1
I0422 14:46:11.127526 32015 net.cpp:425] conv1 <- data_data_0_split_1
I0422 14:46:11.127533 32015 net.cpp:399] conv1 -> conv1
I0422 14:46:11.600947 32015 net.cpp:141] Setting up conv1
I0422 14:46:11.600985 32015 net.cpp:148] Top shape: 15 96 176 176 (44605440)
I0422 14:46:11.600989 32015 net.cpp:156] Memory required for data: 1122143040
I0422 14:46:11.601008 32015 layer_factory.hpp:77] Creating layer relu1
I0422 14:46:11.601027 32015 net.cpp:91] Creating Layer relu1
I0422 14:46:11.601032 32015 net.cpp:425] relu1 <- conv1
I0422 14:46:11.601040 32015 net.cpp:386] relu1 -> conv1 (in-place)
I0422 14:46:11.631469 32015 net.cpp:141] Setting up relu1
I0422 14:46:11.631508 32015 net.cpp:148] Top shape: 15 96 176 176 (44605440)
I0422 14:46:11.631515 32015 net.cpp:156] Memory required for data: 1300564800
I0422 14:46:11.631522 32015 layer_factory.hpp:77] Creating layer pool1
I0422 14:46:11.631544 32015 net.cpp:91] Creating Layer pool1
I0422 14:46:11.631551 32015 net.cpp:425] pool1 <- conv1
I0422 14:46:11.631562 32015 net.cpp:399] pool1 -> pool1
I0422 14:46:11.631690 32015 net.cpp:141] Setting up pool1
I0422 14:46:11.631701 32015 net.cpp:148] Top shape: 15 96 88 88 (11151360)
I0422 14:46:11.631705 32015 net.cpp:156] Memory required for data: 1345170240
I0422 14:46:11.631710 32015 layer_factory.hpp:77] Creating layer norm1
I0422 14:46:11.631723 32015 net.cpp:91] Creating Layer norm1
I0422 14:46:11.631728 32015 net.cpp:425] norm1 <- pool1
I0422 14:46:11.631734 32015 net.cpp:399] norm1 -> norm1
I0422 14:46:11.632311 32015 net.cpp:141] Setting up norm1
I0422 14:46:11.632326 32015 net.cpp:148] Top shape: 15 96 88 88 (11151360)
I0422 14:46:11.632331 32015 net.cpp:156] Memory required for data: 1389775680
I0422 14:46:11.632336 32015 layer_factory.hpp:77] Creating layer conv2
I0422 14:46:11.632357 32015 net.cpp:91] Creating Layer conv2
I0422 14:46:11.632362 32015 net.cpp:425] conv2 <- norm1
I0422 14:46:11.632370 32015 net.cpp:399] conv2 -> conv2
I0422 14:46:11.639494 32015 net.cpp:141] Setting up conv2
I0422 14:46:11.639510 32015 net.cpp:148] Top shape: 15 256 88 88 (29736960)
I0422 14:46:11.639515 32015 net.cpp:156] Memory required for data: 1508723520
I0422 14:46:11.639528 32015 layer_factory.hpp:77] Creating layer relu2
I0422 14:46:11.639538 32015 net.cpp:91] Creating Layer relu2
I0422 14:46:11.639542 32015 net.cpp:425] relu2 <- conv2
I0422 14:46:11.639554 32015 net.cpp:386] relu2 -> conv2 (in-place)
I0422 14:46:11.639900 32015 net.cpp:141] Setting up relu2
I0422 14:46:11.639912 32015 net.cpp:148] Top shape: 15 256 88 88 (29736960)
I0422 14:46:11.639917 32015 net.cpp:156] Memory required for data: 1627671360
I0422 14:46:11.639922 32015 layer_factory.hpp:77] Creating layer pool2
I0422 14:46:11.639932 32015 net.cpp:91] Creating Layer pool2
I0422 14:46:11.639937 32015 net.cpp:425] pool2 <- conv2
I0422 14:46:11.639945 32015 net.cpp:399] pool2 -> pool2
I0422 14:46:11.640012 32015 net.cpp:141] Setting up pool2
I0422 14:46:11.640022 32015 net.cpp:148] Top shape: 15 256 44 44 (7434240)
I0422 14:46:11.640027 32015 net.cpp:156] Memory required for data: 1657408320
I0422 14:46:11.640030 32015 layer_factory.hpp:77] Creating layer norm2
I0422 14:46:11.640040 32015 net.cpp:91] Creating Layer norm2
I0422 14:46:11.640045 32015 net.cpp:425] norm2 <- pool2
I0422 14:46:11.640053 32015 net.cpp:399] norm2 -> norm2
I0422 14:46:11.640270 32015 net.cpp:141] Setting up norm2
I0422 14:46:11.640281 32015 net.cpp:148] Top shape: 15 256 44 44 (7434240)
I0422 14:46:11.640285 32015 net.cpp:156] Memory required for data: 1687145280
I0422 14:46:11.640290 32015 layer_factory.hpp:77] Creating layer conv3
I0422 14:46:11.640303 32015 net.cpp:91] Creating Layer conv3
I0422 14:46:11.640308 32015 net.cpp:425] conv3 <- norm2
I0422 14:46:11.640318 32015 net.cpp:399] conv3 -> conv3
I0422 14:46:11.654187 32015 net.cpp:141] Setting up conv3
I0422 14:46:11.654224 32015 net.cpp:148] Top shape: 15 384 44 44 (11151360)
I0422 14:46:11.654235 32015 net.cpp:156] Memory required for data: 1731750720
I0422 14:46:11.654254 32015 layer_factory.hpp:77] Creating layer relu3
I0422 14:46:11.654268 32015 net.cpp:91] Creating Layer relu3
I0422 14:46:11.654275 32015 net.cpp:425] relu3 <- conv3
I0422 14:46:11.654284 32015 net.cpp:386] relu3 -> conv3 (in-place)
I0422 14:46:11.654631 32015 net.cpp:141] Setting up relu3
I0422 14:46:11.654646 32015 net.cpp:148] Top shape: 15 384 44 44 (11151360)
I0422 14:46:11.654651 32015 net.cpp:156] Memory required for data: 1776356160
I0422 14:46:11.654656 32015 layer_factory.hpp:77] Creating layer cconv4
I0422 14:46:11.654673 32015 net.cpp:91] Creating Layer cconv4
I0422 14:46:11.654678 32015 net.cpp:425] cconv4 <- conv3
I0422 14:46:11.654686 32015 net.cpp:399] cconv4 -> conv4
I0422 14:46:11.665719 32015 net.cpp:141] Setting up cconv4
I0422 14:46:11.665748 32015 net.cpp:148] Top shape: 15 384 44 44 (11151360)
I0422 14:46:11.665752 32015 net.cpp:156] Memory required for data: 1820961600
I0422 14:46:11.665763 32015 layer_factory.hpp:77] Creating layer relu4
I0422 14:46:11.665777 32015 net.cpp:91] Creating Layer relu4
I0422 14:46:11.665783 32015 net.cpp:425] relu4 <- conv4
I0422 14:46:11.665791 32015 net.cpp:386] relu4 -> conv4 (in-place)
I0422 14:46:11.666146 32015 net.cpp:141] Setting up relu4
I0422 14:46:11.666159 32015 net.cpp:148] Top shape: 15 384 44 44 (11151360)
I0422 14:46:11.666163 32015 net.cpp:156] Memory required for data: 1865567040
I0422 14:46:11.666168 32015 layer_factory.hpp:77] Creating layer cconv5
I0422 14:46:11.666191 32015 net.cpp:91] Creating Layer cconv5
I0422 14:46:11.666199 32015 net.cpp:425] cconv5 <- conv4
I0422 14:46:11.666208 32015 net.cpp:399] cconv5 -> conv5
I0422 14:46:11.674373 32015 net.cpp:141] Setting up cconv5
I0422 14:46:11.674388 32015 net.cpp:148] Top shape: 15 256 44 44 (7434240)
I0422 14:46:11.674393 32015 net.cpp:156] Memory required for data: 1895304000
I0422 14:46:11.674406 32015 layer_factory.hpp:77] Creating layer relu5
I0422 14:46:11.674417 32015 net.cpp:91] Creating Layer relu5
I0422 14:46:11.674422 32015 net.cpp:425] relu5 <- conv5
I0422 14:46:11.674427 32015 net.cpp:386] relu5 -> conv5 (in-place)
I0422 14:46:11.674764 32015 net.cpp:141] Setting up relu5
I0422 14:46:11.674777 32015 net.cpp:148] Top shape: 15 256 44 44 (7434240)
I0422 14:46:11.674782 32015 net.cpp:156] Memory required for data: 1925040960
I0422 14:46:11.674785 32015 layer_factory.hpp:77] Creating layer pool5
I0422 14:46:11.674798 32015 net.cpp:91] Creating Layer pool5
I0422 14:46:11.674803 32015 net.cpp:425] pool5 <- conv5
I0422 14:46:11.674809 32015 net.cpp:399] pool5 -> pool5
I0422 14:46:11.674877 32015 net.cpp:141] Setting up pool5
I0422 14:46:11.674887 32015 net.cpp:148] Top shape: 15 256 22 22 (1858560)
I0422 14:46:11.674891 32015 net.cpp:156] Memory required for data: 1932475200
I0422 14:46:11.674896 32015 layer_factory.hpp:77] Creating layer fcc6
I0422 14:46:11.674908 32015 net.cpp:91] Creating Layer fcc6
I0422 14:46:11.674913 32015 net.cpp:425] fcc6 <- pool5
I0422 14:46:11.674923 32015 net.cpp:399] fcc6 -> fcc6
I0422 14:46:12.160262 32015 net.cpp:141] Setting up fcc6
I0422 14:46:12.160307 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:12.160315 32015 net.cpp:156] Memory required for data: 2003499840
I0422 14:46:12.160328 32015 layer_factory.hpp:77] Creating layer relu6
I0422 14:46:12.160341 32015 net.cpp:91] Creating Layer relu6
I0422 14:46:12.160348 32015 net.cpp:425] relu6 <- fcc6
I0422 14:46:12.160357 32015 net.cpp:386] relu6 -> fcc6 (in-place)
I0422 14:46:12.161888 32015 net.cpp:141] Setting up relu6
I0422 14:46:12.161907 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:12.161912 32015 net.cpp:156] Memory required for data: 2074524480
I0422 14:46:12.161918 32015 layer_factory.hpp:77] Creating layer drop6
I0422 14:46:12.161934 32015 net.cpp:91] Creating Layer drop6
I0422 14:46:12.161939 32015 net.cpp:425] drop6 <- fcc6
I0422 14:46:12.161945 32015 net.cpp:386] drop6 -> fcc6 (in-place)
I0422 14:46:12.162012 32015 net.cpp:141] Setting up drop6
I0422 14:46:12.162029 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:12.162032 32015 net.cpp:156] Memory required for data: 2145549120
I0422 14:46:12.162036 32015 layer_factory.hpp:77] Creating layer fcc7
I0422 14:46:12.162055 32015 net.cpp:91] Creating Layer fcc7
I0422 14:46:12.162058 32015 net.cpp:425] fcc7 <- fcc6
I0422 14:46:12.162066 32015 net.cpp:399] fcc7 -> fcc7
I0422 14:46:12.395438 32015 net.cpp:141] Setting up fcc7
I0422 14:46:12.395490 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:12.395496 32015 net.cpp:156] Memory required for data: 2216573760
I0422 14:46:12.395510 32015 layer_factory.hpp:77] Creating layer relu7
I0422 14:46:12.395527 32015 net.cpp:91] Creating Layer relu7
I0422 14:46:12.395534 32015 net.cpp:425] relu7 <- fcc7
I0422 14:46:12.395547 32015 net.cpp:386] relu7 -> fcc7 (in-place)
I0422 14:46:12.396610 32015 net.cpp:141] Setting up relu7
I0422 14:46:12.396637 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:12.396641 32015 net.cpp:156] Memory required for data: 2287598400
I0422 14:46:12.396648 32015 layer_factory.hpp:77] Creating layer drop7
I0422 14:46:12.396662 32015 net.cpp:91] Creating Layer drop7
I0422 14:46:12.396667 32015 net.cpp:425] drop7 <- fcc7
I0422 14:46:12.396675 32015 net.cpp:386] drop7 -> fcc7 (in-place)
I0422 14:46:12.396739 32015 net.cpp:141] Setting up drop7
I0422 14:46:12.396751 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:12.396755 32015 net.cpp:156] Memory required for data: 2358623040
I0422 14:46:12.396760 32015 layer_factory.hpp:77] Creating layer score-fr
I0422 14:46:12.396776 32015 net.cpp:91] Creating Layer score-fr
I0422 14:46:12.396781 32015 net.cpp:425] score-fr <- fcc7
I0422 14:46:12.396791 32015 net.cpp:399] score-fr -> score-fcc7
I0422 14:46:12.397959 32015 net.cpp:141] Setting up score-fr
I0422 14:46:12.397977 32015 net.cpp:148] Top shape: 15 16 17 17 (69360)
I0422 14:46:12.397982 32015 net.cpp:156] Memory required for data: 2358900480
I0422 14:46:12.397990 32015 layer_factory.hpp:77] Creating layer upsample
I0422 14:46:12.398005 32015 net.cpp:91] Creating Layer upsample
I0422 14:46:12.398011 32015 net.cpp:425] upsample <- score-fcc7
I0422 14:46:12.398020 32015 net.cpp:399] upsample -> bigscore
I0422 14:46:12.414289 32015 net.cpp:141] Setting up upsample
I0422 14:46:12.414340 32015 net.cpp:148] Top shape: 15 16 575 575 (79350000)
I0422 14:46:12.414346 32015 net.cpp:156] Memory required for data: 2676300480
I0422 14:46:12.414379 32015 layer_factory.hpp:77] Creating layer crop
I0422 14:46:12.414402 32015 net.cpp:91] Creating Layer crop
I0422 14:46:12.414409 32015 net.cpp:425] crop <- bigscore
I0422 14:46:12.414418 32015 net.cpp:425] crop <- data_data_0_split_2
I0422 14:46:12.414427 32015 net.cpp:399] crop -> score
I0422 14:46:12.414556 32015 net.cpp:141] Setting up crop
I0422 14:46:12.414579 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:12.414584 32015 net.cpp:156] Memory required for data: 2927958720
I0422 14:46:12.414590 32015 layer_factory.hpp:77] Creating layer balance
I0422 14:46:12.414602 32015 net.cpp:91] Creating Layer balance
I0422 14:46:12.414607 32015 net.cpp:425] balance <- score
I0422 14:46:12.414613 32015 net.cpp:425] balance <- jointMap_jointMap_0_split_0
I0422 14:46:12.414619 32015 net.cpp:399] balance -> bscore
I0422 14:46:12.414705 32015 net.cpp:141] Setting up balance
I0422 14:46:12.414716 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:12.414719 32015 net.cpp:156] Memory required for data: 3179616960
I0422 14:46:12.414724 32015 layer_factory.hpp:77] Creating layer loss
I0422 14:46:12.414736 32015 net.cpp:91] Creating Layer loss
I0422 14:46:12.414741 32015 net.cpp:425] loss <- bscore
I0422 14:46:12.414746 32015 net.cpp:425] loss <- jointMap_jointMap_0_split_1
I0422 14:46:12.414753 32015 net.cpp:399] loss -> loss
I0422 14:46:12.414824 32015 net.cpp:141] Setting up loss
I0422 14:46:12.414834 32015 net.cpp:148] Top shape: (1)
I0422 14:46:12.414839 32015 net.cpp:151]     with loss weight 1
I0422 14:46:12.414875 32015 net.cpp:156] Memory required for data: 3179616964
I0422 14:46:12.414885 32015 net.cpp:217] loss needs backward computation.
I0422 14:46:12.414891 32015 net.cpp:217] balance needs backward computation.
I0422 14:46:12.414896 32015 net.cpp:217] crop needs backward computation.
I0422 14:46:12.414901 32015 net.cpp:217] upsample needs backward computation.
I0422 14:46:12.414906 32015 net.cpp:217] score-fr needs backward computation.
I0422 14:46:12.414911 32015 net.cpp:217] drop7 needs backward computation.
I0422 14:46:12.414914 32015 net.cpp:217] relu7 needs backward computation.
I0422 14:46:12.414918 32015 net.cpp:217] fcc7 needs backward computation.
I0422 14:46:12.414923 32015 net.cpp:217] drop6 needs backward computation.
I0422 14:46:12.414927 32015 net.cpp:217] relu6 needs backward computation.
I0422 14:46:12.414932 32015 net.cpp:217] fcc6 needs backward computation.
I0422 14:46:12.414937 32015 net.cpp:217] pool5 needs backward computation.
I0422 14:46:12.414940 32015 net.cpp:217] relu5 needs backward computation.
I0422 14:46:12.414945 32015 net.cpp:217] cconv5 needs backward computation.
I0422 14:46:12.414949 32015 net.cpp:217] relu4 needs backward computation.
I0422 14:46:12.414953 32015 net.cpp:217] cconv4 needs backward computation.
I0422 14:46:12.414958 32015 net.cpp:217] relu3 needs backward computation.
I0422 14:46:12.414961 32015 net.cpp:217] conv3 needs backward computation.
I0422 14:46:12.414965 32015 net.cpp:217] norm2 needs backward computation.
I0422 14:46:12.414969 32015 net.cpp:217] pool2 needs backward computation.
I0422 14:46:12.414974 32015 net.cpp:217] relu2 needs backward computation.
I0422 14:46:12.414978 32015 net.cpp:217] conv2 needs backward computation.
I0422 14:46:12.414983 32015 net.cpp:217] norm1 needs backward computation.
I0422 14:46:12.414986 32015 net.cpp:217] pool1 needs backward computation.
I0422 14:46:12.414990 32015 net.cpp:217] relu1 needs backward computation.
I0422 14:46:12.414995 32015 net.cpp:217] conv1 needs backward computation.
I0422 14:46:12.415000 32015 net.cpp:219] jointMap_jointMap_0_split does not need backward computation.
I0422 14:46:12.415005 32015 net.cpp:219] jointMap does not need backward computation.
I0422 14:46:12.415014 32015 net.cpp:219] data_data_0_split does not need backward computation.
I0422 14:46:12.415020 32015 net.cpp:219] data does not need backward computation.
I0422 14:46:12.415024 32015 net.cpp:261] This network produces output loss
I0422 14:46:12.415047 32015 net.cpp:274] Network initialization done.
I0422 14:46:12.416692 32015 solver.cpp:181] Creating test net (#0) specified by net file: fcn_alxnet_train_val.prototxt
I0422 14:46:12.416786 32015 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer data
I0422 14:46:12.417343 32015 net.cpp:49] Initializing net from parameters: 
name: "FCN-AlexNet-joint"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "jointLocation"
  include {
    phase: TEST
  }
  transform_param {
    mean_value: 104
    mean_value: 116
    mean_value: 122
  }
  image_data_param {
    source: "/home/mengxin1/HumanPoseDetect/preprocess/val_label_resized.txt"
    batch_size: 15
    root_folder: "/home/mengxin1/mpii_human_pose_v1_images_resized/"
    label_num: 48
  }
}
layer {
  name: "jointMap"
  type: "PositionToMap"
  bottom: "data"
  bottom: "jointLocation"
  top: "jointMap"
  gaussian_param {
    std: 25
    half_window_size: 50
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 100
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "cconv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "cconv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fcc6"
  type: "Convolution"
  bottom: "pool5"
  top: "fcc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 6
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fcc6"
  top: "fcc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fcc6"
  top: "fcc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fcc7"
  type: "Convolution"
  bottom: "fcc6"
  top: "fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 4096
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fcc7"
  top: "fcc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fcc7"
  top: "fcc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "score-fr"
  type: "Convolution"
  bottom: "fcc7"
  top: "score-fcc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
    engine: CAFFE
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "score-fcc7"
  top: "bigscore"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    kernel_size: 63
    group: 16
    stride: 32
  }
}
layer {
  name: "crop"
  type: "Crop"
  bottom: "bigscore"
  bottom: "data"
  top: "score"
}
layer {
  name: "balance"
  type: "Balance"
  bottom: "score"
  bottom: "jointMap"
  top: "bscore"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "bscore"
  bottom: "jointMap"
  top: "loss"
}
I0422 14:46:12.417567 32015 layer_factory.hpp:77] Creating layer data
I0422 14:46:12.417594 32015 net.cpp:91] Creating Layer data
I0422 14:46:12.417605 32015 net.cpp:399] data -> data
I0422 14:46:12.417623 32015 net.cpp:399] data -> jointLocation
I0422 14:46:12.417639 32015 image_data_layer.cpp:40] Opening file /home/mengxin1/HumanPoseDetect/preprocess/val_label_resized.txt
I0422 14:46:12.448621 32015 image_data_layer.cpp:62] A total of 2424 images.
I0422 14:46:12.454857 32015 image_data_layer.cpp:89] output data size: 15,3,512,512
I0422 14:46:12.605733 32015 net.cpp:141] Setting up data
I0422 14:46:12.605787 32015 net.cpp:148] Top shape: 15 3 512 512 (11796480)
I0422 14:46:12.605795 32015 net.cpp:148] Top shape: 15 48 1 1 (720)
I0422 14:46:12.605799 32015 net.cpp:156] Memory required for data: 47188800
I0422 14:46:12.605809 32015 layer_factory.hpp:77] Creating layer data_data_0_split
I0422 14:46:12.605829 32015 net.cpp:91] Creating Layer data_data_0_split
I0422 14:46:12.605834 32015 net.cpp:425] data_data_0_split <- data
I0422 14:46:12.605845 32015 net.cpp:399] data_data_0_split -> data_data_0_split_0
I0422 14:46:12.605860 32015 net.cpp:399] data_data_0_split -> data_data_0_split_1
I0422 14:46:12.605868 32015 net.cpp:399] data_data_0_split -> data_data_0_split_2
I0422 14:46:12.606120 32015 net.cpp:141] Setting up data_data_0_split
I0422 14:46:12.606138 32015 net.cpp:148] Top shape: 15 3 512 512 (11796480)
I0422 14:46:12.606148 32015 net.cpp:148] Top shape: 15 3 512 512 (11796480)
I0422 14:46:12.606158 32015 net.cpp:148] Top shape: 15 3 512 512 (11796480)
I0422 14:46:12.606164 32015 net.cpp:156] Memory required for data: 188746560
I0422 14:46:12.606173 32015 layer_factory.hpp:77] Creating layer jointMap
I0422 14:46:12.606186 32015 net.cpp:91] Creating Layer jointMap
I0422 14:46:12.606195 32015 net.cpp:425] jointMap <- data_data_0_split_0
I0422 14:46:12.606204 32015 net.cpp:425] jointMap <- jointLocation
I0422 14:46:12.606215 32015 net.cpp:399] jointMap -> jointMap
I0422 14:46:12.606286 32015 net.cpp:141] Setting up jointMap
I0422 14:46:12.606302 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:12.606308 32015 net.cpp:156] Memory required for data: 440404800
I0422 14:46:12.606315 32015 layer_factory.hpp:77] Creating layer jointMap_jointMap_0_split
I0422 14:46:12.606328 32015 net.cpp:91] Creating Layer jointMap_jointMap_0_split
I0422 14:46:12.606335 32015 net.cpp:425] jointMap_jointMap_0_split <- jointMap
I0422 14:46:12.606346 32015 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_0
I0422 14:46:12.606360 32015 net.cpp:399] jointMap_jointMap_0_split -> jointMap_jointMap_0_split_1
I0422 14:46:12.606461 32015 net.cpp:141] Setting up jointMap_jointMap_0_split
I0422 14:46:12.606477 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:12.606485 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:12.606493 32015 net.cpp:156] Memory required for data: 943721280
I0422 14:46:12.606501 32015 layer_factory.hpp:77] Creating layer conv1
I0422 14:46:12.606528 32015 net.cpp:91] Creating Layer conv1
I0422 14:46:12.606537 32015 net.cpp:425] conv1 <- data_data_0_split_1
I0422 14:46:12.606554 32015 net.cpp:399] conv1 -> conv1
I0422 14:46:12.618165 32015 net.cpp:141] Setting up conv1
I0422 14:46:12.618211 32015 net.cpp:148] Top shape: 15 96 176 176 (44605440)
I0422 14:46:12.618216 32015 net.cpp:156] Memory required for data: 1122143040
I0422 14:46:12.618237 32015 layer_factory.hpp:77] Creating layer relu1
I0422 14:46:12.618252 32015 net.cpp:91] Creating Layer relu1
I0422 14:46:12.618258 32015 net.cpp:425] relu1 <- conv1
I0422 14:46:12.618268 32015 net.cpp:386] relu1 -> conv1 (in-place)
I0422 14:46:12.630851 32015 net.cpp:141] Setting up relu1
I0422 14:46:12.630894 32015 net.cpp:148] Top shape: 15 96 176 176 (44605440)
I0422 14:46:12.630899 32015 net.cpp:156] Memory required for data: 1300564800
I0422 14:46:12.630908 32015 layer_factory.hpp:77] Creating layer pool1
I0422 14:46:12.630926 32015 net.cpp:91] Creating Layer pool1
I0422 14:46:12.630933 32015 net.cpp:425] pool1 <- conv1
I0422 14:46:12.630945 32015 net.cpp:399] pool1 -> pool1
I0422 14:46:12.631078 32015 net.cpp:141] Setting up pool1
I0422 14:46:12.631088 32015 net.cpp:148] Top shape: 15 96 88 88 (11151360)
I0422 14:46:12.631091 32015 net.cpp:156] Memory required for data: 1345170240
I0422 14:46:12.631096 32015 layer_factory.hpp:77] Creating layer norm1
I0422 14:46:12.631108 32015 net.cpp:91] Creating Layer norm1
I0422 14:46:12.631113 32015 net.cpp:425] norm1 <- pool1
I0422 14:46:12.631120 32015 net.cpp:399] norm1 -> norm1
I0422 14:46:12.631419 32015 net.cpp:141] Setting up norm1
I0422 14:46:12.631438 32015 net.cpp:148] Top shape: 15 96 88 88 (11151360)
I0422 14:46:12.631446 32015 net.cpp:156] Memory required for data: 1389775680
I0422 14:46:12.631455 32015 layer_factory.hpp:77] Creating layer conv2
I0422 14:46:12.631479 32015 net.cpp:91] Creating Layer conv2
I0422 14:46:12.631484 32015 net.cpp:425] conv2 <- norm1
I0422 14:46:12.631494 32015 net.cpp:399] conv2 -> conv2
I0422 14:46:12.656404 32015 net.cpp:141] Setting up conv2
I0422 14:46:12.656453 32015 net.cpp:148] Top shape: 15 256 88 88 (29736960)
I0422 14:46:12.656458 32015 net.cpp:156] Memory required for data: 1508723520
I0422 14:46:12.656478 32015 layer_factory.hpp:77] Creating layer relu2
I0422 14:46:12.656494 32015 net.cpp:91] Creating Layer relu2
I0422 14:46:12.656500 32015 net.cpp:425] relu2 <- conv2
I0422 14:46:12.656510 32015 net.cpp:386] relu2 -> conv2 (in-place)
I0422 14:46:12.660167 32015 net.cpp:141] Setting up relu2
I0422 14:46:12.660194 32015 net.cpp:148] Top shape: 15 256 88 88 (29736960)
I0422 14:46:12.660199 32015 net.cpp:156] Memory required for data: 1627671360
I0422 14:46:12.660207 32015 layer_factory.hpp:77] Creating layer pool2
I0422 14:46:12.660219 32015 net.cpp:91] Creating Layer pool2
I0422 14:46:12.660225 32015 net.cpp:425] pool2 <- conv2
I0422 14:46:12.660235 32015 net.cpp:399] pool2 -> pool2
I0422 14:46:12.660351 32015 net.cpp:141] Setting up pool2
I0422 14:46:12.660361 32015 net.cpp:148] Top shape: 15 256 44 44 (7434240)
I0422 14:46:12.660364 32015 net.cpp:156] Memory required for data: 1657408320
I0422 14:46:12.660369 32015 layer_factory.hpp:77] Creating layer norm2
I0422 14:46:12.660380 32015 net.cpp:91] Creating Layer norm2
I0422 14:46:12.660385 32015 net.cpp:425] norm2 <- pool2
I0422 14:46:12.660392 32015 net.cpp:399] norm2 -> norm2
I0422 14:46:12.664268 32015 net.cpp:141] Setting up norm2
I0422 14:46:12.664301 32015 net.cpp:148] Top shape: 15 256 44 44 (7434240)
I0422 14:46:12.664305 32015 net.cpp:156] Memory required for data: 1687145280
I0422 14:46:12.664311 32015 layer_factory.hpp:77] Creating layer conv3
I0422 14:46:12.664331 32015 net.cpp:91] Creating Layer conv3
I0422 14:46:12.664336 32015 net.cpp:425] conv3 <- norm2
I0422 14:46:12.664346 32015 net.cpp:399] conv3 -> conv3
I0422 14:46:12.684511 32015 net.cpp:141] Setting up conv3
I0422 14:46:12.684551 32015 net.cpp:148] Top shape: 15 384 44 44 (11151360)
I0422 14:46:12.684557 32015 net.cpp:156] Memory required for data: 1731750720
I0422 14:46:12.684577 32015 layer_factory.hpp:77] Creating layer relu3
I0422 14:46:12.684592 32015 net.cpp:91] Creating Layer relu3
I0422 14:46:12.684597 32015 net.cpp:425] relu3 <- conv3
I0422 14:46:12.684607 32015 net.cpp:386] relu3 -> conv3 (in-place)
I0422 14:46:12.688786 32015 net.cpp:141] Setting up relu3
I0422 14:46:12.688830 32015 net.cpp:148] Top shape: 15 384 44 44 (11151360)
I0422 14:46:12.688840 32015 net.cpp:156] Memory required for data: 1776356160
I0422 14:46:12.688850 32015 layer_factory.hpp:77] Creating layer cconv4
I0422 14:46:12.688874 32015 net.cpp:91] Creating Layer cconv4
I0422 14:46:12.688880 32015 net.cpp:425] cconv4 <- conv3
I0422 14:46:12.688892 32015 net.cpp:399] cconv4 -> conv4
I0422 14:46:12.713891 32015 net.cpp:141] Setting up cconv4
I0422 14:46:12.713932 32015 net.cpp:148] Top shape: 15 384 44 44 (11151360)
I0422 14:46:12.713937 32015 net.cpp:156] Memory required for data: 1820961600
I0422 14:46:12.713950 32015 layer_factory.hpp:77] Creating layer relu4
I0422 14:46:12.713968 32015 net.cpp:91] Creating Layer relu4
I0422 14:46:12.713973 32015 net.cpp:425] relu4 <- conv4
I0422 14:46:12.713984 32015 net.cpp:386] relu4 -> conv4 (in-place)
I0422 14:46:12.718394 32015 net.cpp:141] Setting up relu4
I0422 14:46:12.718426 32015 net.cpp:148] Top shape: 15 384 44 44 (11151360)
I0422 14:46:12.718431 32015 net.cpp:156] Memory required for data: 1865567040
I0422 14:46:12.718438 32015 layer_factory.hpp:77] Creating layer cconv5
I0422 14:46:12.718467 32015 net.cpp:91] Creating Layer cconv5
I0422 14:46:12.718474 32015 net.cpp:425] cconv5 <- conv4
I0422 14:46:12.718484 32015 net.cpp:399] cconv5 -> conv5
I0422 14:46:12.747802 32015 net.cpp:141] Setting up cconv5
I0422 14:46:12.747844 32015 net.cpp:148] Top shape: 15 256 44 44 (7434240)
I0422 14:46:12.747849 32015 net.cpp:156] Memory required for data: 1895304000
I0422 14:46:12.747871 32015 layer_factory.hpp:77] Creating layer relu5
I0422 14:46:12.747886 32015 net.cpp:91] Creating Layer relu5
I0422 14:46:12.747892 32015 net.cpp:425] relu5 <- conv5
I0422 14:46:12.747902 32015 net.cpp:386] relu5 -> conv5 (in-place)
I0422 14:46:12.752404 32015 net.cpp:141] Setting up relu5
I0422 14:46:12.752439 32015 net.cpp:148] Top shape: 15 256 44 44 (7434240)
I0422 14:46:12.752442 32015 net.cpp:156] Memory required for data: 1925040960
I0422 14:46:12.752451 32015 layer_factory.hpp:77] Creating layer pool5
I0422 14:46:12.752468 32015 net.cpp:91] Creating Layer pool5
I0422 14:46:12.752475 32015 net.cpp:425] pool5 <- conv5
I0422 14:46:12.752486 32015 net.cpp:399] pool5 -> pool5
I0422 14:46:12.752616 32015 net.cpp:141] Setting up pool5
I0422 14:46:12.752626 32015 net.cpp:148] Top shape: 15 256 22 22 (1858560)
I0422 14:46:12.752631 32015 net.cpp:156] Memory required for data: 1932475200
I0422 14:46:12.752635 32015 layer_factory.hpp:77] Creating layer fcc6
I0422 14:46:12.752652 32015 net.cpp:91] Creating Layer fcc6
I0422 14:46:12.752657 32015 net.cpp:425] fcc6 <- pool5
I0422 14:46:12.752667 32015 net.cpp:399] fcc6 -> fcc6
I0422 14:46:13.248896 32015 net.cpp:141] Setting up fcc6
I0422 14:46:13.248930 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:13.248935 32015 net.cpp:156] Memory required for data: 2003499840
I0422 14:46:13.248947 32015 layer_factory.hpp:77] Creating layer relu6
I0422 14:46:13.248960 32015 net.cpp:91] Creating Layer relu6
I0422 14:46:13.248966 32015 net.cpp:425] relu6 <- fcc6
I0422 14:46:13.248978 32015 net.cpp:386] relu6 -> fcc6 (in-place)
I0422 14:46:13.249336 32015 net.cpp:141] Setting up relu6
I0422 14:46:13.249349 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:13.249353 32015 net.cpp:156] Memory required for data: 2074524480
I0422 14:46:13.249358 32015 layer_factory.hpp:77] Creating layer drop6
I0422 14:46:13.249368 32015 net.cpp:91] Creating Layer drop6
I0422 14:46:13.249372 32015 net.cpp:425] drop6 <- fcc6
I0422 14:46:13.249382 32015 net.cpp:386] drop6 -> fcc6 (in-place)
I0422 14:46:13.249429 32015 net.cpp:141] Setting up drop6
I0422 14:46:13.249439 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:13.249442 32015 net.cpp:156] Memory required for data: 2145549120
I0422 14:46:13.249446 32015 layer_factory.hpp:77] Creating layer fcc7
I0422 14:46:13.249462 32015 net.cpp:91] Creating Layer fcc7
I0422 14:46:13.249467 32015 net.cpp:425] fcc7 <- fcc6
I0422 14:46:13.249477 32015 net.cpp:399] fcc7 -> fcc7
I0422 14:46:13.467803 32015 net.cpp:141] Setting up fcc7
I0422 14:46:13.467846 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:13.467851 32015 net.cpp:156] Memory required for data: 2216573760
I0422 14:46:13.467864 32015 layer_factory.hpp:77] Creating layer relu7
I0422 14:46:13.467881 32015 net.cpp:91] Creating Layer relu7
I0422 14:46:13.467890 32015 net.cpp:425] relu7 <- fcc7
I0422 14:46:13.467905 32015 net.cpp:386] relu7 -> fcc7 (in-place)
I0422 14:46:13.468130 32015 net.cpp:141] Setting up relu7
I0422 14:46:13.468145 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:13.468149 32015 net.cpp:156] Memory required for data: 2287598400
I0422 14:46:13.468154 32015 layer_factory.hpp:77] Creating layer drop7
I0422 14:46:13.468168 32015 net.cpp:91] Creating Layer drop7
I0422 14:46:13.468173 32015 net.cpp:425] drop7 <- fcc7
I0422 14:46:13.468178 32015 net.cpp:386] drop7 -> fcc7 (in-place)
I0422 14:46:13.468233 32015 net.cpp:141] Setting up drop7
I0422 14:46:13.468245 32015 net.cpp:148] Top shape: 15 4096 17 17 (17756160)
I0422 14:46:13.468248 32015 net.cpp:156] Memory required for data: 2358623040
I0422 14:46:13.468252 32015 layer_factory.hpp:77] Creating layer score-fr
I0422 14:46:13.468267 32015 net.cpp:91] Creating Layer score-fr
I0422 14:46:13.468272 32015 net.cpp:425] score-fr <- fcc7
I0422 14:46:13.468286 32015 net.cpp:399] score-fr -> score-fcc7
I0422 14:46:13.470489 32015 net.cpp:141] Setting up score-fr
I0422 14:46:13.470507 32015 net.cpp:148] Top shape: 15 16 17 17 (69360)
I0422 14:46:13.470512 32015 net.cpp:156] Memory required for data: 2358900480
I0422 14:46:13.470520 32015 layer_factory.hpp:77] Creating layer upsample
I0422 14:46:13.470532 32015 net.cpp:91] Creating Layer upsample
I0422 14:46:13.470540 32015 net.cpp:425] upsample <- score-fcc7
I0422 14:46:13.470549 32015 net.cpp:399] upsample -> bigscore
I0422 14:46:13.472446 32015 net.cpp:141] Setting up upsample
I0422 14:46:13.472461 32015 net.cpp:148] Top shape: 15 16 575 575 (79350000)
I0422 14:46:13.472466 32015 net.cpp:156] Memory required for data: 2676300480
I0422 14:46:13.472481 32015 layer_factory.hpp:77] Creating layer crop
I0422 14:46:13.472492 32015 net.cpp:91] Creating Layer crop
I0422 14:46:13.472497 32015 net.cpp:425] crop <- bigscore
I0422 14:46:13.472503 32015 net.cpp:425] crop <- data_data_0_split_2
I0422 14:46:13.472509 32015 net.cpp:399] crop -> score
I0422 14:46:13.472568 32015 net.cpp:141] Setting up crop
I0422 14:46:13.472580 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:13.472585 32015 net.cpp:156] Memory required for data: 2927958720
I0422 14:46:13.472589 32015 layer_factory.hpp:77] Creating layer balance
I0422 14:46:13.472597 32015 net.cpp:91] Creating Layer balance
I0422 14:46:13.472602 32015 net.cpp:425] balance <- score
I0422 14:46:13.472607 32015 net.cpp:425] balance <- jointMap_jointMap_0_split_0
I0422 14:46:13.472614 32015 net.cpp:399] balance -> bscore
I0422 14:46:13.472688 32015 net.cpp:141] Setting up balance
I0422 14:46:13.472697 32015 net.cpp:148] Top shape: 15 16 512 512 (62914560)
I0422 14:46:13.472702 32015 net.cpp:156] Memory required for data: 3179616960
I0422 14:46:13.472705 32015 layer_factory.hpp:77] Creating layer loss
I0422 14:46:13.472713 32015 net.cpp:91] Creating Layer loss
I0422 14:46:13.472718 32015 net.cpp:425] loss <- bscore
I0422 14:46:13.472725 32015 net.cpp:425] loss <- jointMap_jointMap_0_split_1
I0422 14:46:13.472731 32015 net.cpp:399] loss -> loss
I0422 14:46:13.472806 32015 net.cpp:141] Setting up loss
I0422 14:46:13.472815 32015 net.cpp:148] Top shape: (1)
I0422 14:46:13.472820 32015 net.cpp:151]     with loss weight 1
I0422 14:46:13.472834 32015 net.cpp:156] Memory required for data: 3179616964
I0422 14:46:13.472839 32015 net.cpp:217] loss needs backward computation.
I0422 14:46:13.472843 32015 net.cpp:217] balance needs backward computation.
I0422 14:46:13.472848 32015 net.cpp:217] crop needs backward computation.
I0422 14:46:13.472853 32015 net.cpp:217] upsample needs backward computation.
I0422 14:46:13.472857 32015 net.cpp:217] score-fr needs backward computation.
I0422 14:46:13.472862 32015 net.cpp:217] drop7 needs backward computation.
I0422 14:46:13.472865 32015 net.cpp:217] relu7 needs backward computation.
I0422 14:46:13.472869 32015 net.cpp:217] fcc7 needs backward computation.
I0422 14:46:13.472873 32015 net.cpp:217] drop6 needs backward computation.
I0422 14:46:13.472878 32015 net.cpp:217] relu6 needs backward computation.
I0422 14:46:13.472887 32015 net.cpp:217] fcc6 needs backward computation.
I0422 14:46:13.472892 32015 net.cpp:217] pool5 needs backward computation.
I0422 14:46:13.472895 32015 net.cpp:217] relu5 needs backward computation.
I0422 14:46:13.472899 32015 net.cpp:217] cconv5 needs backward computation.
I0422 14:46:13.472904 32015 net.cpp:217] relu4 needs backward computation.
I0422 14:46:13.472908 32015 net.cpp:217] cconv4 needs backward computation.
I0422 14:46:13.472913 32015 net.cpp:217] relu3 needs backward computation.
I0422 14:46:13.472918 32015 net.cpp:217] conv3 needs backward computation.
I0422 14:46:13.472921 32015 net.cpp:217] norm2 needs backward computation.
I0422 14:46:13.472926 32015 net.cpp:217] pool2 needs backward computation.
I0422 14:46:13.472930 32015 net.cpp:217] relu2 needs backward computation.
I0422 14:46:13.472934 32015 net.cpp:217] conv2 needs backward computation.
I0422 14:46:13.472939 32015 net.cpp:217] norm1 needs backward computation.
I0422 14:46:13.472944 32015 net.cpp:217] pool1 needs backward computation.
I0422 14:46:13.472949 32015 net.cpp:217] relu1 needs backward computation.
I0422 14:46:13.472952 32015 net.cpp:217] conv1 needs backward computation.
I0422 14:46:13.472957 32015 net.cpp:219] jointMap_jointMap_0_split does not need backward computation.
I0422 14:46:13.472962 32015 net.cpp:219] jointMap does not need backward computation.
I0422 14:46:13.472968 32015 net.cpp:219] data_data_0_split does not need backward computation.
I0422 14:46:13.472973 32015 net.cpp:219] data does not need backward computation.
I0422 14:46:13.472976 32015 net.cpp:261] This network produces output loss
I0422 14:46:13.473001 32015 net.cpp:274] Network initialization done.
I0422 14:46:13.473101 32015 solver.cpp:60] Solver scaffolding done.
I0422 14:46:13.754901 32015 upgrade_proto.cpp:43] Attempting to upgrade input file specified using deprecated transformation parameters: /home/mengxin1/HumanPoseDetect/fcn_joint_predict/bvlc_reference_caffenet.caffemodel
I0422 14:46:13.754945 32015 upgrade_proto.cpp:46] Successfully upgraded file specified using deprecated data transformation parameters.
W0422 14:46:13.754950 32015 upgrade_proto.cpp:48] Note that future Caffe releases will only support transform_param messages for transformation fields.
I0422 14:46:13.754953 32015 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/mengxin1/HumanPoseDetect/fcn_joint_predict/bvlc_reference_caffenet.caffemodel
I0422 14:46:14.724195 32015 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I0422 14:46:14.726763 32015 net.cpp:752] Ignoring source layer conv4
I0422 14:46:14.726794 32015 net.cpp:752] Ignoring source layer conv5
I0422 14:46:14.726799 32015 net.cpp:752] Ignoring source layer fc6
I0422 14:46:14.726804 32015 net.cpp:752] Ignoring source layer fc7
I0422 14:46:14.726807 32015 net.cpp:752] Ignoring source layer fc8
I0422 14:46:14.734221 32015 solver.cpp:337] Iteration 0, Testing net (#0)
I0422 14:47:06.293390 32015 solver.cpp:404]     Test net output #0: loss = 8.62719 (* 1 = 8.62719 loss)
I0422 14:47:09.777681 32015 solver.cpp:228] Iteration 0, loss = 9.49368
I0422 14:47:09.777740 32015 solver.cpp:244]     Train net output #0: loss = 9.49368 (* 1 = 9.49368 loss)
I0422 14:47:09.777751 32015 sgd_solver.cpp:106] Iteration 0, lr = 0.0008
I0422 14:48:44.624274 32015 solver.cpp:228] Iteration 20, loss = 6.72559
I0422 14:48:44.624778 32015 solver.cpp:244]     Train net output #0: loss = 6.72559 (* 1 = 6.72559 loss)
I0422 14:48:44.624799 32015 sgd_solver.cpp:106] Iteration 20, lr = 0.0008
I0422 14:50:21.274863 32015 solver.cpp:228] Iteration 40, loss = 5.94694
I0422 14:50:21.275292 32015 solver.cpp:244]     Train net output #0: loss = 5.94694 (* 1 = 5.94694 loss)
I0422 14:50:21.275306 32015 sgd_solver.cpp:106] Iteration 40, lr = 0.0008
I0422 14:51:56.934448 32015 solver.cpp:228] Iteration 60, loss = 6.18417
I0422 14:51:56.934501 32015 solver.cpp:244]     Train net output #0: loss = 6.18417 (* 1 = 6.18417 loss)
I0422 14:51:56.934520 32015 sgd_solver.cpp:106] Iteration 60, lr = 0.0008
I0422 14:53:34.223446 32015 solver.cpp:228] Iteration 80, loss = 6.20074
I0422 14:53:34.223965 32015 solver.cpp:244]     Train net output #0: loss = 6.20074 (* 1 = 6.20074 loss)
I0422 14:53:34.223979 32015 sgd_solver.cpp:106] Iteration 80, lr = 0.0008
I0422 14:55:09.102319 32015 solver.cpp:228] Iteration 100, loss = 5.94359
I0422 14:55:09.102716 32015 solver.cpp:244]     Train net output #0: loss = 5.94359 (* 1 = 5.94359 loss)
I0422 14:55:09.102730 32015 sgd_solver.cpp:106] Iteration 100, lr = 0.0008
I0422 14:56:43.616623 32015 solver.cpp:228] Iteration 120, loss = 6.36866
I0422 14:56:43.617064 32015 solver.cpp:244]     Train net output #0: loss = 6.36866 (* 1 = 6.36866 loss)
I0422 14:56:43.617080 32015 sgd_solver.cpp:106] Iteration 120, lr = 0.0008
I0422 14:58:18.225666 32015 solver.cpp:228] Iteration 140, loss = 6.17915
I0422 14:58:18.226117 32015 solver.cpp:244]     Train net output #0: loss = 6.17915 (* 1 = 6.17915 loss)
I0422 14:58:18.226131 32015 sgd_solver.cpp:106] Iteration 140, lr = 0.0008
I0422 14:59:47.087178 32015 solver.cpp:228] Iteration 160, loss = 6.18159
I0422 14:59:47.087648 32015 solver.cpp:244]     Train net output #0: loss = 6.18159 (* 1 = 6.18159 loss)
I0422 14:59:47.087664 32015 sgd_solver.cpp:106] Iteration 160, lr = 0.0008
I0422 15:01:15.086197 32015 solver.cpp:228] Iteration 180, loss = 6.06869
I0422 15:01:15.086604 32015 solver.cpp:244]     Train net output #0: loss = 6.06869 (* 1 = 6.06869 loss)
I0422 15:01:15.086618 32015 sgd_solver.cpp:106] Iteration 180, lr = 0.0008
I0422 15:02:50.588192 32015 solver.cpp:228] Iteration 200, loss = 7.05541
I0422 15:02:50.588677 32015 solver.cpp:244]     Train net output #0: loss = 7.05541 (* 1 = 7.05541 loss)
I0422 15:02:50.588696 32015 sgd_solver.cpp:106] Iteration 200, lr = 0.0008
I0422 15:04:22.440382 32015 solver.cpp:228] Iteration 220, loss = 5.34011
I0422 15:04:22.440856 32015 solver.cpp:244]     Train net output #0: loss = 5.34011 (* 1 = 5.34011 loss)
I0422 15:04:22.440871 32015 sgd_solver.cpp:106] Iteration 220, lr = 0.0008
I0422 15:05:55.218765 32015 solver.cpp:228] Iteration 240, loss = 6.65715
I0422 15:05:55.219177 32015 solver.cpp:244]     Train net output #0: loss = 6.65715 (* 1 = 6.65715 loss)
I0422 15:05:55.219194 32015 sgd_solver.cpp:106] Iteration 240, lr = 0.0008
I0422 15:07:27.007449 32015 solver.cpp:228] Iteration 260, loss = 7.31264
I0422 15:07:27.007900 32015 solver.cpp:244]     Train net output #0: loss = 7.31264 (* 1 = 7.31264 loss)
I0422 15:07:27.007915 32015 sgd_solver.cpp:106] Iteration 260, lr = 0.0008
I0422 15:09:04.317060 32015 solver.cpp:228] Iteration 280, loss = 5.61613
I0422 15:09:04.317560 32015 solver.cpp:244]     Train net output #0: loss = 5.61613 (* 1 = 5.61613 loss)
I0422 15:09:04.317580 32015 sgd_solver.cpp:106] Iteration 280, lr = 0.0008
I0422 15:10:35.957574 32015 solver.cpp:228] Iteration 300, loss = 3.98198
I0422 15:10:35.957630 32015 solver.cpp:244]     Train net output #0: loss = 3.98198 (* 1 = 3.98198 loss)
I0422 15:10:35.957643 32015 sgd_solver.cpp:106] Iteration 300, lr = 0.0008
I0422 15:12:09.255424 32015 solver.cpp:228] Iteration 320, loss = 4.14559
I0422 15:12:09.255939 32015 solver.cpp:244]     Train net output #0: loss = 4.14559 (* 1 = 4.14559 loss)
I0422 15:12:09.255961 32015 sgd_solver.cpp:106] Iteration 320, lr = 0.0008
I0422 15:13:44.588069 32015 solver.cpp:228] Iteration 340, loss = 4.18395
I0422 15:13:44.588557 32015 solver.cpp:244]     Train net output #0: loss = 4.18395 (* 1 = 4.18395 loss)
I0422 15:13:44.588574 32015 sgd_solver.cpp:106] Iteration 340, lr = 0.0008
I0422 15:15:16.123239 32015 solver.cpp:228] Iteration 360, loss = 4.15342
I0422 15:15:16.123289 32015 solver.cpp:244]     Train net output #0: loss = 4.15342 (* 1 = 4.15342 loss)
I0422 15:15:16.123301 32015 sgd_solver.cpp:106] Iteration 360, lr = 0.0008
I0422 15:16:48.213510 32015 solver.cpp:228] Iteration 380, loss = 4.17502
I0422 15:16:48.214027 32015 solver.cpp:244]     Train net output #0: loss = 4.17502 (* 1 = 4.17502 loss)
I0422 15:16:48.214045 32015 sgd_solver.cpp:106] Iteration 380, lr = 0.0008
I0422 15:18:19.481616 32015 solver.cpp:228] Iteration 400, loss = 4.17661
I0422 15:18:19.482126 32015 solver.cpp:244]     Train net output #0: loss = 4.17661 (* 1 = 4.17661 loss)
I0422 15:18:19.482147 32015 sgd_solver.cpp:106] Iteration 400, lr = 0.0008
I0422 15:19:52.979415 32015 solver.cpp:228] Iteration 420, loss = 9.05375
I0422 15:19:53.008622 32015 solver.cpp:244]     Train net output #0: loss = 9.05375 (* 1 = 9.05375 loss)
I0422 15:19:53.008642 32015 sgd_solver.cpp:106] Iteration 420, lr = 0.0008
I0422 15:21:23.453006 32015 solver.cpp:228] Iteration 440, loss = 6.24412
I0422 15:21:23.453464 32015 solver.cpp:244]     Train net output #0: loss = 6.24412 (* 1 = 6.24412 loss)
I0422 15:21:23.453485 32015 sgd_solver.cpp:106] Iteration 440, lr = 0.0008
I0422 15:22:57.675469 32015 solver.cpp:228] Iteration 460, loss = 4.53369
I0422 15:22:57.675984 32015 solver.cpp:244]     Train net output #0: loss = 4.53369 (* 1 = 4.53369 loss)
I0422 15:22:57.676003 32015 sgd_solver.cpp:106] Iteration 460, lr = 0.0008
I0422 15:24:28.837105 32015 solver.cpp:228] Iteration 480, loss = 5.60131
I0422 15:24:28.837512 32015 solver.cpp:244]     Train net output #0: loss = 5.60131 (* 1 = 5.60131 loss)
I0422 15:24:28.837525 32015 sgd_solver.cpp:106] Iteration 480, lr = 0.0008
I0422 15:26:00.584079 32015 solver.cpp:228] Iteration 500, loss = 4.62586
I0422 15:26:00.584460 32015 solver.cpp:244]     Train net output #0: loss = 4.62586 (* 1 = 4.62586 loss)
I0422 15:26:00.584472 32015 sgd_solver.cpp:106] Iteration 500, lr = 0.0008
I0422 15:27:32.157820 32015 solver.cpp:228] Iteration 520, loss = 5.32686
I0422 15:27:32.158172 32015 solver.cpp:244]     Train net output #0: loss = 5.32686 (* 1 = 5.32686 loss)
I0422 15:27:32.158192 32015 sgd_solver.cpp:106] Iteration 520, lr = 0.0008
I0422 15:29:03.995728 32015 solver.cpp:228] Iteration 540, loss = 3.37264
I0422 15:29:03.996129 32015 solver.cpp:244]     Train net output #0: loss = 3.37264 (* 1 = 3.37264 loss)
I0422 15:29:03.996143 32015 sgd_solver.cpp:106] Iteration 540, lr = 0.0008
I0422 15:30:33.836158 32015 solver.cpp:228] Iteration 560, loss = 3.7487
I0422 15:30:33.836967 32015 solver.cpp:244]     Train net output #0: loss = 3.7487 (* 1 = 3.7487 loss)
I0422 15:30:33.836985 32015 sgd_solver.cpp:106] Iteration 560, lr = 0.0008
I0422 15:32:08.795739 32015 solver.cpp:228] Iteration 580, loss = 3.87856
I0422 15:32:08.796156 32015 solver.cpp:244]     Train net output #0: loss = 3.87856 (* 1 = 3.87856 loss)
I0422 15:32:08.796172 32015 sgd_solver.cpp:106] Iteration 580, lr = 0.0008
I0422 15:33:39.609865 32015 solver.cpp:228] Iteration 600, loss = 4.49441
I0422 15:33:39.610250 32015 solver.cpp:244]     Train net output #0: loss = 4.49441 (* 1 = 4.49441 loss)
I0422 15:33:39.610270 32015 sgd_solver.cpp:106] Iteration 600, lr = 0.0008
I0422 15:35:10.616322 32015 solver.cpp:228] Iteration 620, loss = 5.12095
I0422 15:35:10.616675 32015 solver.cpp:244]     Train net output #0: loss = 5.12095 (* 1 = 5.12095 loss)
I0422 15:35:10.616695 32015 sgd_solver.cpp:106] Iteration 620, lr = 0.0008
